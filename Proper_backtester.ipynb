{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf8eb7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import calendar\n",
    "import numpy as np\n",
    "import glob\n",
    "import sys\n",
    "import gc\n",
    "import re\n",
    "#import quantstats as qs\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nest_asyncio # install this package to avoid running in loop\n",
    "\n",
    "import urllib\n",
    "import pandas_ta as ta\n",
    "\n",
    "import warnings\n",
    "\n",
    "import requests\n",
    "\n",
    "import hmac\n",
    "import hashlib\n",
    "\n",
    "import aiohttp\n",
    "import asyncio\n",
    "\n",
    "import logging\n",
    "from datetime import timezone\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "# Filter depreciation warnings from pandas regarding the append method\n",
    "warnings.filterwarnings('ignore', category=FutureWarning, message=\".*append.*\")\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Set pandas to display all digits for floating-point numbers\n",
    "pd.options.display.float_format = '{:.8f}'.format\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# Ignore only FutureWarning\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Ignore only RuntimeWarning\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "import importlib\n",
    "\n",
    "\n",
    "from Orats_options_fetch_functions import *\n",
    "\n",
    "from faster_numba_strategy_functions import *\n",
    "\n",
    "import strategy_functions\n",
    "\n",
    "importlib.reload(strategy_functions)\n",
    "from strategy_functions import *\n",
    "\n",
    "# import Common_functions\n",
    "\n",
    "# importlib.reload(strategy_functions)\n",
    "# importlib.reload(Common_functions)\n",
    "\n",
    "# from Common_functions import *\n",
    "# coin_desk_api_key =  \"e43c1fd991e660b9bf959645f0800bb7e76fb4a3537ab773cec62b2fad31af2d\"\n",
    "from datetime import time\n",
    "from pathlib import Path\n",
    "# Get the user's home directory dynamically\n",
    "home_dir = r\"C:\\Data\\Options_logs\"\n",
    "#====================================\n",
    "\n",
    "strategy_name = \"Backtest_v1\"\n",
    "\n",
    "# Create a folder for the strategy_name if it doesn't exist in home directory\n",
    "\n",
    "strategy_folder = os.path.join(home_dir, strategy_name)\n",
    "if not os.path.exists(strategy_folder):\n",
    "    os.makedirs(strategy_folder)\n",
    "\n",
    "# Create the full path to the output file. this file contains all the trades\n",
    "\n",
    "output_all_trades_file = os.path.join(strategy_folder, f\"All_trades_{strategy_name}.csv\")\n",
    "trade_html_file = os.path.join(strategy_folder, f\"Trade_stats_{strategy_name}.html\")\n",
    "\n",
    "#=== location to store the trade dataframe for debug\n",
    "\n",
    "base_folder_df = \"strategy_dataframe\"\n",
    "df_folder = os.path.join(strategy_folder, base_folder_df)\n",
    "if not os.path.exists(df_folder):\n",
    "    os.makedirs(df_folder)\n",
    "    print(\"\\n Created strategy dataframe folder : \" ,df_folder )\n",
    "\n",
    "#----- symbol_wise_trades\n",
    "\n",
    "trades_folder = \"sym_wise_trades\"\n",
    "trades_strategy_folder = os.path.join(strategy_folder, trades_folder)\n",
    "\n",
    "if not os.path.exists(trades_strategy_folder):\n",
    "    print(\"\\n Making folder for symbol wise trades of strategy\" , trades_strategy_folder)\n",
    "    os.makedirs(trades_strategy_folder)\n",
    "\n",
    "logger = setup_logger(\"Straddle_0DTE\", strategy_folder, log_file=\"strategy.log\" , to_console=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f6f7dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying data from: s3://duckdata/ORATS/Options/**/*.parquet\n",
      "\n",
      "‚úÖ Query successful! First 5 rows of data:\n",
      "      expiry       strike  dte  optionType                  ts         low  \\\n",
      "0 2024-01-26 421.00000000   25          -1 2024-01-02 17:13:00 17.18000000   \n",
      "1 2024-01-12 414.00000000   11          -1 2024-01-02 17:13:00 10.41500000   \n",
      "\n",
      "         high        open       close     volume  ...      askSize  \\\n",
      "0 17.18000000 17.18000000 17.18000000 0.00000000  ...  50.00000000   \n",
      "1 10.41500000 10.41500000 10.41500000 0.00000000  ... 151.00000000   \n",
      "\n",
      "     bidPrice    askPrice      bidIv      askIv         iv          oi  \\\n",
      "0 17.12000000 17.24000000 0.13785700 0.14511500 0.14148600 15.00000000   \n",
      "1 10.34000000 10.49000000 0.14267100 0.15199600 0.14733300 54.00000000   \n",
      "\n",
      "    stockPrice        day ticker  \n",
      "0 404.00000000 2024-01-02    QQQ  \n",
      "1 404.00000000 2024-01-02    QQQ  \n",
      "\n",
      "[2 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Connect to an in-memory database\n",
    "con = duckdb.connect()\n",
    "\n",
    "# --- Your setup code (which is correct) ---\n",
    "con.execute(\"INSTALL httpfs;\")\n",
    "con.execute(\"LOAD httpfs;\")\n",
    "con.execute(\"\"\"\n",
    "SET s3_endpoint='122.176.133.73:9000';\n",
    "SET s3_use_ssl=false;\n",
    "SET s3_access_key_id='minioadmin';\n",
    "SET s3_secret_access_key='minioadmin';\n",
    "\"\"\")\n",
    "con.execute(\"SET s3_url_style='path';\") \n",
    "con.execute(\"PRAGMA threads=8;\")\n",
    "con.execute(\"PRAGMA enable_object_cache;\")\n",
    "\n",
    "# --- THE MISSING STEP: Querying a file from S3 ---\n",
    "# Replace with your actual S3 bucket and file path\n",
    "# This can be a .parquet, .csv, or .json file\n",
    "s3_file_path = 's3://duckdata/ORATS/Options/**/*.parquet'\n",
    "\n",
    "try:\n",
    "    # Execute a query on the S3 file and fetch the result as a Pandas DataFrame\n",
    "    print(f\"Querying data from: {s3_file_path}\")\n",
    "    df = con.execute(f\"\"\"\n",
    "        SELECT *\n",
    "        FROM '{s3_file_path}'\n",
    "        LIMIT 2;\n",
    "    \"\"\").df()\n",
    "\n",
    "    print(\"\\n‚úÖ Query successful! First 5 rows of data:\")\n",
    "    print(df.head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå An error occurred during the query: {e}\")\n",
    "\n",
    "# finally:\n",
    "#     # It's good practice to close the connection\n",
    "#     con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14059a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r\"C:\\Data\\Aggregate_Data\\US_top_250.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db1ed545",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Symbol'] = df['Symbol'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52456e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_top_250 = df['Symbol'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "912c76a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NVDA',\n",
       " 'MSFT',\n",
       " 'AAPL',\n",
       " 'GOOGL',\n",
       " 'AMZN',\n",
       " 'META',\n",
       " 'AVGO',\n",
       " 'TSLA',\n",
       " 'TSM',\n",
       " 'BRK.B',\n",
       " 'ORCL',\n",
       " 'JPM',\n",
       " 'WMT',\n",
       " 'LLY',\n",
       " 'V',\n",
       " 'MA',\n",
       " 'NFLX',\n",
       " 'XOM',\n",
       " 'JNJ',\n",
       " 'COST',\n",
       " 'HD',\n",
       " 'PLTR',\n",
       " 'ABBV',\n",
       " 'BAC',\n",
       " 'PG',\n",
       " 'BABA',\n",
       " 'ASML',\n",
       " 'CVX',\n",
       " 'UNH',\n",
       " 'GE',\n",
       " 'SAP',\n",
       " 'KO',\n",
       " 'TMUS',\n",
       " 'CSCO',\n",
       " 'AMD',\n",
       " 'WFC',\n",
       " 'TM',\n",
       " 'PM',\n",
       " 'MS',\n",
       " 'GS',\n",
       " 'NVO',\n",
       " 'AZN',\n",
       " 'IBM',\n",
       " 'NVS',\n",
       " 'HSBC',\n",
       " 'CRM',\n",
       " 'ABT',\n",
       " 'AXP',\n",
       " 'BX',\n",
       " 'LIN',\n",
       " 'MCD',\n",
       " 'RTX',\n",
       " 'T',\n",
       " 'SHEL',\n",
       " 'DIS',\n",
       " 'UBER',\n",
       " 'CAT',\n",
       " 'RY',\n",
       " 'MRK',\n",
       " 'APP',\n",
       " 'NOW',\n",
       " 'PEP',\n",
       " 'SHOP',\n",
       " 'VZ',\n",
       " 'C',\n",
       " 'ANET',\n",
       " 'INTU',\n",
       " 'PDD',\n",
       " 'BKNG',\n",
       " 'TMO',\n",
       " 'MUFG',\n",
       " 'MU',\n",
       " 'QCOM',\n",
       " 'BLK',\n",
       " 'SONY',\n",
       " 'GEV',\n",
       " 'HDB',\n",
       " 'SCHW',\n",
       " 'SPGI',\n",
       " 'BA',\n",
       " 'ARM',\n",
       " 'TXN',\n",
       " 'TJX',\n",
       " 'ISRG',\n",
       " 'UL',\n",
       " 'LOW',\n",
       " 'LRCX',\n",
       " 'SAN',\n",
       " 'ACN',\n",
       " 'BSX',\n",
       " 'AMGN',\n",
       " 'ADBE',\n",
       " 'NEE',\n",
       " 'ETN',\n",
       " 'APH',\n",
       " 'COF',\n",
       " 'SPOT',\n",
       " 'SYK',\n",
       " 'PGR',\n",
       " 'GILD',\n",
       " 'BHP',\n",
       " 'PFE',\n",
       " 'AMAT',\n",
       " 'DHR',\n",
       " 'PANW',\n",
       " 'TTE',\n",
       " 'HON',\n",
       " 'TD',\n",
       " 'UBS',\n",
       " 'KLAC',\n",
       " 'KKR',\n",
       " 'UNP',\n",
       " 'DE',\n",
       " 'BTI',\n",
       " 'ADI',\n",
       " 'CMCSA',\n",
       " 'MDT',\n",
       " 'MELI',\n",
       " 'ADP',\n",
       " 'COP',\n",
       " 'INTC',\n",
       " 'IBN',\n",
       " 'BUD',\n",
       " 'SNY',\n",
       " 'WELL',\n",
       " 'SE',\n",
       " 'CRWD',\n",
       " 'BBVA',\n",
       " 'LMT',\n",
       " 'DASH',\n",
       " 'CB',\n",
       " 'MO',\n",
       " 'PLD',\n",
       " 'NKE',\n",
       " 'RIO',\n",
       " 'ENB',\n",
       " 'SMFG',\n",
       " 'BN',\n",
       " 'CEG',\n",
       " 'HOOD',\n",
       " 'SO',\n",
       " 'VRTX',\n",
       " 'NTES',\n",
       " 'ICE',\n",
       " 'MMC',\n",
       " 'PH',\n",
       " 'CDNS',\n",
       " 'DUK',\n",
       " 'RBLX',\n",
       " 'BMY',\n",
       " 'HCA',\n",
       " 'SBUX',\n",
       " 'MSTR',\n",
       " 'CME',\n",
       " 'CVS',\n",
       " 'BAM',\n",
       " 'BMO',\n",
       " 'MCO',\n",
       " 'ORLY',\n",
       " 'AMT',\n",
       " 'RCL',\n",
       " 'TT',\n",
       " 'SCCO',\n",
       " 'SHW',\n",
       " 'BP',\n",
       " 'GD',\n",
       " 'MCK',\n",
       " 'WM',\n",
       " 'NEM',\n",
       " 'RELX',\n",
       " 'DELL',\n",
       " 'RACE',\n",
       " 'COIN',\n",
       " 'MMM',\n",
       " 'NOC',\n",
       " 'MFG',\n",
       " 'GSK',\n",
       " 'CTAS',\n",
       " 'MSI',\n",
       " 'PBR',\n",
       " 'PBR.A',\n",
       " 'PNC',\n",
       " 'MDLZ',\n",
       " 'CVNA',\n",
       " 'CI',\n",
       " 'BNS',\n",
       " 'APO',\n",
       " 'NET',\n",
       " 'AON',\n",
       " 'SNPS',\n",
       " 'EQIX',\n",
       " 'AEM',\n",
       " 'TRI',\n",
       " 'ITW',\n",
       " 'NU',\n",
       " 'ECL',\n",
       " 'USB',\n",
       " 'SNOW',\n",
       " 'EMR',\n",
       " 'HWM',\n",
       " 'CRH',\n",
       " 'BK',\n",
       " 'AJG',\n",
       " 'CM',\n",
       " 'ABNB',\n",
       " 'BCS',\n",
       " 'ITUB',\n",
       " 'ING',\n",
       " 'TDG',\n",
       " 'VST',\n",
       " 'FI',\n",
       " 'MAR',\n",
       " 'DB',\n",
       " 'WMB',\n",
       " 'RSG',\n",
       " 'UPS',\n",
       " 'INFY',\n",
       " 'AZO',\n",
       " 'NGG',\n",
       " 'JCI',\n",
       " 'CP',\n",
       " 'SPG',\n",
       " 'ELV',\n",
       " 'EPD',\n",
       " 'ADSK',\n",
       " 'LYG',\n",
       " 'CNQ',\n",
       " 'GLW',\n",
       " 'CL',\n",
       " 'FCX',\n",
       " 'ZTS',\n",
       " 'APD',\n",
       " 'EOG',\n",
       " 'PYPL',\n",
       " 'HLT',\n",
       " 'MNST',\n",
       " 'CRWV',\n",
       " 'TEL',\n",
       " 'TRV',\n",
       " 'NSC',\n",
       " 'FTNT',\n",
       " 'EQNR',\n",
       " 'AMX',\n",
       " 'CPNG',\n",
       " 'URI',\n",
       " 'KMI',\n",
       " 'DLR',\n",
       " 'CSX',\n",
       " 'ALNY',\n",
       " 'ET']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_top_250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b9006c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#====================================\n",
    "ifol = r\"C:\\Data\\Aggregate_Data\\Correct_US_SP500_2019_2025_15min.parquet\"\n",
    "\n",
    "\n",
    "#ifol = r\"C:\\Data\\Aggregate_Data\\US_Stocks_2022_2024\\qspark\\US_qspark_15_min_US_mid_liquidity_267_stocks_2019_2015.parquet\"\n",
    "\n",
    "com_df2 = pd.read_parquet(ifol)\n",
    "\n",
    "com_df2 = com_df2.loc[:, ~com_df2.columns.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "468bf11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the most important change to fix your issue.\n",
    "final_symbols = com_df2.filter(like='_close').columns.str.replace('_close', '').unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9d2da5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['A', 'AAPL', 'ABBV', 'ABT', 'ACGL', 'ACN', 'ADBE', 'ADI', 'ADM', 'ADP',\n",
       "       ...\n",
       "       'WRB', 'WSM', 'WY', 'WYNN', 'XEL', 'XOM', 'XYL', 'YUM', 'ZBH', 'ZTS'],\n",
       "      dtype='object', length=502)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "adef0b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of final symbols: 174\n"
     ]
    }
   ],
   "source": [
    "\n",
    "symbols_to_keep = [symbol for symbol in final_symbols if symbol in us_top_250]\n",
    "\n",
    "# Now, len(symbols_to_keep) should be the true unique count (e.g., 174)\n",
    "\n",
    "# Step 3: Create a list of all columns that should be kept using a more robust check\n",
    "columns_to_keep = []\n",
    "for symbol in symbols_to_keep:\n",
    "    # This is a more robust way to match, preventing 'WMT' from matching 'WMTB'\n",
    "    cols_for_symbol = [col for col in com_df2.columns if col.startswith(symbol + '_')]\n",
    "    columns_to_keep.extend(cols_for_symbol)\n",
    "\n",
    "# Step 4: Create the new DataFrame\n",
    "filtered_com_df = com_df2[columns_to_keep]\n",
    "\n",
    "# --- Verification Step ---\n",
    "# This should now give you the expected count of 174\n",
    "final_symbols2 = filtered_com_df.filter(like='_close').columns.str.replace('_close', '')\n",
    "print(f\"Length of final symbols: {len(final_symbols2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7d892109",
   "metadata": {},
   "outputs": [],
   "source": [
    "com_df4 = quick_float32_convert2(filtered_com_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f0d74969",
   "metadata": {},
   "outputs": [],
   "source": [
    "com_df4.to_parquet(r\"C:\\Data\\Aggregate_Data\\US_top_200_2019_2025_15min.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0687ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_symbols2 = filtered_com_df.filter(like='_close').columns.str.replace('_close', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc07fedc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['AAPL', 'ABBV', 'ABT', 'ACN', 'ADBE', 'ADI', 'ADP', 'ADSK', 'AJG',\n",
       "       'AMAT',\n",
       "       ...\n",
       "       'VZ', 'WELL', 'WFC', 'WM', 'WMB', 'WMT', 'WMB', 'WMT', 'XOM', 'ZTS'],\n",
       "      dtype='object', length=296)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_symbols2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb6303c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STRATEGY CONFIGURATION ---\n",
    "# This dictionary now controls the entire strategy.\n",
    "\n",
    "# Example 1: 0DTE At-the-Money LONG STRADDLE\n",
    "strategy_config = {\n",
    "    'Leg1': {'action': 'SELL', 'option_type': 1, 'expiry_offset': 0, 'strike_offset': 5},  # Buy ATM Call\n",
    "    'Leg2': {'action': 'BUY', 'option_type': 1, 'expiry_offset': 1, 'strike_offset': 7}   # Buy ATM Put\n",
    "}\n",
    "\n",
    "# Example 2: 0DTE SHORT STRANGLE\n",
    "# strategy_config = {\n",
    "#     'Leg1': {'action': 'SELL', 'option_type': 1, 'expiry_offset': 0, 'strike_offset': 5}, # Sell OTM Call\n",
    "#     'Leg2': {'action': 'SELL', 'option_type': 0, 'expiry_offset': 0, 'strike_offset': -5} # Sell OTM Put\n",
    "# }\n",
    "\n",
    "# This defines the value of one strike step (e.g., for SPY, strikes are 1 point apart)\n",
    "strike_interval = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaeb8dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total symbols found after all filtering:  2\n",
      "Calculating 3-day MA for 2 symbols...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ifol = r\"C:\\Data\\Aggregate_Data\\Options\\USA\\SPY_1min_2024_2025_ORATS_close.parquet\"\n",
    "#== read the RELIANCE data\n",
    "\n",
    "com_df2 = pd.read_parquet(ifol)\n",
    "\n",
    "com_df2 = com_df2.drop_duplicates(keep='first')\n",
    "\n",
    "com_df2.dropna(how='all', inplace=True)\n",
    "\n",
    "vix_df = pd.read_parquet(r\"C:\\Data\\Aggregate_Data\\VIX_1_min_2019_2025.parquet\")\n",
    "com_df2 = com_df2.join(vix_df)\n",
    "\n",
    "\n",
    "final_symbols = com_df2.filter(like='close').columns.str.replace('_close', '')\n",
    "print(\"Total symbols found after all filtering: \", len(final_symbols))\n",
    "\n",
    "# Calculate the number of rows in the DataFrame\n",
    "\n",
    "com_df2[[f\"{symbol}_intraday_low\" for symbol in final_symbols]] =com_df2[[f\"{symbol}_close\" for symbol in final_symbols]].apply(lambda x: intraday_low(com_df2, low_col=x.name,agg_func='min'))\n",
    "\n",
    "com_df2[[f\"{symbol}_intraday_high\" for symbol in final_symbols]] =com_df2[[f\"{symbol}_close\" for symbol in final_symbols]].apply(lambda x: intraday_high(com_df2, high_col=x.name,agg_func='max'))\n",
    "\n",
    "com_df2[[f\"{symbol}_todayo\" for symbol in final_symbols]] = com_df2[[f'{symbol}_close' for symbol in final_symbols]].resample('D').transform('first')\n",
    "\n",
    "# com_df2[[f\"{symbol}_daily_ROC\" for symbol in final_symbols]] = com_df2[[f\"{symbol}_close\" for symbol in final_symbols]].pct_change(periods=50)\n",
    "\n",
    "# com_df2[[f\"{symbol}_ROC\" for symbol in final_symbols]] = com_df2[[f\"{symbol}_close\" for symbol in final_symbols]].pct_change()\n",
    "\n",
    "# com_df2[[f\"{symbol}_ROC_4\" for symbol in final_symbols]] = com_df2[[f\"{symbol}_close\" for symbol in final_symbols]].pct_change(periods=4)\n",
    "\n",
    "com_df2[[f\"{symbol}_prevdayl\" for symbol in final_symbols]] = com_df2[[f\"{symbol}_close\" for symbol in final_symbols]].apply(lambda x: get_x_day_low_numba(com_df2, n=1, column=x.name))\n",
    "\n",
    "com_df2[[f\"{symbol}_prevdayh\" for symbol in final_symbols]] = com_df2[[f\"{symbol}_close\" for symbol in final_symbols]].apply(lambda x: get_x_day_high_numba(com_df2, n=1, column=x.name))\n",
    "\n",
    "com_df2[[f\"{symbol}_secdayh\" for symbol in final_symbols]] = com_df2[[f\"{symbol}_close\" for symbol in final_symbols]].apply(lambda x: get_x_day_high_numba(com_df2, n=2, column=x.name))\n",
    "\n",
    "\n",
    "# This single call does it for the 20-day MA\n",
    "ma_20d_df = calculate_daily_ma_vectorized(df=com_df2, symbols=final_symbols, periods=3 , agg_func='last' , col_suffix='close')\n",
    "\n",
    "    # Join the new features back to the main DataFrame\n",
    "com_df2 = pd.concat([com_df2, ma_20d_df] , axis=1 , join='outer')\n",
    "\n",
    "\n",
    "com_df2[[f\"{symbol}_ma\" for symbol in final_symbols]] = com_df2[[f\"{symbol}_close\" for symbol in final_symbols]].rolling(2).mean()\n",
    "\n",
    "com_df2[[f\"{symbol}_ROC_30\" for symbol in final_symbols]] = com_df2[[f\"{symbol}_ma\" for symbol in final_symbols]].pct_change(periods=30)\n",
    "\n",
    "\n",
    "#com_df2[[f\"{symbol}_ma_3d_low\" for symbol in final_symbols]] = com_df2[[f\"{symbol}_ma\" for symbol in final_symbols]].values / com_df2[[f\"{symbol}_3d_MA\" for symbol in final_symbols]].values\n",
    "\n",
    "com_df2[[f\"{symbol}_prevdayc\" for symbol in final_symbols]] = com_df2[[f\"{symbol}_close\" for symbol in final_symbols]].apply(lambda x: calculate_prev_day_close(com_df2, close_col=x.name, n_days=1))\n",
    "\n",
    "com_df2[[f\"{symbol}_secdayc\" for symbol in final_symbols]] = com_df2[[f\"{symbol}_close\" for symbol in final_symbols]].apply(lambda x: calculate_prev_day_close(com_df2, close_col=x.name, n_days=2))\n",
    "\n",
    "\n",
    "# #=== hourly volume\n",
    "# com_df2[[f\"{symbol}_vol_ma\" for symbol in final_symbols]] = com_df2[[f\"{symbol}_volume\" for symbol in final_symbols]].rolling(4).mean()\n",
    "\n",
    "# #==avg 3 day volume\n",
    "\n",
    "# com_df2[[f\"{symbol}_3d_vol_MA\" for symbol in final_symbols]] = com_df2[[f\"{symbol}_volume\" for symbol in final_symbols]].apply(lambda x: daily_moving_average(com_df2, timeframe='1D', column=x.name, periods=3, agg_func='last'))\n",
    "\n",
    "com_df2[[f\"{symbol}_ma2_low\" for symbol in final_symbols]] = com_df2[[f\"{symbol}_close\" for symbol in final_symbols]].rolling(5).mean()\n",
    "\n",
    "\n",
    "com_df2[[f\"{symbol}_3dlow\" for symbol in final_symbols]] = com_df2[[f\"{symbol}_ma2_low\" for symbol in final_symbols]].apply(lambda x: get_x_day_low_numba(com_df2, n=3, column=x.name))\n",
    "\n",
    "com_df2[[f\"{symbol}_3dhigh\" for symbol in final_symbols]] = com_df2[[f\"{symbol}_ma2_low\" for symbol in final_symbols]].apply(lambda x: get_x_day_high_numba(com_df2, n=3, column=x.name))\n",
    "\n",
    "\n",
    "\n",
    "# 1. Get the lists of column names\n",
    "high_3d_cols = [f\"{symbol}_3dhigh\" for symbol in final_symbols]\n",
    "low_3d_cols = [f\"{symbol}_3dlow\" for symbol in final_symbols]\n",
    "\n",
    "# 2. Perform the calculation on NumPy arrays\n",
    "range_values = (com_df2[high_3d_cols].values / com_df2[low_3d_cols].values) - 1\n",
    "\n",
    "# 3. Create a new DataFrame from the results\n",
    "# This is the key step to avoid the error.\n",
    "range_cols = [f\"{symbol}_3d_rng\" for symbol in final_symbols]\n",
    "range_df = pd.DataFrame(range_values, index=com_df2.index, columns=range_cols)\n",
    "\n",
    "# 4. Join the new DataFrame back to the main one\n",
    "com_df2 = com_df2.join(range_df)\n",
    "\n",
    "\n",
    "\n",
    "# # Calculate highest high of last 4 bars\n",
    "# com_df2[[f\"{symbol}_high_4bar\" for symbol in final_symbols]] = com_df2[[f\"{symbol}_close\" for symbol in final_symbols]].rolling(10).max()\n",
    "\n",
    "# # Calculate lowest low of last 4 bars\n",
    "# com_df2[[f\"{symbol}_low_4bar\" for symbol in final_symbols]] = com_df2[[f\"{symbol}_close\" for symbol in final_symbols]].rolling(10).min()\n",
    "\n",
    "# # 1. Get the lists of column names\n",
    "# high_cols = [f\"{symbol}_high_4bar\" for symbol in final_symbols]\n",
    "# low_cols = [f\"{symbol}_low_4bar\" for symbol in final_symbols]\n",
    "\n",
    "# # 2. Perform the calculation on the NumPy arrays\n",
    "# range_values = com_df2[high_cols].values - com_df2[low_cols].values\n",
    "\n",
    "# # 3. Create a new DataFrame from the result (This is the key step)\n",
    "# range_cols = [f\"{symbol}_range_4bar\" for symbol in final_symbols]\n",
    "# range_df = pd.DataFrame(range_values, index=com_df2.index, columns=range_cols)\n",
    "\n",
    "# # 4. Join the new DataFrame back to the main one\n",
    "# com_df2 = com_df2.join(range_df)\n",
    "\n",
    "\n",
    "# # Calculate mean range (average of the 4-bar ranges)\n",
    "# com_df2[[f\"{symbol}_mean_range_4bar\" for symbol in final_symbols]] = com_df2[[f\"{symbol}_range_4bar\" for symbol in final_symbols]].rolling(12).mean()\n",
    "\n",
    "\n",
    "# gc.collect()\n",
    "\n",
    "# for symbol in final_symbols:\n",
    "\n",
    "   \n",
    "#     com_df2[f'{symbol}_iATR2'] = com_df2[f\"{symbol}_ATR\"].values / com_df2[f\"{symbol}_close\"].values\n",
    "\n",
    "\n",
    "# 1. Define the column groups\n",
    "close_cols = [f\"{symbol}_close\" for symbol in final_symbols]\n",
    "low_3d_cols = [f\"{symbol}_3dlow\" for symbol in final_symbols]\n",
    "high_3d_cols = [f\"{symbol}_3dhigh\" for symbol in final_symbols]\n",
    "\n",
    "# 2. Perform the calculation in a more chained and memory-conscious way\n",
    "# Using .to_numpy(dtype=np.float32) can cut memory usage by 50%\n",
    "numerator = com_df2[close_cols].to_numpy(dtype=np.float32) - com_df2[low_3d_cols].to_numpy(dtype=np.float32)\n",
    "denominator = com_df2[high_3d_cols].to_numpy(dtype=np.float32) - com_df2[low_3d_cols].to_numpy(dtype=np.float32)\n",
    "\n",
    "# np.divide handles division by zero safely\n",
    "percentile_values = np.divide(numerator, denominator, where=denominator != 0)\n",
    "\n",
    "# 3. Create the new feature DataFrame\n",
    "pctl_cols = [f\"{symbol}_3d_Pctl\" for symbol in final_symbols]\n",
    "percentile_df = pd.DataFrame(percentile_values, index=com_df2.index, columns=pctl_cols)\n",
    "\n",
    "# 4. Clip values and join back (same as your code)\n",
    "percentile_df = percentile_df.clip(0, 1)\n",
    "com_df2 = com_df2.join(percentile_df)\n",
    "\n",
    "\n",
    "com_df2['decent'] = np.where((com_df2['SPY_todayo'] < com_df2['SPY_prevdayc'] * 1.005) & (com_df2['SPY_todayo'] > com_df2['SPY_prevdayc'] * 0.98), 1, 0)\n",
    "com_df = com_df2[com_df2['decent']==1]\n",
    "com_df = com_df[com_df.index.year>=2024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bf0283",
   "metadata": {},
   "outputs": [],
   "source": [
    "com_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bcba291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfc3e4439aa34fe3864e580ef06766ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Found Data for Date Range ---\n",
      "                          strike     expiry  dte  optionType        volume  \\\n",
      "ts                                                                           \n",
      "2024-04-10 09:30:00 510.00000000 2024-04-12    3           1 -700.00000000   \n",
      "2024-04-10 09:31:00 510.00000000 2024-04-12    3           1    8.00000000   \n",
      "2024-04-10 09:32:00 510.00000000 2024-04-12    3           1    0.00000000   \n",
      "2024-04-10 09:33:00 510.00000000 2024-04-12    3           1   36.00000000   \n",
      "2024-04-10 09:34:00 510.00000000 2024-04-12    3           1    8.00000000   \n",
      "...                          ...        ...  ...         ...           ...   \n",
      "2024-04-12 15:56:00 510.00000000 2024-04-12    1           1 1958.00000000   \n",
      "2024-04-12 15:57:00 510.00000000 2024-04-12    1           1  695.00000000   \n",
      "2024-04-12 15:58:00 510.00000000 2024-04-12    1           1  604.00000000   \n",
      "2024-04-12 15:59:00 510.00000000 2024-04-12    1           1  609.00000000   \n",
      "2024-04-12 16:00:00 510.00000000 2024-04-12    1           1 1229.00000000   \n",
      "\n",
      "                               oi       close    bidPrice    askPrice  \\\n",
      "ts                                                                      \n",
      "2024-04-10 09:30:00    0.00000000 11.00000000 10.94000000 11.06000000   \n",
      "2024-04-10 09:31:00 3013.00000000  5.46500000  5.42000000  5.51000000   \n",
      "2024-04-10 09:32:00 3013.00000000  5.27000000  5.23000000  5.31000000   \n",
      "2024-04-10 09:33:00 3013.00000000  5.53000000  5.50000000  5.56000000   \n",
      "2024-04-10 09:34:00 3013.00000000  5.59000000  5.56000000  5.62000000   \n",
      "...                           ...         ...         ...         ...   \n",
      "2024-04-12 15:56:00 2692.00000000  0.67500000  0.66000000  0.69000000   \n",
      "2024-04-12 15:57:00 2692.00000000  0.68500000  0.67000000  0.70000000   \n",
      "2024-04-12 15:58:00 2692.00000000  0.69000000  0.67000000  0.71000000   \n",
      "2024-04-12 15:59:00 2692.00000000  0.73000000  0.70000000  0.76000000   \n",
      "2024-04-12 16:00:00 2692.00000000  0.81500000  0.78000000  0.85000000   \n",
      "\n",
      "                            iv   stockPrice ticker  \n",
      "ts                                                  \n",
      "2024-04-10 09:30:00 0.57730700 513.62000000    SPY  \n",
      "2024-04-10 09:31:00 0.20712800 513.27000000    SPY  \n",
      "2024-04-10 09:32:00 0.20801600 512.96000000    SPY  \n",
      "2024-04-10 09:33:00 0.20310400 513.45000000    SPY  \n",
      "2024-04-10 09:34:00 0.21444500 513.30000000    SPY  \n",
      "...                        ...          ...    ...  \n",
      "2024-04-12 15:56:00 0.15471300 510.50000000    SPY  \n",
      "2024-04-12 15:57:00 0.15918100 510.50000000    SPY  \n",
      "2024-04-12 15:58:00 0.15793700 510.53000000    SPY  \n",
      "2024-04-12 15:59:00 0.16718600 510.56000000    SPY  \n",
      "2024-04-12 16:00:00 0.15913800 510.70000000    SPY  \n",
      "\n",
      "[1172 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas_market_calendars as mcal\n",
    "\n",
    "def fetch_option_data_for_n_days(con, ticker, end_date_str, n_days, strike, expiry_str, option_type):\n",
    "    \"\"\"\n",
    "    Fetches a range of minute-level data for a single option contract\n",
    "    for a specified number of business days ending on a given date.\n",
    "\n",
    "    Args:\n",
    "        con: The DuckDB connection object.\n",
    "        ticker (str): The stock ticker (e.g., 'SPY').\n",
    "        end_date_str (str): The last day to query, in 'YYYY-MM-DD' format.\n",
    "        n_days (int): The number of business days of data to fetch.\n",
    "        strike (float): The specific strike price.\n",
    "        expiry_str (str): The expiry date to filter for, in 'YYYY-MM-DD' format.\n",
    "        option_type (int): The type of option to fetch (1 for Calls, 0 for Puts).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the minute-by-minute data.\n",
    "    \"\"\"\n",
    "    option_name = \"Call\" if option_type == 1 else \"Put\"\n",
    "    #print(f\"Fetching last {n_days} b-days of data ending on {end_date_str} for {ticker} {strike} {option_name} expiring {expiry_str}...\")\n",
    "    \n",
    "    try:\n",
    "        # --- Business Day Calculation Logic ---\n",
    "        # 1. Get the market calendar\n",
    "        nyse = mcal.get_calendar('NYSE')\n",
    "        \n",
    "        # 2. To be safe, find a start date far enough in the past\n",
    "        #    (n * 2 is a safe buffer for weekends/holidays)\n",
    "        start_buffer = pd.to_datetime(end_date_str) - pd.Timedelta(days=n_days * 2)\n",
    "        \n",
    "        # 3. Generate a schedule of valid trading days\n",
    "        schedule = nyse.schedule(start_date=start_buffer, end_date=end_date_str)\n",
    "        \n",
    "        # 4. Get the last 'n_days' from the schedule and format them\n",
    "        business_day_list = [d.strftime('%Y-%m-%d') for d in schedule.index[-n_days:]]\n",
    "        \n",
    "        # --- Query Logic (remains the same) ---\n",
    "        path_list = [\n",
    "            f\"'s3://duckdata/ORATS/Options/ticker={ticker}/day={d}/*.parquet'\"\n",
    "            for d in business_day_list\n",
    "        ]\n",
    "        \n",
    "        query = f\"\"\"\n",
    "            SELECT ts, strike, expiry, dte, optionType, volume, oi, close,\n",
    "                bidPrice, askPrice, iv, stockPrice, ticker\n",
    "            FROM read_parquet([{\",\".join(path_list)}])\n",
    "            WHERE \n",
    "                strike = {strike} AND\n",
    "                expiry = '{expiry_str}' AND\n",
    "                optionType = {option_type}\n",
    "            ORDER BY ts;\n",
    "        \"\"\"\n",
    "        \n",
    "        multi_day_df = con.execute(query).df()\n",
    "        \n",
    "        if multi_day_df.empty:\n",
    "            print(\"INFO: No data found for this contract in the specified date range.\")\n",
    "        \n",
    "        multi_day_df['ts'] = pd.to_datetime(multi_day_df['ts'].dt.tz_localize('UTC').dt.tz_convert('America/New_York').dt.tz_localize(None))\n",
    "        multi_day_df.set_index('ts' , inplace=True)\n",
    "        return multi_day_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# ==============================================================================\n",
    "# --- Example Usage ---\n",
    "# ==============================================================================\n",
    "\n",
    "# Assume 'con' is your active DuckDB connection\n",
    "\n",
    "# Get data for the 5 business days ending on September 12, 2025\n",
    "multi_day_df = fetch_option_data_for_n_days(\n",
    "    con=con,\n",
    "    ticker='SPY',\n",
    "    end_date_str='2024-04-12', # The last day of the range\n",
    "    n_days=3,                  # How many business days to go back\n",
    "    strike=510.0,\n",
    "    expiry_str='2024-04-12',\n",
    "    option_type=1              # 1 for Call\n",
    ")\n",
    "\n",
    "if not multi_day_df.empty:\n",
    "    print(\"\\n--- Found Data for Date Range ---\")\n",
    "    print(multi_day_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce934cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_option_data_for_n_days_MOST_RELIABLE(con, ticker, end_date_str, n_days, strike, expiry_str, option_type):\n",
    "    try:\n",
    "        # --- Use mcal for the most accurate date calculation ---\n",
    "        nyse = mcal.get_calendar('NYSE')\n",
    "        schedule = nyse.schedule(start_date=pd.to_datetime(end_date_str) - pd.Timedelta(days=n_days * 2), end_date=end_date_str)\n",
    "        if len(schedule) < n_days:\n",
    "            raise ValueError(\"Not enough historical trading days in the calendar for the requested n_days.\")\n",
    "        \n",
    "        # Get the precise start date from the calendar\n",
    "        start_date_str = schedule.index[-n_days].strftime('%Y-%m-%d')\n",
    "\n",
    "        # --- Use the fast, optimized DuckDB query ---\n",
    "        s3_path = f's3://duckdata/ORATS/Options/ticker={ticker}/day=*/*.parquet'\n",
    "        query = f\"\"\"\n",
    "            SELECT ts, strike, expiry, dte, optionType, volume, oi, close,\n",
    "                bidPrice, askPrice, iv, stockPrice, ticker\n",
    "            FROM read_parquet('{s3_path}', hive_partitioning = 1)\n",
    "            WHERE day >= '{start_date_str}' AND day <= '{end_date_str}'\n",
    "              AND strike = {strike} AND expiry = '{expiry_str}' AND optionType = {option_type}\n",
    "            ORDER BY ts;\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        multi_day_df = con.execute(query).df()\n",
    "\n",
    "        if multi_day_df.empty:\n",
    "            print(\"INFO: No data found for this contract in the specified date range.\")\n",
    "        \n",
    "        multi_day_df['ts'] = pd.to_datetime(multi_day_df['ts'].dt.tz_localize('UTC').dt.tz_convert('America/New_York').dt.tz_localize(None))\n",
    "        multi_day_df.set_index('ts' , inplace=True)\n",
    "        return multi_day_df\n",
    "\n",
    "        # ... (rest of the function remains the same) ...\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159f5d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "com_df.index.normalize().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7e7aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get data for the 5 business days ending on September 12, 2025\n",
    "multi_day_df = fetch_option_data_for_n_days_MOST_RELIABLE(\n",
    "    con=con,\n",
    "    ticker='SPY',\n",
    "    end_date_str='2024-09-12', # The last day of the range\n",
    "    n_days=3,                  # How many business days to go back\n",
    "    strike=510.0,\n",
    "    expiry_str='2024-09-12',\n",
    "    option_type=1              # 1 for Call\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e905d616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit \n",
    "# --- 2. Test Parameters ---\n",
    "test_dates = pd.to_datetime([\n",
    "    '2024-09-12', '2024-09-13', '2024-09-16', '2024-09-17', '2024-09-18'\n",
    "])\n",
    "test_params = {\n",
    "    'ticker': 'SPY',\n",
    "    'n_days': 5,\n",
    "    'strike': 520.0,\n",
    "    'expiry_str': '2024-09-19',\n",
    "    'option_type': 1\n",
    "}\n",
    "\n",
    "original_times = []\n",
    "reliable_times = []\n",
    "\n",
    "\n",
    "# --- 3. Loop through dates and time each function ---\n",
    "for date in test_dates:\n",
    "    date_str = date.strftime('%Y-%m-%d')\n",
    "    print(f\"\\nTesting for end_date: {date_str}\")\n",
    "\n",
    "\n",
    "    # --- Time the MOST_RELIABLE function using timeit.default_timer() ---\n",
    "    print(\"  -> Timing MOST_RELIABLE (Hive) function...\")\n",
    "    start_time = timeit.default_timer()\n",
    "    df2 = fetch_option_data_for_n_days_MOST_RELIABLE(con=con, end_date_str=date_str, **test_params)\n",
    "    duration = timeit.default_timer() - start_time\n",
    "    reliable_times.append(duration)\n",
    "    print(f\"     Done in {duration:.4f} seconds. Found {len(df2)} rows.\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # --- Time the ORIGINAL function using timeit.default_timer() ---\n",
    "    print(\"  -> Timing Original function...\")\n",
    "    start_time = timeit.default_timer()\n",
    "    df1 = fetch_option_data_for_n_days(con=con, end_date_str=date_str, **test_params)\n",
    "    duration = timeit.default_timer() - start_time\n",
    "    original_times.append(duration)\n",
    "    print(f\"     Done in {duration:.4f} seconds. Found {len(df1)} rows.\")\n",
    "# --- 4. Print Summary ---\n",
    "avg_original = np.mean(original_times)\n",
    "avg_reliable = np.mean(reliable_times)\n",
    "\n",
    "print(\"\\n--- ‚è±Ô∏è BENCHMARK COMPLETE ---\")\n",
    "print(f\"üê¢ Original Function Average Time:   {avg_original:.4f} seconds\")\n",
    "print(f\"üöÄ MOST_RELIABLE Function Average Time: {avg_reliable:.4f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1624541e",
   "metadata": {},
   "outputs": [],
   "source": [
    "strike_interval = 1\n",
    "lookback_days = 3\n",
    "option_multiplier = 100\n",
    "from collections import  defaultdict\n",
    "\n",
    "# ==============================================================================\n",
    "# SCRIPT STARTS HERE\n",
    "# ==============================================================================\n",
    "\n",
    "trade_log = []\n",
    "nyse = mcal.get_calendar('NYSE') \n",
    "unique_days = com_df.index.normalize().unique()\n",
    "print(f\"üöÄ Starting backtest for {len(unique_days)} unique days...\")\n",
    "\n",
    "intial_capital = 100000\n",
    "position_cap = 100000\n",
    "\n",
    "# Track local trade log\n",
    "local_trade_log2 = pd.DataFrame(columns=['name', 'Side', 'entry_date', 'entry_price' , 'qty' ,  'exit_date', 'exit_price', 'order_no','profit' ])\n",
    "\n",
    "# ‚úÖ Initialize a list to hold all trade dictionaries.\n",
    "final_trade_log_list = []\n",
    "\n",
    "# --- Main Loop: Iterate Through Each Day ---\n",
    "for day in unique_days[2:7]:\n",
    "    \n",
    "    day_str = day.strftime('%Y-%m-%d')\n",
    "    print(f\"\\n--- Processing Day: {day_str} ---\")\n",
    "\n",
    "    try:\n",
    "        \n",
    "        daily_underlying_df = com_df[com_df.index.date == day.date()]\n",
    "        if daily_underlying_df.empty: continue\n",
    "\n",
    "        open_price = daily_underlying_df['SPY_close'].iloc[0]\n",
    "        if pd.isna(open_price): continue\n",
    "        \n",
    "        atm_strike = np.round(open_price)\n",
    "        #print(f\"  STEP 1: ATM Strike for the day set to {atm_strike}\")\n",
    "\n",
    "        # --- Dynamically Build and Fetch Data Based on Config ---\n",
    "        all_legs_hist_data = {}\n",
    "        fetch_success = True\n",
    "        legs_to_trade = []\n",
    "\n",
    "        for leg_name, params in strategy_config.items():\n",
    "            \n",
    "            target_strike = atm_strike + (params['strike_offset'] * strike_interval)\n",
    "\n",
    "            print(f\"Atm strike is {atm_strike} and taegt strike is {target_strike} and optiontyoe is {params['option_type']}\")\n",
    "            schedule = nyse.schedule(start_date=day, end_date=day + pd.Timedelta(days=14))\n",
    "            target_expiry_str = schedule.index[params['expiry_offset']].strftime('%Y-%m-%d')\n",
    "            print(f\"Traget expiry for {leg_name} and {day_str} is {target_expiry_str} \")\n",
    "            legs_to_trade.append({\n",
    "                'name': leg_name, 'strike': target_strike,\n",
    "                'expiry': target_expiry_str, 'type': params['option_type']})\n",
    "            \n",
    "            leg_hist_df = fetch_option_data_for_n_days(\n",
    "                con=con, ticker='SPY', end_date_str=day_str, n_days=lookback_days,\n",
    "                strike=target_strike, expiry_str=target_expiry_str, option_type=params['option_type'] )\n",
    "                    \n",
    "            # ‚úÖ ADD THIS LINE TO DEBUG:\n",
    "            #print(f\"Columns found for {leg_name}: {leg_hist_df.columns.tolist()}\")\n",
    "\n",
    "            if not leg_hist_df.empty:\n",
    "                # Find rows where both bidPrice and askPrice are 0\n",
    "                zero_rows = leg_hist_df[ (leg_hist_df['bidPrice'] == 0) & (leg_hist_df['askPrice'] == 0) ]\n",
    "                zero_count = len(zero_rows)\n",
    "                total_rows = len(leg_hist_df)\n",
    "                \n",
    "                #print(f\"  üìä {leg_name}: {zero_count}/{total_rows} rows have both bid & ask = 0\")\n",
    "                # One-liner: Replace zeros with NaN and forward fill for both columns\n",
    "                leg_hist_df[['bidPrice', 'askPrice']] = leg_hist_df[['bidPrice', 'askPrice']].replace(0, np.nan).ffill()\n",
    "                \n",
    "                if zero_count > 1:\n",
    "                    print(f\"  ‚ö†Ô∏è  {zero_count} Rows with zero prices for {leg_name}:\")\n",
    "\n",
    "                    if leg_hist_df.empty:\n",
    "                        print(f\"  ‚ö†Ô∏è Could not fetch data for {leg_name}. Skipping day.\")\n",
    "                        fetch_success = False\n",
    "                        break\n",
    "                all_legs_hist_data[leg_name] = leg_hist_df\n",
    "            else:\n",
    "                print(f\"No options data found for {date_str}\")\n",
    "        if not fetch_success: \n",
    "            print(f\"Not able to fetch options for {day} \")\n",
    "            continue\n",
    "\n",
    "        # --- Merge and Prepare Data ---\n",
    "        prefixed_dfs_today = []\n",
    "        for leg_name, leg_hist_df in all_legs_hist_data.items():\n",
    "            \n",
    "            prefixed_dfs_today.append(leg_hist_df.add_prefix(f'{leg_name}_'))\n",
    "       \n",
    "        # 1. First, concatenate all the prefixed option leg DataFrames together\n",
    "        com_legs_df = pd.concat(prefixed_dfs_today, axis=1, join='outer')\n",
    "        com_legs_df = com_legs_df[com_legs_df.index.time>time(9 , 30)]\n",
    "        \n",
    "        if ( len(com_legs_df)<10):\n",
    "            print(\"\\n\\n =============\")\n",
    "            print(f\"Leg data very less for {date_str}\")\n",
    "\n",
    "\n",
    "        #==== now do all calculations and indicators on this\n",
    "        legs_to_process = ['Leg1', 'Leg2']\n",
    "                \n",
    "        com_legs_df[[f\"{legs}_intraday_low\" for legs in legs_to_process]] = com_legs_df[[f\"{legs}_bidPrice\" for legs in legs_to_process]].apply(lambda x: intraday_low(com_legs_df, low_col=x.name,agg_func='min'))\n",
    "\n",
    "        com_legs_df[[f\"{legs}_intraday_high\" for legs in legs_to_process]] = com_legs_df[[f\"{legs}_askPrice\" for legs in legs_to_process]].apply(lambda x: intraday_high(com_legs_df, high_col=x.name,agg_func='max'))\n",
    "\n",
    "\n",
    "        # Calculate Today's Opening Price for each leg\n",
    "        com_legs_df[[f'{leg}_todayo' for leg in legs_to_process]] =  com_legs_df[[f'{leg}_close' for leg in legs_to_process]].resample('D').transform('first')\n",
    "\n",
    "        # Calculate Previous Day's Low (using bidPrice) for each leg\n",
    "        com_legs_df[[f\"{leg}_prevdayl\" for leg in legs_to_process]] = com_legs_df[[f\"{leg}_bidPrice\" for leg in legs_to_process]].apply(lambda x: get_x_day_low_numba(com_legs_df, n=1, column=x.name))\n",
    "\n",
    "        # Calculate Previous Day's High (using askPrice) for each leg\n",
    "        com_legs_df[[f\"{leg}_prevdayh\" for leg in legs_to_process]] = \\\n",
    "            com_legs_df[[f\"{leg}_askPrice\" for leg in legs_to_process]].apply(lambda x: get_x_day_high_numba(com_legs_df, n=1, column=x.name) )\n",
    "\n",
    "        close_cols = [f'{leg}_close' for leg in legs_to_process]\n",
    "        window_size = 15\n",
    "\n",
    "        # # 1. Rolling 15-Bar Highest Close\n",
    "        # com_legs_df[[f'{leg}_15min_high' for leg in legs_to_process]] = \\\n",
    "        #     com_legs_df[close_cols].rolling(window=window_size).max()\n",
    "\n",
    "        # # 2. Rolling 15-Bar Lowest Close\n",
    "        # com_legs_df[[f'{leg}_15min_low' for leg in legs_to_process]] = \\\n",
    "        #     com_legs_df[close_cols].rolling(window=window_size).min()\n",
    "\n",
    "        # # 3. Rolling 15-Bar Mean Close\n",
    "        # com_legs_df[[f'{leg}_15min_mean' for leg in legs_to_process]] = \\\n",
    "        #     com_legs_df[close_cols].rolling(window=window_size).mean()\n",
    "\n",
    "        com_legs_df[[f'{leg}_ROC_5' for leg in legs_to_process]] = com_legs_df[[f\"{leg}_close\" for leg in legs_to_process]].pct_change(periods=5)\n",
    "#=== merge underlying back with com_legs_df\n",
    "\n",
    "        com_df3 = pd.concat([daily_underlying_df , com_legs_df] , join='outer' , axis=1)\n",
    "\n",
    "        # --- FIX: Add this line to filter for the current day ---\n",
    "        com_df3 = com_df3[com_df3.index.date == pd.to_datetime(day_str).date()]\n",
    "\n",
    "         # Initialize state for this day's trading (if needed)\n",
    "        instrument_positions = {leg: False for leg in legs_to_process}\n",
    "        entry_prices = {leg: None for leg in legs_to_process}\n",
    "        # ... (add any other state variables you need: qty, entry_dates, etc.) ...\n",
    "       \n",
    "        entry_dates = {leg: None for leg in legs_to_process}\n",
    "                \n",
    "        # To track the current drawdown value for each active leg\n",
    "        drawdown = {leg: None for leg in legs_to_process}\n",
    "\n",
    "        # To track P&L for each leg (can be redundant if using series)\n",
    "        symbol_pnl = {leg: None for leg in legs_to_process}\n",
    "\n",
    "        # To track the maximum drawdown seen for each leg\n",
    "        max_drawdown = {leg: 0 for leg in legs_to_process}\n",
    "\n",
    "        # To count how many bars an active position has been held\n",
    "        bar_count = {leg: 0 for leg in legs_to_process}\n",
    "\n",
    "        # To store the quantity/number of contracts for each leg\n",
    "        qty = {leg: 0 for leg in legs_to_process}\n",
    "\n",
    "        # To store an order identifier for each leg's trade\n",
    "        order_no = {leg: 0 for leg in legs_to_process}\n",
    "\n",
    "        # To store the long/short direction (e.g., 1 for long, -1 for short)\n",
    "        position_type = {leg: 0 for leg in legs_to_process}\n",
    "\n",
    "        # To store the drawdown history for each leg\n",
    "        drawdown_series = {leg: [] for leg in legs_to_process}\n",
    "\n",
    "        # To store the P&L history for each leg\n",
    "        symbol_pnl_series = {leg: [] for leg in legs_to_process}\n",
    "\n",
    "        # To store another P&L history (can be consolidated with the one above)\n",
    "        absolute_symbol_pnl_series = {leg: [] for leg in legs_to_process}\n",
    "        # ===================================================================\n",
    "        # --- Bar-by-Bar Simulation Loop using for i in range() ---\n",
    "        # ===================================================================\n",
    "        daily_trade = {'legs': {}}\n",
    "        print(\"  STEP 3: Starting bar-by-bar simulation...\")\n",
    "        \n",
    "        logger.info(f\"Running analysis on: {intial_capital} of capital\")\n",
    "        cumulative_realized_pnl = 0.0\n",
    "\n",
    "\n",
    "        # === Exposure & portfolio trackers ===\n",
    "        exposure = 0.0        # net exposure\n",
    "        gross_exposure = 0.0  # absolute exposure\n",
    "\n",
    "        exposure_series = {}          # net exposure at each timestamp\n",
    "        gross_exposure_series = {}    # absolute exposure\n",
    "        portfolio_value_series = {}   # portfolio equity curve\n",
    "        \n",
    "        # Initialize before main loop\n",
    "        total_pnl_series = defaultdict(float)\n",
    "        # Process each row in time order\n",
    "\n",
    "        for i in range(len(com_df3)):\n",
    "\n",
    "            row = com_df3.iloc[i]\n",
    "            timestamp = com_df3.index[i]\n",
    "\n",
    "            exposure = 0.0\n",
    "            gross_exposure = 0.0\n",
    "            # Initialize the unrealized P&L for the current bar to zero\n",
    "            unrealized_pnl_for_this_bar = 0.0\n",
    "\n",
    "            if i == 0:\n",
    "                in_position = 0\n",
    "                current_pnl = 0\n",
    "                exposure_series[timestamp] = 0.0\n",
    "                gross_exposure_series[timestamp] = 0.0\n",
    "                portfolio_value_series[timestamp] = intial_capital  # starting capital      \n",
    "                \n",
    "                total_pnl_series[timestamp] = 0.0  \n",
    "\n",
    "            else:\n",
    "                prev_timestamp = com_df3.index[i - 1]\n",
    "                \n",
    "                #=== long and short position based on previous bar\n",
    "\n",
    "                # Get previous PnL value\n",
    "                current_pnl = total_pnl_series.get(prev_timestamp, 0.0)\n",
    "\n",
    "\n",
    "            for legs in legs_to_process:\n",
    "               \n",
    "                if f'{legs}_close' not in row:\n",
    "\n",
    "                        #logger.warning(f\"Data not found for {symbol} at {timestamp}\")\n",
    "                        continue  # Skip if data for this instrument is missing\n",
    "                # Fetch the instrument's specific close price\n",
    "                close_price = row[f'{legs}_close']\n",
    "                in_position = instrument_positions.get(legs , False)\n",
    "\n",
    "                if not in_position:\n",
    "                    #  and com_df3[\"VIX_close\"].iloc[i-1]<com_df3[\"VIX_prevdayc\"].iloc[i - 1]*0.98\n",
    "                    # and com_df3[\"VIX_close\"].iloc[i-1]<com_df3[\"VIX_prevdayc\"].iloc[i - 1]*1.02\n",
    "                    if( time(9 , 55)<= timestamp.time()<=time(13 , 15) and legs=='Leg1' and row[f\"{legs}_close\"]<1.5 and row[f\"{legs}_close\"]>0.35    and com_df3[f\"SPY_3d_Pctl\"].iloc[i-1]<0.8 and  com_df3[f\"SPY_3d_Pctl\"].iloc[i-1]>0.2  and row[f\"{legs}_close\"]<min((com_df3[f\"{legs}_intraday_high\"].iloc[i - 1]*0.9) , com_df3[f\"{legs}_close\"].iloc[i - 1]*0.999 )  and com_df3[f\"{legs}_ROC_5\"].iloc[i - 1]<0.1  ) :        \n",
    "\n",
    "                        com_df3.loc[timestamp, f'{legs}_action'] = 'sell'\n",
    "                        com_df3.loc[timestamp, f'{legs}_trade_price'] = row[f\"{legs}_close\"]\n",
    "                        com_df3.loc[timestamp, f'{legs}_qty'] = 10000\n",
    "                        com_df3.loc[timestamp, f'{legs}_entry_price'] = row[f\"{legs}_close\"]\n",
    "                        com_df3.loc[timestamp, f'{legs}_position'] = -1\n",
    "                        instrument_positions[legs] = True  # Track that this PT_address is now in position\n",
    "                        entry_prices[legs] = com_df3.loc[timestamp, f'{legs}_entry_price']\n",
    "                        #=== we note the hedge entry price and qty for the hedge position\n",
    "                        \n",
    "                        #==============================================================\n",
    "                        position_type[legs] = -1\n",
    "                        entry_dates[legs] = timestamp\n",
    "                        bar_count[legs] = 0\n",
    "                        qty[legs] = np.floor(position_cap/entry_prices[legs])\n",
    "                        \n",
    "                        print(f\"Short entry in {legs} at entry price {entry_prices[legs]} for qty: {qty[legs]} at {timestamp}\")\n",
    "                        #print(f\"Symbol rank is :{row[f\"{symbol}_close_S1_ratio_Rank\"]} and VIX is : {com_df[\"VIX_close\"].iloc[i]}  \" )\n",
    "\n",
    "                if position_type[legs]<0 and time(15 , 45) < timestamp.time() and entry_prices[legs]>=0    :\n",
    "                     #print(\"\\n Short Stop loss found for entry: \" , entry_prices[symbol])\n",
    "                    com_df3.loc[timestamp, f'{legs}_action'] = 'sell'\n",
    "                    com_df3.loc[timestamp, f'{legs}_trade_price'] = close_price\n",
    "                    com_df3.loc[timestamp, f'{legs}_exit_price'] = close_price\n",
    "                    profit = (entry_prices[legs] -com_df3.loc[timestamp, f'{legs}_exit_price'] ) * qty[legs]\n",
    "                    com_df3.loc[timestamp, f'{legs}_profit'] = profit\n",
    "                    com_df3.loc[timestamp, f'{legs}_position'] = 0  # Exit position\n",
    "                    instrument_positions[legs] = False  # Remove from active positions\n",
    "                    bar_count[legs] = 0\n",
    "                    order_no[legs] = -13\n",
    "\n",
    "                    profit = (entry_prices[legs] - close_price) * qty[legs] # Example for a long trade\n",
    "                    \n",
    "                    # 2. Append a dictionary to the list\n",
    "                    final_trade_log_list.append({\n",
    "                        'name': legs,\n",
    "                        'Side': 'sell',\n",
    "                        'entry_date': entry_dates[legs],\n",
    "                        'entry_price': entry_prices[legs],\n",
    "                        'qty': qty[legs],\n",
    "                        'exit_date': timestamp,\n",
    "                        'exit_price': close_price,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "\n",
    "                    # ‚úÖ Reset entry price after selling\n",
    "                    entry_prices[legs] = None\n",
    "                    #in_position = 0\n",
    "                    position_type[legs]=0\n",
    "                \n",
    "                if position_type[legs]<0 and time(10 , 5) < timestamp.time() and  row[f\"{legs}_close\"]>entry_prices[legs]+0.5    :\n",
    "                    print(\"\\n Short Stop loss found for entry: \" , entry_prices[legs])\n",
    "                    com_df3.loc[timestamp, f'{legs}_action'] = 'sell'\n",
    "                    com_df3.loc[timestamp, f'{legs}_trade_price'] = close_price\n",
    "                    com_df3.loc[timestamp, f'{legs}_exit_price'] = close_price\n",
    "                    profit = (entry_prices[legs] -com_df3.loc[timestamp, f'{legs}_exit_price'] ) * qty[legs]\n",
    "                    com_df3.loc[timestamp, f'{legs}_profit'] = profit\n",
    "                    com_df3.loc[timestamp, f'{legs}_position'] = 0  # Exit position\n",
    "                    instrument_positions[legs] = False  # Remove from active positions\n",
    "                    bar_count[legs] = 0\n",
    "                    order_no[legs] = -13\n",
    "\n",
    "                    profit = (entry_prices[legs] - close_price) * qty[legs] # Example for a long trade\n",
    "                    \n",
    "                    # 2. Append a dictionary to the list\n",
    "                    final_trade_log_list.append({\n",
    "                        'name': legs,\n",
    "                        'Side': 'sell',\n",
    "                        'entry_date': entry_dates[legs],\n",
    "                        'entry_price': entry_prices[legs],\n",
    "                        'qty': qty[legs],\n",
    "                        'exit_date': timestamp,\n",
    "                        'exit_price': close_price,\n",
    "                        'profit': profit\n",
    "                    })\n",
    "\n",
    "                    # ‚úÖ Reset entry price after selling\n",
    "                    entry_prices[legs] = None\n",
    "                    #in_position = 0\n",
    "                    position_type[legs]=0    \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå An error occurred on {day_str}: {e}\")\n",
    "    \n",
    "\n",
    "# (Final results analysis)\n",
    "log_df = pd.DataFrame(final_trade_log_list)\n",
    "print(\"\\n--- BACKTEST COMPLETE ---\")\n",
    "if not log_df.empty:\n",
    "    print(log_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20a7166",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_trade_log2 = log_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f0cfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4216171",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'entry_date' in local_trade_log2.columns:\n",
    "\n",
    "    local_trade_log2.set_index('entry_date' , inplace=True)\n",
    "\n",
    "    local_trade_log2.sort_index( ascending=True, inplace=True)\n",
    "\n",
    "if 'exit_date' in local_trade_log2.columns:\n",
    "    local_trade_log2['exit_date'] = pd.to_datetime(local_trade_log2['exit_date'])\n",
    "\n",
    "# Calculate the absolute percentage difference between entry and exit prices\n",
    "local_trade_log2['price_change_pct'] = (local_trade_log2['exit_price'] - local_trade_log2['entry_price']).abs() / local_trade_log2['entry_price']\n",
    "\n",
    "# Count trades before filtering\n",
    "trades_before_filtering = len(local_trade_log2)\n",
    "\n",
    "average_trade_profit = local_trade_log2['profit'].mean()\n",
    "\n",
    "# First ensure consistent case for Side column\n",
    "local_trade_log2['Side'] = local_trade_log2['Side'].str.lower()\n",
    "\n",
    "# Calculate profit in bps (handles both buy/long and sell/short)\n",
    "local_trade_log2['profit_bps'] = np.where(\n",
    "    local_trade_log2['Side'].isin(['buy', 'long']),\n",
    "    ((local_trade_log2['exit_price'] - local_trade_log2['entry_price']) / local_trade_log2['entry_price']) * 10000,\n",
    "    ((local_trade_log2['entry_price'] - local_trade_log2['exit_price']) / local_trade_log2['entry_price']) * 10000)\n",
    "\n",
    "# open_positions = list(long_position_count.values())\n",
    "# print(f\"Max open long positions are: {np.max(open_positions)}\")\n",
    "\n",
    "# open__short_positions = list(short_position_count.values())\n",
    "# print(f\"Max open shrt positions are: {np.max(open__short_positions)}\")\n",
    "\n",
    "total_trade_count = len(local_trade_log2)\n",
    "max_entries_per_day = local_trade_log2.groupby(local_trade_log2.index.date).size().max()\n",
    "\n",
    "# Identify losing trades and percentage\n",
    "\n",
    "losers = local_trade_log2[local_trade_log2['profit'] < 0]\n",
    "num_losing_trades = len(losers)\n",
    "losers_percentage = (num_losing_trades / total_trade_count) * 100 if total_trade_count > 0 else 0\n",
    "\n",
    "winners = local_trade_log2[local_trade_log2['profit'] > 0]\n",
    "\n",
    "# 1. Create a pandas Series from your PnL data\n",
    "pnl_series = pd.Series(total_pnl_series)\n",
    "\n",
    "# 2. Find the minimum value in the series\n",
    "max_unrealized_loss = pnl_series.min()\n",
    "\n",
    "print(f\"The highest unrealized loss (minimum PnL) was: ${max_unrealized_loss:,.2f}\")\n",
    "\n",
    "# Updated side checks throughout\n",
    "num_long_trades = len(local_trade_log2[local_trade_log2['Side'].isin(['buy', 'long'])])\n",
    "num_short_trades = len(local_trade_log2[local_trade_log2['Side'].isin(['sell', 'short'])])\n",
    "print(f\"Number of long trades: {num_long_trades} and short trades: {num_short_trades} out of total trades: {total_trade_count}\")\n",
    "\n",
    "# Updated profit calculations\n",
    "avg_long_profit_bps = local_trade_log2[local_trade_log2['Side'].isin(['buy', 'long'])]['profit_bps'].mean()\n",
    "avg_short_profit_bps = local_trade_log2[local_trade_log2['Side'].isin(['sell', 'short'])]['profit_bps'].mean()\n",
    "print(f\"Average long trade profit: {avg_long_profit_bps:.2f} bps and short: {avg_short_profit_bps:.2f} bps\")\n",
    "\n",
    "profit_total_long = local_trade_log2[local_trade_log2['Side'].isin(['buy', 'long'])]['profit'].sum()\n",
    "profit_total_short = local_trade_log2[local_trade_log2['Side'].isin(['sell', 'short'])]['profit'].sum()\n",
    "print(f\"Net long profit: {profit_total_long:.2f} and short profit: {profit_total_short:.2f}\")\n",
    "\n",
    "# Compute holding period\n",
    "local_trade_log2['holding_days'] = (local_trade_log2['exit_date'] - local_trade_log2.index).dt.days\n",
    "\n",
    "# Compute equity curve (EC)\n",
    "local_trade_log2['EC'] = (position_cap*100) + local_trade_log2['profit'].cumsum()\n",
    "\n",
    "#============================================\n",
    "\n",
    "# The index should be the 'exit_date' because that's when the equity changes\n",
    "equity_at_exit = pd.Series(data=local_trade_log2['EC'].values, index=pd.to_datetime(local_trade_log2['exit_date']))\n",
    "\n",
    "print(f\"Equity at exit is : {equity_at_exit}\")\n",
    "\n",
    "# Print average trade profit\n",
    "print(f\"Average Trade Profit: {average_trade_profit:.2f}\")\n",
    "\n",
    "# Print profit in basis points (bps)\n",
    "print(\"Profit (bps) for each trade:\")\n",
    "print(f\"Avergae trade profit in bps : {local_trade_log2['profit_bps'].mean()}\" )\n",
    "\n",
    "# Print total trade count\n",
    "print(f\"Total Trade Count: {total_trade_count}\")\n",
    "\n",
    "# Print max entries per day\n",
    "print(f\"Max Entries Per Day: {max_entries_per_day}\")\n",
    "\n",
    "# Print number of losing trades\n",
    "print(f\"Number of Losing Trades: {num_losing_trades}\")\n",
    "\n",
    "print(f\"Losing Trades Percentage: {losers_percentage:.2f}%\")\n",
    "\n",
    "# Print holding period\n",
    "print(\"Avg Holding Period in (Days) :\")\n",
    "print(local_trade_log2['holding_days'].mean())\n",
    "\n",
    "# Add: Number of trades with loss greater than 10% (i.e. -1000 bps)\n",
    "loss_threshold_bps = -1000  # -10%\n",
    "deep_losers = local_trade_log2[local_trade_log2['profit_bps'] < loss_threshold_bps]\n",
    "num_deep_losers = len(deep_losers)\n",
    "\n",
    "print(f\"Number of Losing Trades > 10% Loss: {num_deep_losers}\")\n",
    "\n",
    "# Add: Average loss of all losing trades\n",
    "avg_losing_trade = losers['profit'].mean()\n",
    "print(f\"Average Loss per Losing Trade: {avg_losing_trade:.4f}\")\n",
    "\n",
    "# Calculate average profit percentage (in % terms)\n",
    "average_profit_percent = local_trade_log2['profit'].mean() / position_cap * 100\n",
    "print(f\"\\nAverage Profit Percentage per Trade: {average_profit_percent:.2f}%\")\n",
    "\n",
    "# Calculate Trade Expectancy (in monetary terms)\n",
    "win_rate = len(winners) / total_trade_count\n",
    "avg_win = winners['profit'].mean()\n",
    "avg_loss = losers['profit'].mean()\n",
    "trade_expectancy = (win_rate * avg_win) - ((1 - win_rate) * abs(avg_loss))\n",
    "print(f\"Trade Expectancy: ${trade_expectancy:.2f} per trade\")\n",
    "\n",
    "# Calculate Risk-Adjusted Metrics\n",
    "risk_reward_ratio = abs(avg_win / avg_loss) if avg_loss != 0 else float('inf')\n",
    "print(f\"Risk-Reward Ratio: {risk_reward_ratio:.2f}:1\")\n",
    "\n",
    "\n",
    "# Add: Average loss of all losing trades\n",
    "avg_winning_trade = winners['profit'].mean()\n",
    "print(f\"Average Loss per Losing Trade: {avg_winning_trade:.4f}\")\n",
    "\n",
    "\n",
    "abs_profit_df =  pd.DataFrame()\n",
    "for symbol , data in absolute_symbol_pnl_series.items():\n",
    "\n",
    "    tdf = pd.DataFrame(data , columns=['timestamp' , symbol])\n",
    "    tdf.set_index('timestamp' , inplace=True)\n",
    "\n",
    "    # Reindex to match df_index, filling missing values with 0\n",
    "    abs_profit_df[symbol] = tdf.reindex(com_df2.index, fill_value=0)\n",
    "\n",
    "max_unrealised_drawdown = abs_profit_df.sum(axis=1).min()\n",
    "print(f\"Max unrealised loss ( which is unrealised drawdown on notional) is {max_unrealised_drawdown}\")\n",
    "\n",
    "\n",
    "# Long/Short specific metrics\n",
    "# Update all calculations to use the new condition\n",
    "long_trades = local_trade_log2[local_trade_log2['Side'].isin(['buy', 'long'])]\n",
    "short_trades = local_trade_log2[local_trade_log2['Side'].isin(['sell', 'short'])]\n",
    "\n",
    "num_long_trades = len(long_trades)\n",
    "num_short_trades = len(short_trades)\n",
    "\n",
    "# Profit metrics\n",
    "avg_long_profit_bps = long_trades['profit_bps'].mean()\n",
    "avg_short_profit_bps = short_trades['profit_bps'].mean()\n",
    "profit_total_long = long_trades['profit'].sum()\n",
    "profit_total_short = short_trades['profit'].sum()\n",
    "\n",
    "# Win/loss metrics\n",
    "def calculate_consecutive_wins_losses(trades):\n",
    "    trades = trades.sort_index()\n",
    "    consecutive = []\n",
    "    current_streak = 0\n",
    "    prev_result = None\n",
    "\n",
    "    for profit in trades['profit']:\n",
    "        current_result = 'win' if profit > 0 else 'loss'\n",
    "        if current_result == prev_result or prev_result is None:\n",
    "            current_streak += 1\n",
    "        else:\n",
    "            consecutive.append((prev_result, current_streak))\n",
    "            current_streak = 1\n",
    "        prev_result = current_result\n",
    "\n",
    "    if current_streak > 0:\n",
    "        consecutive.append((prev_result, current_streak))\n",
    "\n",
    "    return pd.DataFrame(consecutive, columns=['type', 'length'])\n",
    "\n",
    "# Calculate for long and short separately\n",
    "long_consecutive = calculate_consecutive_wins_losses(long_trades)\n",
    "short_consecutive = calculate_consecutive_wins_losses(short_trades)\n",
    "\n",
    "def print_streak_stats(name, df):\n",
    "    if len(df) > 0:\n",
    "        max_win_streak = df[df['type'] == 'win']['length'].max()\n",
    "        max_loss_streak = df[df['type'] == 'loss']['length'].max()\n",
    "        avg_win_streak = df[df['type'] == 'win']['length'].mean()\n",
    "        avg_loss_streak = df[df['type'] == 'loss']['length'].mean()\n",
    "        print(f\"\\n{name} Streaks:\")\n",
    "        print(f\"Max Winning Streak: {max_win_streak}\")\n",
    "        print(f\"Max Losing Streak: {max_loss_streak}\")\n",
    "        print(f\"Avg Winning Streak: {avg_win_streak:.1f}\")\n",
    "        print(f\"Avg Losing Streak: {avg_loss_streak:.1f}\")\n",
    "\n",
    "print_streak_stats(\"Long\", long_consecutive)\n",
    "print_streak_stats(\"Short\", short_consecutive)\n",
    "\n",
    "# Winning ratios\n",
    "long_winners = len(long_trades[long_trades['profit'] > 0])\n",
    "short_winners = len(short_trades[short_trades['profit'] > 0])\n",
    "\n",
    "long_win_ratio = long_winners / num_long_trades if num_long_trades > 0 else 0\n",
    "short_win_ratio = short_winners / num_short_trades if num_short_trades > 0 else 0\n",
    "\n",
    "print(f\"\\nLong Trades Win Ratio: {long_win_ratio:.2%}\")\n",
    "print(f\"\\nShort Trades Win Ratio: {short_win_ratio:.2%}\")\n",
    "\n",
    "# Print all metrics\n",
    "print(\"\\n==========================================\")\n",
    "print(f\"Number of long trades: {num_long_trades}\")\n",
    "print(f\"Number of short trades: {num_short_trades}\")\n",
    "print(f\"Total trades: {total_trade_count}\")\n",
    "\n",
    "print(f\"\\nAverage long trade profit: {avg_long_profit_bps:.2f} bps\")\n",
    "print(f\"Average short trade profit: {avg_short_profit_bps:.2f} bps\")\n",
    "print(f\"Net long profit: {profit_total_long:.2f}\")\n",
    "print(f\"Net short profit: {profit_total_short:.2f}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scatter plot of trades\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# X-axis: trade index (or entry date if you want timeline)\n",
    "x = range(len(local_trade_log2))\n",
    "\n",
    "# Y-axis: profit\n",
    "y = local_trade_log2['profit']\n",
    "\n",
    "# Scatter: green for winners, red for losers\n",
    "colors = ['green' if p > 0 else 'red' for p in y]\n",
    "\n",
    "plt.scatter(x, y, c=colors, alpha=0.7, edgecolors='k')\n",
    "\n",
    "# Add horizontal line at 0 (break-even)\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "\n",
    "plt.title(\"Scatter of Trade Profits\")\n",
    "plt.xlabel(\"Trade Index\")\n",
    "plt.ylabel(\"Profit ($)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "# Create equity curve plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "local_trade_log2['EC'].plot(title='Equity Curve', grid=True)\n",
    "plt.ylabel('Portfolio Value')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot to temporary buffer\n",
    "buffer = BytesIO()\n",
    "plt.savefig(buffer, format='png')\n",
    "plt.close()\n",
    "plot_data = base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "\n",
    "\n",
    "#=== symbols specific\n",
    "\n",
    "# 2. Group by the symbol name and calculate the sum of profits for each.\n",
    "symbol_profits = local_trade_log2.groupby('name')['profit'].sum()\n",
    "\n",
    "# Display the total profit for each symbol\n",
    "print(\"--- Total Profit per Symbol ---\")\n",
    "print(symbol_profits)\n",
    "\n",
    "\n",
    "# 3. Sort the results to easily find the best and worst.\n",
    "sorted_symbols = symbol_profits.sort_values(ascending=False)\n",
    "\n",
    "\n",
    "# 4. Extract the best and worst symbols from the sorted list.\n",
    "# The best symbol is the first item (index 0).\n",
    "best_symbol = sorted_symbols.index[0]\n",
    "best_profit = sorted_symbols.iloc[0]\n",
    "\n",
    "# The worst symbol is the last item (index -1).\n",
    "worst_symbol = sorted_symbols.index[-1]\n",
    "worst_loss = sorted_symbols.iloc[-1]\n",
    "\n",
    "\n",
    "# 3. Sort the profits in ASCENDING order.\n",
    "#    This places the symbols with the largest losses at the top.\n",
    "sorted_by_loss = symbol_profits.sort_values(ascending=True)\n",
    "\n",
    "\n",
    "# 4. Select the first 10 rows from the sorted list using .head(10).\n",
    "worst_10_symbols = sorted_by_loss.head(10)\n",
    "\n",
    "\n",
    "# 5. Print the final result.\n",
    "print(\"--- 10 Worst Performing Symbols (by Total Profit) ---\")\n",
    "print(worst_10_symbols)\n",
    "\n",
    "#======================================\n",
    "\n",
    "\n",
    "\n",
    "# --- NEW: Calculate and print the additional statistics ---\n",
    "\n",
    "# Count the number of symbols with negative total profit (losers)\n",
    "num_loss_making_symbols = (symbol_profits <= 0).sum()\n",
    "\n",
    "# Count the number of symbols with positive total profit (winners)\n",
    "num_profit_making_symbols = (symbol_profits > 0).sum()\n",
    "\n",
    "# Calculate the ratio of losers to winners\n",
    "# Add a check to prevent division by zero if there are no winners\n",
    "if num_profit_making_symbols > 0:\n",
    "    loss_to_profit_ratio = num_loss_making_symbols / num_profit_making_symbols\n",
    "else:\n",
    "    loss_to_profit_ratio = float('inf') # Indicate infinite ratio if no winners\n",
    "\n",
    "# Print the new stats\n",
    "print(\"\\n--- Symbol Profit/Loss Summary ---\")\n",
    "print(f\"Number of Profit-Making Symbols: {num_profit_making_symbols}\")\n",
    "print(f\"Number of Loss-Making Symbols:   {num_loss_making_symbols}\")\n",
    "print(f\"Ratio of Loss-Makers to Profit-Makers: {loss_to_profit_ratio:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_max_daily_gmv(trade_log):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculates the maximum and mean Gross Merchandise Value (GMV) on a single day\n",
    "    from a trade log DataFrame. The mean calculation accounts for all days in the\n",
    "    period, including non-trading days.\n",
    "\n",
    "    Args:\n",
    "        trade_log (pd.DataFrame): DataFrame containing the trade log. \n",
    "                                  Index must be a datetime object ('entry_date').\n",
    "                                  Must contain 'entry_price', 'exit_price', \n",
    "                                  'qty', and 'exit_date' columns.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the max_gmv_value and the mean_gmv_value.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the total value of each entry and exit transaction\n",
    "    trade_log['entry_value'] = trade_log['entry_price'] * trade_log['qty']\n",
    "    trade_log['exit_value'] = trade_log['exit_price'] * trade_log['qty']\n",
    "\n",
    "    # Group all entry values by their calendar day and sum them up\n",
    "    daily_entries = trade_log.groupby(trade_log.index.date)['entry_value'].sum()\n",
    "\n",
    "    # Group all exit values by their calendar day and sum them up\n",
    "    # CORRECTED: Group exits by 'exit_date', not the index ('entry_date')\n",
    "    daily_exits = trade_log.groupby(trade_log['exit_date'].dt.date)['exit_value'].sum()\n",
    "\n",
    "    # Combine the daily entry and exit sums to get the total GMV for each day\n",
    "    # This series only contains days where a trade occurred.\n",
    "    daily_gmv = daily_entries.add(daily_exits, fill_value=0)\n",
    "\n",
    "    # Find the maximum value. This can be done before reindexing for efficiency.\n",
    "    if daily_gmv.empty:\n",
    "        return 0, 0\n",
    "    max_gmv_value = daily_gmv.max()\n",
    "\n",
    "    # --- MODIFICATION TO ACCOUNT FOR NON-TRADING DAYS ---\n",
    "\n",
    "    # 1. Determine the full date range of the backtest period.\n",
    "    #    We use the earliest entry and latest exit to define the period.\n",
    "    start_date = trade_log.index.min().date()\n",
    "    end_date = trade_log['exit_date'].max().date()\n",
    "    all_days_in_period = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "    # 2. Reindex the 'daily_gmv' Series to include all days.\n",
    "    #    This assigns a GMV of 0 to all non-trading days.\n",
    "    daily_gmv = daily_gmv.reindex(all_days_in_period, fill_value=0)\n",
    "    \n",
    "    # --- END OF MODIFICATION ---\n",
    "\n",
    "    # Now, this calculation correctly finds the mean over the entire period.\n",
    "    mean_gmv_value = daily_gmv.mean()\n",
    "\n",
    "    return max_gmv_value, mean_gmv_value\n",
    "\n",
    "\n",
    "max_gmv , mean_gmv = find_max_daily_gmv(local_trade_log2)\n",
    "\n",
    "print(f\"Average daily GMV is {mean_gmv}\")\n",
    "print(f\"Max daily gmv is {max_gmv}\")\n",
    "\n",
    "\n",
    "print(f\"Return on average GMV is {local_trade_log2['profit'].sum()/mean_gmv}\")\n",
    "print(f\"Return on average GMV is {local_trade_log2['profit'].sum()/max_gmv}\")\n",
    "\n",
    "\n",
    "\n",
    "# Create HTML report\n",
    "html_report = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Backtest Performance Report</title>\n",
    "    <style>\n",
    "        body {{ font-family: Arial, sans-serif; margin: 20px; }}\n",
    "        h1 {{ color: #2c3e50; border-bottom: 2px solid #3498db; }}\n",
    "        h2 {{ color: #2980b9; }}\n",
    "        .metric-container {{ display: flex; flex-wrap: wrap; gap: 20px; }}\n",
    "        .metric-card {{\n",
    "            background: #f8f9fa;\n",
    "            border: 1px solid #dee2e6;\n",
    "            border-radius: 5px;\n",
    "            padding: 15px;\n",
    "            flex: 1 1 300px;\n",
    "            box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n",
    "        }}\n",
    "        .metric-title {{ font-weight: bold; color: #2c3e50; }}\n",
    "        .metric-value {{ font-size: 1.2em; color: #27ae60; }}\n",
    "        .negative {{ color: #e74c3c; }}\n",
    "        .plot-container {{ margin: 30px 0; text-align: center; }}\n",
    "        table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}\n",
    "        th, td {{ padding: 10px; text-align: left; border-bottom: 1px solid #ddd; }}\n",
    "        th {{ background-color: #3498db; color: white; }}\n",
    "        tr:nth-child(even) {{ background-color: #f2f2f2; }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Strategy Backtest Performance Report</h1>\n",
    "\n",
    "    <div class=\"plot-container\">\n",
    "        <h2>Equity Curve</h2>\n",
    "        <img src=\"data:image/png;base64,{plot_data}\" alt=\"Equity Curve\">\n",
    "    </div>\n",
    "\n",
    "    <h2>Key Performance Metrics</h2>\n",
    "    <div class=\"metric-container\">\n",
    "        <div class=\"metric-card\">\n",
    "            <div class=\"metric-title\">Total Trades</div>\n",
    "            <div class=\"metric-value\">{total_trade_count:,}</div>\n",
    "        </div>\n",
    "        <div class=\"metric-card\">\n",
    "            <div class=\"metric-title\">Win Rate</div>\n",
    "            <div class=\"metric-value\">{100 - losers_percentage:.1f}%</div>\n",
    "        </div>\n",
    "        <div class=\"metric-card\">\n",
    "            <div class=\"metric-title\">Average Trade Profit</div>\n",
    "            <div class=\"metric-value\">${average_trade_profit:,.2f}</div>\n",
    "        </div>\n",
    "        <div class=\"metric-card\">\n",
    "            <div class=\"metric-title\">Trade Expectancy</div>\n",
    "            <div class=\"metric-value\">${trade_expectancy:,.2f}</div>\n",
    "        </div>\n",
    "        <div class=\"metric-card\">\n",
    "            <div class=\"metric-title\">Risk-Reward Ratio</div>\n",
    "            <div class=\"metric-value\">{risk_reward_ratio:.2f}:1</div>\n",
    "        </div>\n",
    "        <div class=\"metric-card\">\n",
    "            <div class=\"metric-title\">Max Drawdown</div>\n",
    "            <div class=\"metric-value negative\">${max_unrealised_drawdown:,.2f}</div>\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    <h2>Long/Short Breakdown</h2>\n",
    "    <table>\n",
    "        <tr>\n",
    "            <th>Metric</th>\n",
    "            <th>Long Trades</th>\n",
    "            <th>Short Trades</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Count</td>\n",
    "            <td>{num_long_trades:,}</td>\n",
    "            <td>{num_short_trades:,}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Win Rate</td>\n",
    "            <td>{long_win_ratio:.1%}</td>\n",
    "            <td>{short_win_ratio:.1%}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Avg Profit (bps)</td>\n",
    "            <td>{avg_long_profit_bps:.1f}</td>\n",
    "            <td>{avg_short_profit_bps:.1f}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Total Profit</td>\n",
    "            <td>${profit_total_long:,.2f}</td>\n",
    "            <td>${profit_total_short:,.2f}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Max Win Streak</td>\n",
    "            <td>{long_consecutive[long_consecutive['type']=='win']['length'].max() if len(long_consecutive) > 0 else 0}</td>\n",
    "            <td>{short_consecutive[short_consecutive['type']=='win']['length'].max() if len(short_consecutive) > 0 else 0}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Max Loss Streak</td>\n",
    "            <td>{long_consecutive[long_consecutive['type']=='loss']['length'].max() if len(long_consecutive) > 0 else 0}</td>\n",
    "            <td>{short_consecutive[short_consecutive['type']=='loss']['length'].max() if len(short_consecutive) > 0 else 0}</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "\n",
    "    <h2>Position Sizing</h2>\n",
    "    <div class=\"metric-container\">\n",
    "        \n",
    "        <div class=\"metric-card\">\n",
    "            <div class=\"metric-title\">Max Entries Per Day</div>\n",
    "            <div class=\"metric-value\">{max_entries_per_day}</div>\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    <h2>Trade Duration</h2>\n",
    "    <div class=\"metric-container\">\n",
    "        <div class=\"metric-card\">\n",
    "            <div class=\"metric-title\">Avg Holding Days</div>\n",
    "            <div class=\"metric-value\">{local_trade_log2['holding_days'].mean():.1f}</div>\n",
    "        </div>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Display the report\n",
    "HTML(html_report)\n",
    "\n",
    "# To save the report to a file\n",
    "with open('Nasdaq_Vol_stocks.html', 'w') as f:\n",
    "    f.write(html_report)\n",
    "\n",
    "local_trade_log2.to_csv(\"Nasdaq_TL.csv\", index=True)\n",
    "\n",
    "# Convert total_pnl_series dict (timestamp ‚Üí pnl) into DataFrame\n",
    "mtm_equity_df = pd.DataFrame(list(total_pnl_series.items()), columns=[\"timestamp\", \"MTM_PnL\"])\n",
    "\n",
    "# Sort by timestamp (important to keep the curve ordered)\n",
    "mtm_equity_df.sort_values(\"timestamp\", inplace=True)\n",
    "\n",
    "# Add equity column: starting capital + cumulative PnL\n",
    "mtm_equity_df[\"Equity\"] = intial_capital + mtm_equity_df[\"MTM_PnL\"].cumsum()\n",
    "\n",
    "# Set timestamp as index for convenience\n",
    "mtm_equity_df.set_index(\"timestamp\", inplace=True)\n",
    "\n",
    "# Define full path\n",
    "mtm_equity_file = os.path.join(trades_strategy_folder, \"MTM_equity.csv\")\n",
    "\n",
    "# Save to CSV\n",
    "mtm_equity_df.to_csv(mtm_equity_file)\n",
    "\n",
    "print(f\"‚úÖ MTM equity curve saved at: {mtm_equity_file}\")\n",
    "\n",
    "\n",
    "for symbol, data in absolute_symbol_pnl_series.items():\n",
    "    if not data:\n",
    "        print(f\"No data for {symbol}, skipping.\")\n",
    "        continue\n",
    "    try:\n",
    "        tdf = pd.DataFrame(data, columns=['timestamp', 'pnl_absolute'])\n",
    "        tdf.set_index('timestamp', inplace=True)\n",
    "        file_path = os.path.join(df_folder, f\"{symbol}_pnl.csv\")\n",
    "        tdf.to_csv(file_path)\n",
    "        print(f\"Successfully saved {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {symbol}: {e}\")\n",
    "\n",
    "\n",
    "import quantstats as qs\n",
    "qs.extend_pandas()\n",
    "\n",
    "qs.reports.basic(local_trade_log2['EC'])\n",
    "\n",
    "# --- 2. Plotting the 'EC' column ---\n",
    "plt.figure(figsize=(12, 6)) # Optional: Adjust the figure size\n",
    "\n",
    "local_trade_log2['EC'].plot(grid=True) # The main plotting command\n",
    "\n",
    "# --- 3. Add labels and a title for clarity ---\n",
    "plt.title('EC Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('EC Value')\n",
    "\n",
    "# --- 4. Display the plot ---\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3188a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "com_legs_df[[f\"{legs}_intraday_low\" for legs in legs_to_process]] =com_legs_df[[f\"{legs}_bidPrice\" for legs in legs_to_process]].apply(lambda x: intraday_low(com_legs_df, low_col=x.name,agg_func='min'))\n",
    "\n",
    "com_legs_df[[f\"{legs}_intraday_high\" for legs in legs_to_process]] =com_legs_df[[f\"{legs}_askPrice\" for legs in legs_to_process]].apply(lambda x: intraday_high(com_legs_df, high_col=x.name,agg_func='max'))\n",
    "\n",
    "\n",
    "# Calculate Today's Opening Price for each leg\n",
    "com_legs_df[[f'{leg}_todayo' for leg in legs_to_process]] =  com_legs_df[[f'{leg}_close' for leg in legs_to_process]].resample('D').transform('first')\n",
    "\n",
    "# Calculate Previous Day's Low (using bidPrice) for each leg\n",
    "com_legs_df[[f\"{leg}_prevdayl\" for leg in legs_to_process]] = com_legs_df[[f\"{leg}_bidPrice\" for leg in legs_to_process]].apply(lambda x: get_x_day_low_numba(com_legs_df, n=1, column=x.name))\n",
    "\n",
    "# Calculate Previous Day's High (using askPrice) for each leg\n",
    "com_legs_df[[f\"{leg}_prevdayh\" for leg in legs_to_process]] = \\\n",
    "    com_legs_df[[f\"{leg}_askPrice\" for leg in legs_to_process]].apply(lambda x: get_x_day_high_numba(com_legs_df, n=1, column=x.name) )\n",
    "\n",
    "close_cols = [f'{leg}_close' for leg in legs_to_process]\n",
    "window_size = 15\n",
    "\n",
    "# 1. Rolling 15-Bar Highest Close\n",
    "com_legs_df[[f'{leg}_15min_high' for leg in legs_to_process]] = \\\n",
    "    com_legs_df[close_cols].rolling(window=window_size).max()\n",
    "\n",
    "# 2. Rolling 15-Bar Lowest Close\n",
    "com_legs_df[[f'{leg}_15min_low' for leg in legs_to_process]] = \\\n",
    "    com_legs_df[close_cols].rolling(window=window_size).min()\n",
    "\n",
    "# 3. Rolling 15-Bar Mean Close\n",
    "com_legs_df[[f'{leg}_15min_mean' for leg in legs_to_process]] = \\\n",
    "    com_legs_df[close_cols].rolling(window=window_size).mean()\n",
    "\n",
    "\n",
    "com_legs_df[[f'{leg}_ROC_5' for leg in legs_to_process]] = com_legs_df[[f\"{leg}_close\" for leg in legs_to_process]].pct_change(periods=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fefc8931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ---  HELPER FUNCTION for Clean and Fast Trade Logging ---\n",
    "# ==============================================================================\n",
    "def log_and_close_trade(trade_log_list, leg, exit_timestamp, exit_price, exit_reason, state_vars):\n",
    "    \"\"\"\n",
    "    Calculates P&L, appends the trade to a log list, and resets the state for that leg.\n",
    "    \"\"\"\n",
    "    # Unpack state variables for easier access\n",
    "    entry_dates, entry_prices, qty, position_type, instrument_positions, bar_count = (\n",
    "        state_vars['entry_dates'], state_vars['entry_prices'], state_vars['qty'],\n",
    "        state_vars['position_type'], state_vars['instrument_positions'], state_vars['bar_count']\n",
    "    )\n",
    "    \n",
    "    if entry_prices[leg] is None: return # Avoid logging if no entry price exists\n",
    "\n",
    "    if position_type[leg] == 1:  # Long position\n",
    "        profit = (exit_price - entry_prices[leg]) * qty[leg]\n",
    "        side = 'BUY'\n",
    "    else: # Short position\n",
    "        profit = (entry_prices[leg] - exit_price) * qty[leg]\n",
    "        side = 'SELL'\n",
    "\n",
    "    trade_log_list.append({\n",
    "        'name': leg, 'Side': side, 'entry_date': entry_dates[leg],\n",
    "        'entry_price': entry_prices[leg], 'qty': qty[leg],\n",
    "        'exit_date': exit_timestamp, 'exit_price': exit_price,\n",
    "        'profit': profit, 'exit_reason': exit_reason\n",
    "    })\n",
    "\n",
    "    # Reset state variables\n",
    "    instrument_positions[leg] = False\n",
    "    entry_prices[leg] = None\n",
    "    bar_count[leg] = 0\n",
    "    \n",
    "    print(f\"   -> EXIT {leg} ({side}) at {exit_price:.2f} for {exit_reason}. P&L: ${profit:.2f}\")\n",
    "\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b00b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "strike_interval = 1\n",
    "lookback_days = 3\n",
    "option_multiplier = 100\n",
    "from collections import  defaultdict\n",
    "\n",
    "# ==============================================================================\n",
    "# SCRIPT STARTS HERE\n",
    "# ==============================================================================\n",
    "\n",
    "trade_log = []\n",
    "nyse = mcal.get_calendar('NYSE') \n",
    "unique_days = com_df.index.normalize().unique()\n",
    "print(f\"üöÄ Starting backtest for {len(unique_days)} unique days...\")\n",
    "\n",
    "intial_capital = 100000\n",
    "position_cap = 100000\n",
    "\n",
    "# Track local trade log\n",
    "local_trade_log2 = pd.DataFrame(columns=['name', 'Side', 'entry_date', 'entry_price' , 'qty' ,  'exit_date', 'exit_price', 'order_no','profit' ])\n",
    "\n",
    "# ‚úÖ Initialize a list to hold all trade dictionaries.\n",
    "final_trade_log_list = []\n",
    "\n",
    "# --- Main Loop: Iterate Through Each Day ---\n",
    "for day in unique_days[2:]:\n",
    "    \n",
    "    day_str = day.strftime('%Y-%m-%d')\n",
    "    print(f\"\\n--- Processing Day: {day_str} ---\")\n",
    "\n",
    "    try:\n",
    "        \n",
    "        daily_underlying_df = com_df[com_df.index.date == day.date()]\n",
    "        if daily_underlying_df.empty: continue\n",
    "\n",
    "        open_price = daily_underlying_df['SPY_close'].iloc[0]\n",
    "        if pd.isna(open_price): continue\n",
    "        \n",
    "        atm_strike = np.round(open_price)\n",
    "        #print(f\"  STEP 1: ATM Strike for the day set to {atm_strike}\")\n",
    "\n",
    "        # --- Dynamically Build and Fetch Data Based on Config ---\n",
    "        all_legs_hist_data = {}\n",
    "        fetch_success = True\n",
    "        legs_to_trade = []\n",
    "\n",
    "        for leg_name, params in strategy_config.items():\n",
    "            \n",
    "            target_strike = atm_strike + (params['strike_offset'] * strike_interval)\n",
    "\n",
    "            print(f\"Atm strike is {atm_strike} and taegt strike is {target_strike} and optiontyoe is {params['option_type']}\")\n",
    "            schedule = nyse.schedule(start_date=day, end_date=day + pd.Timedelta(days=14))\n",
    "            target_expiry_str = schedule.index[params['expiry_offset']].strftime('%Y-%m-%d')\n",
    "            print(f\"Traget expiry for {leg_name} and {day_str} is {target_expiry_str} \")\n",
    "            legs_to_trade.append({\n",
    "                'name': leg_name, 'strike': target_strike,\n",
    "                'expiry': target_expiry_str, 'type': params['option_type']})\n",
    "            \n",
    "            leg_hist_df = fetch_option_data_for_n_days(\n",
    "                con=con, ticker='SPY', end_date_str=day_str, n_days=lookback_days,\n",
    "                strike=target_strike, expiry_str=target_expiry_str, option_type=params['option_type'] )\n",
    "                    \n",
    "            # ‚úÖ ADD THIS LINE TO DEBUG:\n",
    "            #print(f\"Columns found for {leg_name}: {leg_hist_df.columns.tolist()}\")\n",
    "\n",
    "            if not leg_hist_df.empty:\n",
    "                # Find rows where both bidPrice and askPrice are 0\n",
    "                zero_rows = leg_hist_df[ (leg_hist_df['bidPrice'] == 0) & (leg_hist_df['askPrice'] == 0) ]\n",
    "                zero_count = len(zero_rows)\n",
    "                total_rows = len(leg_hist_df)\n",
    "                \n",
    "                #print(f\"  üìä {leg_name}: {zero_count}/{total_rows} rows have both bid & ask = 0\")\n",
    "                # One-liner: Replace zeros with NaN and forward fill for both columns\n",
    "                leg_hist_df[['bidPrice', 'askPrice']] = leg_hist_df[['bidPrice', 'askPrice']].replace(0, np.nan).ffill()\n",
    "                \n",
    "                if zero_count > 1:\n",
    "                    print(f\"  ‚ö†Ô∏è  {zero_count} Rows with zero prices for {leg_name}:\")\n",
    "\n",
    "                    if leg_hist_df.empty:\n",
    "                        print(f\"  ‚ö†Ô∏è Could not fetch data for {leg_name}. Skipping day.\")\n",
    "                        fetch_success = False\n",
    "                        break\n",
    "                all_legs_hist_data[leg_name] = leg_hist_df\n",
    "            else:\n",
    "                print(f\"No options data found for {date_str}\")\n",
    "        if not fetch_success: \n",
    "            print(f\"Not able to fetch options for {day} \")\n",
    "            continue\n",
    "\n",
    "        # --- Merge and Prepare Data ---\n",
    "        prefixed_dfs_today = []\n",
    "        for leg_name, leg_hist_df in all_legs_hist_data.items():\n",
    "            \n",
    "            prefixed_dfs_today.append(leg_hist_df.add_prefix(f'{leg_name}_'))\n",
    "       \n",
    "        # 1. First, concatenate all the prefixed option leg DataFrames together\n",
    "        com_legs_df = pd.concat(prefixed_dfs_today, axis=1, join='outer')\n",
    "        com_legs_df = com_legs_df[com_legs_df.index.time>time(9 , 30)]\n",
    "        \n",
    "        if ( len(com_legs_df)<10):\n",
    "            print(\"\\n\\n =============\")\n",
    "            print(f\"Leg data very less for {date_str}\")\n",
    "\n",
    "\n",
    "        #==== now do all calculations and indicators on this\n",
    "        legs_to_process = ['Leg1', 'Leg2']\n",
    "                \n",
    "        com_legs_df[[f\"{legs}_intraday_low\" for legs in legs_to_process]] = com_legs_df[[f\"{legs}_bidPrice\" for legs in legs_to_process]].apply(lambda x: intraday_low(com_legs_df, low_col=x.name,agg_func='min'))\n",
    "\n",
    "        com_legs_df[[f\"{legs}_intraday_high\" for legs in legs_to_process]] = com_legs_df[[f\"{legs}_askPrice\" for legs in legs_to_process]].apply(lambda x: intraday_high(com_legs_df, high_col=x.name,agg_func='max'))\n",
    "\n",
    "\n",
    "        # Calculate Today's Opening Price for each leg\n",
    "        com_legs_df[[f'{leg}_todayo' for leg in legs_to_process]] =  com_legs_df[[f'{leg}_close' for leg in legs_to_process]].resample('D').transform('first')\n",
    "\n",
    "        # Calculate Previous Day's Low (using bidPrice) for each leg\n",
    "        com_legs_df[[f\"{leg}_prevdayl\" for leg in legs_to_process]] = com_legs_df[[f\"{leg}_bidPrice\" for leg in legs_to_process]].apply(lambda x: get_x_day_low_numba(com_legs_df, n=1, column=x.name))\n",
    "\n",
    "        # Calculate Previous Day's High (using askPrice) for each leg\n",
    "        com_legs_df[[f\"{leg}_prevdayh\" for leg in legs_to_process]] = \\\n",
    "            com_legs_df[[f\"{leg}_askPrice\" for leg in legs_to_process]].apply(lambda x: get_x_day_high_numba(com_legs_df, n=1, column=x.name) )\n",
    "\n",
    "        close_cols = [f'{leg}_close' for leg in legs_to_process]\n",
    "        window_size = 15\n",
    "\n",
    "        # # 1. Rolling 15-Bar Highest Close\n",
    "        # com_legs_df[[f'{leg}_15min_high' for leg in legs_to_process]] = \\\n",
    "        #     com_legs_df[close_cols].rolling(window=window_size).max()\n",
    "\n",
    "        # # 2. Rolling 15-Bar Lowest Close\n",
    "        # com_legs_df[[f'{leg}_15min_low' for leg in legs_to_process]] = \\\n",
    "        #     com_legs_df[close_cols].rolling(window=window_size).min()\n",
    "\n",
    "        # # 3. Rolling 15-Bar Mean Close\n",
    "        # com_legs_df[[f'{leg}_15min_mean' for leg in legs_to_process]] = \\\n",
    "        #     com_legs_df[close_cols].rolling(window=window_size).mean()\n",
    "\n",
    "        com_legs_df[[f'{leg}_ROC_5' for leg in legs_to_process]] = com_legs_df[[f\"{leg}_close\" for leg in legs_to_process]].pct_change(periods=5)\n",
    "#=== merge underlying back with com_legs_df\n",
    "\n",
    "        com_df3 = pd.concat([daily_underlying_df , com_legs_df] , join='outer' , axis=1)\n",
    "\n",
    "        # --- FIX: Add this line to filter for the current day ---\n",
    "        com_df3 = com_df3[com_df3.index.date == pd.to_datetime(day_str).date()]\n",
    "\n",
    "         # Initialize state for this day's trading (if needed)\n",
    "        instrument_positions = {leg: False for leg in legs_to_process}\n",
    "        entry_prices = {leg: None for leg in legs_to_process}\n",
    "        # ... (add any other state variables you need: qty, entry_dates, etc.) ...\n",
    "       \n",
    "        entry_dates = {leg: None for leg in legs_to_process}\n",
    "                \n",
    "        # To track the current drawdown value for each active leg\n",
    "        drawdown = {leg: None for leg in legs_to_process}\n",
    "\n",
    "        # To track P&L for each leg (can be redundant if using series)\n",
    "        symbol_pnl = {leg: None for leg in legs_to_process}\n",
    "\n",
    "        # To track the maximum drawdown seen for each leg\n",
    "        max_drawdown = {leg: 0 for leg in legs_to_process}\n",
    "\n",
    "        # To count how many bars an active position has been held\n",
    "        bar_count = {leg: 0 for leg in legs_to_process}\n",
    "\n",
    "        # To store the quantity/number of contracts for each leg\n",
    "        qty = {leg: 0 for leg in legs_to_process}\n",
    "\n",
    "        # To store an order identifier for each leg's trade\n",
    "        order_no = {leg: 0 for leg in legs_to_process}\n",
    "\n",
    "        # To store the long/short direction (e.g., 1 for long, -1 for short)\n",
    "        position_type = {leg: 0 for leg in legs_to_process}\n",
    "\n",
    "        # To store the drawdown history for each leg\n",
    "        drawdown_series = {leg: [] for leg in legs_to_process}\n",
    "\n",
    "        # To store the P&L history for each leg\n",
    "        symbol_pnl_series = {leg: [] for leg in legs_to_process}\n",
    "\n",
    "        # To store another P&L history (can be consolidated with the one above)\n",
    "        absolute_symbol_pnl_series = {leg: [] for leg in legs_to_process}\n",
    "        # ===================================================================\n",
    "        # --- Bar-by-Bar Simulation Loop using for i in range() ---\n",
    "        # ===================================================================\n",
    "        daily_trade = {'legs': {}}\n",
    "        print(\"  STEP 3: Starting bar-by-bar simulation...\")\n",
    "        \n",
    "        logger.info(f\"Running analysis on: {intial_capital} of capital\")\n",
    "        cumulative_realized_pnl = 0.0\n",
    "\n",
    "\n",
    "        # === Exposure & portfolio trackers ===\n",
    "        exposure = 0.0        # net exposure\n",
    "        gross_exposure = 0.0  # absolute exposure\n",
    "\n",
    "        exposure_series = {}          # net exposure at each timestamp\n",
    "        gross_exposure_series = {}    # absolute exposure\n",
    "        portfolio_value_series = {}   # portfolio equity curve\n",
    "        \n",
    "        # Initialize before main loop\n",
    "        total_pnl_series = defaultdict(float)\n",
    "        # Process each row in time order\n",
    "\n",
    "        # --- 2C. INITIALIZE STATE FOR THE DAY ---\n",
    "        state_vars = {\n",
    "            'instrument_positions': {leg: False for leg in legs_to_process},\n",
    "            'entry_prices': {leg: None for leg in legs_to_process},\n",
    "            'entry_dates': {leg: None for leg in legs_to_process},\n",
    "            'bar_count': {leg: 0 for leg in legs_to_process},\n",
    "            'qty': {leg: 0 for leg in legs_to_process},\n",
    "            'position_type': {leg: 0 for leg in legs_to_process}\n",
    "        }\n",
    "\n",
    "        # --- 2D. BAR-BY-BAR SIMULATION FOR THE CURRENT DAY ---\n",
    "        print(\"   STEP 3: Starting bar-by-bar simulation...\")\n",
    "        \n",
    "        for i in range(1, len(com_df3)):\n",
    "            \n",
    "            row = com_df3.iloc[i]\n",
    "            timestamp = com_df3.index[i]\n",
    "\n",
    "            position_is_open = state_vars['instrument_positions']['Leg1'] # Check one leg to represent the spread\n",
    "\n",
    "            # --- CHECK FOR EXITS FIRST ---\n",
    "            if position_is_open:\n",
    "                state_vars['bar_count']['Leg1'] += 1\n",
    "                state_vars['bar_count']['Leg2'] += 1\n",
    "                \n",
    "                # Exit Condition Example: End of day\n",
    "                if timestamp.time() >= time(15, 45) and state_vars['bar_count']['Leg1']>1  :\n",
    "                    print(f\"   -> Exiting spread at {timestamp.time()} for End of Day.\")\n",
    "                    # Close both legs\n",
    "                    leg1_exit_price = row['Leg1_bidPrice'] if state_vars['position_type']['Leg1'] == 1 else row['Leg1_askPrice']\n",
    "                    leg2_exit_price = row['Leg2_bidPrice'] if state_vars['position_type']['Leg2'] == 1 else row['Leg2_askPrice']\n",
    "                    \n",
    "                    log_and_close_trade(final_trade_log_list, 'Leg1', timestamp, leg1_exit_price, \"EOD\", state_vars)\n",
    "                    log_and_close_trade(final_trade_log_list, 'Leg2', timestamp, leg2_exit_price, \"EOD\", state_vars)\n",
    "                    break # Day is over, exit simulation loop\n",
    "\n",
    "                if timestamp.time() >= time(9, 45) and state_vars['bar_count']['Leg1']>5 and row['Leg1_bidPrice']>state_vars['entry_prices']['Leg1']+0.5  :\n",
    "                    print(f\"   -> Exiting spread at {timestamp.time()} for End of Day.\")\n",
    "                    # Close both legs\n",
    "                    leg1_exit_price = row['Leg1_bidPrice'] if state_vars['position_type']['Leg1'] == 1 else row['Leg1_askPrice']\n",
    "                    leg2_exit_price = row['Leg2_bidPrice'] if state_vars['position_type']['Leg2'] == 1 else row['Leg2_askPrice']\n",
    "                    \n",
    "                    log_and_close_trade(final_trade_log_list, 'Leg1', timestamp, leg1_exit_price, \"EOD\", state_vars)\n",
    "                    log_and_close_trade(final_trade_log_list, 'Leg2', timestamp, leg2_exit_price, \"EOD\", state_vars)\n",
    "                    print(f\"Stop loss hit\")\n",
    "                    break # Day is over, exit simulation loop\n",
    "        \n",
    "\n",
    "\n",
    "            # --- THEN, CHECK FOR ENTRIES ---\n",
    "            else: # if not position_is_open\n",
    "                \n",
    "                # Define your entry condition for the entire spread\n",
    "                entry_condition_met = (\n",
    "                    time(9, 40) <= timestamp.time() <= time(13, 15) and  row[f\"Leg1_close\"]<1.5 and row[f\"Leg1_close\"]>1    and com_df3[f\"SPY_3d_Pctl\"].iloc[i-1]<0.5 and  com_df3[f\"SPY_3d_Pctl\"].iloc[i-1]>0  and row[f\"Leg1_close\"]<min((com_df3[f\"Leg1_intraday_high\"].iloc[i - 1]*0.9) , com_df3[f\"Leg1_close\"].iloc[i - 1]*0.999 )  and com_df3[f\"Leg1_ROC_5\"].iloc[i - 1]>0.01\n",
    "                    \n",
    "                )\n",
    "\n",
    "                if entry_condition_met:\n",
    "                    print(f\"   -> ENTRY signal for spread triggered at {timestamp.time()}\")\n",
    "                    \n",
    "                    # --- Open Position for BOTH LEGS ---\n",
    "                    for leg_name, params in strategy_config.items():\n",
    "                        if params['action'] == 'BUY':\n",
    "                            entry_price = row[f\"{leg_name}_askPrice\"]\n",
    "                            pos_type = 1\n",
    "                        else: # SELL\n",
    "                            entry_price = row[f\"{leg_name}_bidPrice\"]\n",
    "                            pos_type = -1\n",
    "                        # --- ‚úÖ Set quantity based on leg_name inside the loop ---\n",
    "                        if leg_name == 'Leg1':\n",
    "                            trade_qty = 100  # Set quantity for Leg1\n",
    "                        elif leg_name == 'Leg2':\n",
    "                            trade_qty = 100  # Set quantity for Leg2\n",
    "                        \n",
    "                        # Update state for the current leg\n",
    "                        state_vars['instrument_positions'][leg_name] = True\n",
    "                        state_vars['entry_prices'][leg_name] = entry_price\n",
    "                        state_vars['entry_dates'][leg_name] = timestamp\n",
    "                        state_vars['position_type'][leg_name] = pos_type\n",
    "                        state_vars['qty'][leg_name] = trade_qty\n",
    "                        state_vars['bar_count'][leg_name] = 1\n",
    "                        print(f\"      -> Executing {leg_name} ({params['action']}) at {entry_price:.2f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå An error occurred on {day_str}: {e}\")\n",
    "\n",
    "# --- 3. FINAL RESULTS ANALYSIS ---\n",
    "print(\"\\n--- BACKTEST COMPLETE ---\")\n",
    "if final_trade_log_list:\n",
    "    log_df = pd.DataFrame(final_trade_log_list)\n",
    "    print(log_df)\n",
    "else:\n",
    "    print(\"No trades were logged.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdb8978",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df['profit'].cumsum().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74979d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ad58e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Group by the date of the 'entry_date' and sum the 'profit' column\n",
    "daily_profit = log_df.groupby(log_df['entry_date'].dt.date)['profit'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3a0b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_profit.cumsum().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc42ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
