{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c6faa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import calendar\n",
    "import numpy as np\n",
    "import glob\n",
    "import sys\n",
    "import gc\n",
    "import re\n",
    "#import quantstats as qs\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nest_asyncio # install this package to avoid running in loop\n",
    "\n",
    "import urllib\n",
    "import pandas_ta as ta\n",
    "\n",
    "import warnings\n",
    "\n",
    "import requests\n",
    "\n",
    "import hmac\n",
    "import hashlib\n",
    "\n",
    "import aiohttp\n",
    "import asyncio\n",
    "\n",
    "import logging\n",
    "from datetime import timezone\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "# Filter depreciation warnings from pandas regarding the append method\n",
    "warnings.filterwarnings('ignore', category=FutureWarning, message=\".*append.*\")\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Set pandas to display all digits for floating-point numbers\n",
    "pd.options.display.float_format = '{:.8f}'.format\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# Ignore only FutureWarning\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Ignore only RuntimeWarning\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "import importlib\n",
    "\n",
    "\n",
    "from Orats_options_fetch_functions import *\n",
    "\n",
    "from faster_numba_strategy_functions import *\n",
    "\n",
    "import strategy_functions\n",
    "\n",
    "importlib.reload(strategy_functions)\n",
    "from strategy_functions import *\n",
    "\n",
    "# import Common_functions\n",
    "\n",
    "# importlib.reload(strategy_functions)\n",
    "# importlib.reload(Common_functions)\n",
    "\n",
    "# from Common_functions import *\n",
    "# coin_desk_api_key =  \"e43c1fd991e660b9bf959645f0800bb7e76fb4a3537ab773cec62b2fad31af2d\"\n",
    "from datetime import time\n",
    "from pathlib import Path\n",
    "# Get the user's home directory dynamically\n",
    "home_dir = r\"C:\\Data\\Options_logs\"\n",
    "#====================================\n",
    "\n",
    "strategy_name = \"Put_buy_0DTE\"\n",
    "\n",
    "# Create a folder for the strategy_name if it doesn't exist in home directory\n",
    "\n",
    "strategy_folder = os.path.join(home_dir, strategy_name)\n",
    "if not os.path.exists(strategy_folder):\n",
    "    os.makedirs(strategy_folder)\n",
    "\n",
    "# Create the full path to the output file. this file contains all the trades\n",
    "\n",
    "output_all_trades_file = os.path.join(strategy_folder, f\"All_trades_{strategy_name}.csv\")\n",
    "trade_html_file = os.path.join(strategy_folder, f\"Trade_stats_{strategy_name}.html\")\n",
    "\n",
    "#=== location to store the trade dataframe for debug\n",
    "\n",
    "base_folder_df = \"strategy_dataframe\"\n",
    "df_folder = os.path.join(strategy_folder, base_folder_df)\n",
    "if not os.path.exists(df_folder):\n",
    "    os.makedirs(df_folder)\n",
    "    print(\"\\n Created strategy dataframe folder : \" ,df_folder )\n",
    "\n",
    "#----- symbol_wise_trades\n",
    "\n",
    "trades_folder = \"sym_wise_trades\"\n",
    "trades_strategy_folder = os.path.join(strategy_folder, trades_folder)\n",
    "\n",
    "if not os.path.exists(trades_strategy_folder):\n",
    "    print(\"\\n Making folder for symbol wise trades of strategy\" , trades_strategy_folder)\n",
    "    os.makedirs(trades_strategy_folder)\n",
    "\n",
    "logger = setup_logger(\"Straddle_0DTE\", strategy_folder, log_file=\"strategy.log\" , to_console=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af557d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying data from: s3://duckdata/ORATS/Options/**/*.parquet\n",
      "\n",
      "✅ Query successful! First 5 rows of data:\n",
      "      expiry       strike  dte  optionType                  ts         low  \\\n",
      "0 2024-01-26 421.00000000   25          -1 2024-01-02 17:13:00 17.18000000   \n",
      "1 2024-01-12 414.00000000   11          -1 2024-01-02 17:13:00 10.41500000   \n",
      "\n",
      "         high        open       close     volume  ...      askSize  \\\n",
      "0 17.18000000 17.18000000 17.18000000 0.00000000  ...  50.00000000   \n",
      "1 10.41500000 10.41500000 10.41500000 0.00000000  ... 151.00000000   \n",
      "\n",
      "     bidPrice    askPrice      bidIv      askIv         iv          oi  \\\n",
      "0 17.12000000 17.24000000 0.13785700 0.14511500 0.14148600 15.00000000   \n",
      "1 10.34000000 10.49000000 0.14267100 0.15199600 0.14733300 54.00000000   \n",
      "\n",
      "    stockPrice        day ticker  \n",
      "0 404.00000000 2024-01-02    QQQ  \n",
      "1 404.00000000 2024-01-02    QQQ  \n",
      "\n",
      "[2 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Connect to an in-memory database\n",
    "con = duckdb.connect()\n",
    "\n",
    "# --- Your setup code (which is correct) ---\n",
    "con.execute(\"INSTALL httpfs;\")\n",
    "con.execute(\"LOAD httpfs;\")\n",
    "con.execute(\"\"\"\n",
    "SET s3_endpoint='122.176.133.73:9000';\n",
    "SET s3_use_ssl=false;\n",
    "SET s3_access_key_id='minioadmin';\n",
    "SET s3_secret_access_key='minioadmin';\n",
    "\"\"\")\n",
    "con.execute(\"SET s3_url_style='path';\") \n",
    "con.execute(\"PRAGMA threads=8;\")\n",
    "con.execute(\"PRAGMA enable_object_cache;\")\n",
    "\n",
    "# --- THE MISSING STEP: Querying a file from S3 ---\n",
    "# Replace with your actual S3 bucket and file path\n",
    "# This can be a .parquet, .csv, or .json file\n",
    "s3_file_path = 's3://duckdata/ORATS/Options/**/*.parquet'\n",
    "\n",
    "try:\n",
    "    # Execute a query on the S3 file and fetch the result as a Pandas DataFrame\n",
    "    print(f\"Querying data from: {s3_file_path}\")\n",
    "    df = con.execute(f\"\"\"\n",
    "        SELECT *\n",
    "        FROM '{s3_file_path}'\n",
    "        LIMIT 2;\n",
    "    \"\"\").df()\n",
    "\n",
    "    print(\"\\n✅ Query successful! First 5 rows of data:\")\n",
    "    print(df.head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ An error occurred during the query: {e}\")\n",
    "\n",
    "# finally:\n",
    "#     # It's good practice to close the connection\n",
    "#     con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3a2d6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example 1: 0DTE At-the-Money LONG STRADDLE\n",
    "strategy_config = {\n",
    "    'Leg1': {'action': 'SELL', 'option_type': -1, 'expiry_offset': 0, 'strike_offset': 1}  # Sell ATM Call\n",
    "    }   # Buy 1DTE call\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cbbf5265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "option_ts\n",
       "2024-01-04 11:47:00   470.14000000\n",
       "2024-01-04 11:53:00   469.64000000\n",
       "2024-01-04 12:04:00   469.47000000\n",
       "2024-01-04 12:36:00   469.14000000\n",
       "2024-01-04 12:50:00   468.95000000\n",
       "2024-01-04 12:53:00   468.98000000\n",
       "2024-01-04 12:57:00   469.00000000\n",
       "2024-01-04 13:00:00   469.12000000\n",
       "2024-01-04 13:11:00   469.25000000\n",
       "2024-01-05 09:36:00   468.15000000\n",
       "2024-01-05 09:38:00   468.14000000\n",
       "2024-01-05 09:39:00   468.20000000\n",
       "2024-01-05 09:40:00   467.94000000\n",
       "2024-01-05 09:41:00   467.67000000\n",
       "2024-01-05 09:42:00   467.92000000\n",
       "2024-01-05 09:43:00   467.99000000\n",
       "2024-01-05 09:44:00   468.06000000\n",
       "2024-01-05 09:45:00   468.01000000\n",
       "2024-01-05 09:46:00   468.37000000\n",
       "2024-01-05 09:47:00   468.38000000\n",
       "2024-01-05 09:49:00   468.32000000\n",
       "2024-01-05 09:51:00   468.42000000\n",
       "2024-01-05 09:52:00   468.47000000\n",
       "2024-01-05 09:54:00   468.50000000\n",
       "2024-01-05 09:55:00   468.40000000\n",
       "2024-01-05 09:57:00   468.19000000\n",
       "2024-01-05 09:58:00   468.12000000\n",
       "2024-01-05 09:59:00   468.11000000\n",
       "2024-01-05 10:04:00   469.66000000\n",
       "2024-01-05 10:22:00   469.40000000\n",
       "2024-01-05 10:44:00   469.98000000\n",
       "2024-01-05 11:03:00   470.30000000\n",
       "2024-01-05 11:49:00   468.73000000\n",
       "2024-01-05 11:58:00   468.70000000\n",
       "2024-01-05 12:02:00   468.65000000\n",
       "2024-01-05 12:03:00   468.58000000\n",
       "2024-01-05 12:04:00   468.64000000\n",
       "2024-01-05 12:06:00   468.56000000\n",
       "2024-01-05 12:08:00   468.23000000\n",
       "2024-01-05 12:09:00   467.81000000\n",
       "2024-01-05 12:10:00   467.70000000\n",
       "2024-01-05 12:11:00   467.53000000\n",
       "2024-01-05 12:12:00   467.63000000\n",
       "2024-01-05 12:13:00   467.87000000\n",
       "2024-01-05 12:15:00   467.89000000\n",
       "2024-01-05 12:16:00   467.83000000\n",
       "2024-01-05 12:17:00   467.95000000\n",
       "2024-01-05 12:19:00   467.78000000\n",
       "2024-01-05 12:20:00   467.61000000\n",
       "2024-01-05 12:21:00   467.72000000\n",
       "Name: SPY_close, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com_df['SPY_close'].iloc[150:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b487b9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total symbols found after all filtering:  2\n",
      "Calculating 3-day MA for 2 symbols...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ifol = r\"C:\\Data\\Aggregate_Data\\Options\\USA\\SPY_1min_2024_2025_ORATS_close.parquet\"\n",
    "#== read the RELIANCE data\n",
    "\n",
    "com_df2 = pd.read_parquet(ifol)\n",
    "\n",
    "com_df2 = com_df2.drop_duplicates(keep='first')\n",
    "\n",
    "com_df2.dropna(how='all', inplace=True)\n",
    "\n",
    "vix_df = pd.read_parquet(r\"C:\\Data\\Aggregate_Data\\VIX_1_min_2019_2025.parquet\")\n",
    "com_df2 = com_df2.join(vix_df)\n",
    "\n",
    "\n",
    "final_symbols = com_df2.filter(like='close').columns.str.replace('_close', '')\n",
    "print(\"Total symbols found after all filtering: \", len(final_symbols))\n",
    "\n",
    "# Calculate the number of rows in the DataFrame\n",
    "\n",
    "com_df2[[f\"{symbol}_intraday_low\" for symbol in final_symbols]] =com_df2[[f\"{symbol}_close\" for symbol in final_symbols]].apply(lambda x: intraday_low(com_df2, low_col=x.name,agg_func='min'))\n",
    "\n",
    "com_df2[[f\"{symbol}_intraday_high\" for symbol in final_symbols]] =com_df2[[f\"{symbol}_close\" for symbol in final_symbols]].apply(lambda x: intraday_high(com_df2, high_col=x.name,agg_func='max'))\n",
    "\n",
    "com_df2[[f\"{symbol}_todayo\" for symbol in final_symbols]] = com_df2[[f'{symbol}_close' for symbol in final_symbols]].resample('D').transform('first')\n",
    "\n",
    "# com_df2[[f\"{symbol}_daily_ROC\" for symbol in final_symbols]] = com_df2[[f\"{symbol}_close\" for symbol in final_symbols]].pct_change(periods=50)\n",
    "\n",
    "# com_df2[[f\"{symbol}_ROC\" for symbol in final_symbols]] = com_df2[[f\"{symbol}_close\" for symbol in final_symbols]].pct_change()\n",
    "\n",
    "# com_df2[[f\"{symbol}_ROC_4\" for symbol in final_symbols]] = com_df2[[f\"{symbol}_close\" for symbol in final_symbols]].pct_change(periods=4)\n",
    "\n",
    "com_df2[[f\"{symbol}_prevdayl\" for symbol in final_symbols]] = com_df2[[f\"{symbol}_close\" for symbol in final_symbols]].apply(lambda x: get_x_day_low_numba(com_df2, n=1, column=x.name))\n",
    "\n",
    "com_df2[[f\"{symbol}_prevdayh\" for symbol in final_symbols]] = com_df2[[f\"{symbol}_close\" for symbol in final_symbols]].apply(lambda x: get_x_day_high_numba(com_df2, n=1, column=x.name))\n",
    "\n",
    "com_df2[[f\"{symbol}_secdayh\" for symbol in final_symbols]] = com_df2[[f\"{symbol}_close\" for symbol in final_symbols]].apply(lambda x: get_x_day_high_numba(com_df2, n=2, column=x.name))\n",
    "\n",
    "\n",
    "# This single call does it for the 20-day MA\n",
    "ma_20d_df = calculate_daily_ma_vectorized(df=com_df2, symbols=final_symbols, periods=3 , agg_func='last' , col_suffix='close')\n",
    "\n",
    "    # Join the new features back to the main DataFrame\n",
    "com_df2 = pd.concat([com_df2, ma_20d_df] , axis=1 , join='outer')\n",
    "\n",
    "\n",
    "com_df2[[f\"{symbol}_ma\" for symbol in final_symbols]] = com_df2[[f\"{symbol}_close\" for symbol in final_symbols]].rolling(2).mean()\n",
    "\n",
    "com_df2[[f\"{symbol}_ROC_30\" for symbol in final_symbols]] = com_df2[[f\"{symbol}_ma\" for symbol in final_symbols]].pct_change(periods=30)\n",
    "\n",
    "\n",
    "#com_df2[[f\"{symbol}_ma_3d_low\" for symbol in final_symbols]] = com_df2[[f\"{symbol}_ma\" for symbol in final_symbols]].values / com_df2[[f\"{symbol}_3d_MA\" for symbol in final_symbols]].values\n",
    "\n",
    "com_df2[[f\"{symbol}_prevdayc\" for symbol in final_symbols]] = com_df2[[f\"{symbol}_close\" for symbol in final_symbols]].apply(lambda x: calculate_prev_day_close(com_df2, close_col=x.name, n_days=1))\n",
    "\n",
    "com_df2[[f\"{symbol}_secdayc\" for symbol in final_symbols]] = com_df2[[f\"{symbol}_close\" for symbol in final_symbols]].apply(lambda x: calculate_prev_day_close(com_df2, close_col=x.name, n_days=2))\n",
    "\n",
    "\n",
    "# #=== hourly volume\n",
    "# com_df2[[f\"{symbol}_vol_ma\" for symbol in final_symbols]] = com_df2[[f\"{symbol}_volume\" for symbol in final_symbols]].rolling(4).mean()\n",
    "\n",
    "# #==avg 3 day volume\n",
    "\n",
    "# com_df2[[f\"{symbol}_3d_vol_MA\" for symbol in final_symbols]] = com_df2[[f\"{symbol}_volume\" for symbol in final_symbols]].apply(lambda x: daily_moving_average(com_df2, timeframe='1D', column=x.name, periods=3, agg_func='last'))\n",
    "\n",
    "com_df2[[f\"{symbol}_ma2_low\" for symbol in final_symbols]] = com_df2[[f\"{symbol}_close\" for symbol in final_symbols]].rolling(5).mean()\n",
    "\n",
    "\n",
    "com_df2[[f\"{symbol}_3dlow\" for symbol in final_symbols]] = com_df2[[f\"{symbol}_ma2_low\" for symbol in final_symbols]].apply(lambda x: get_x_day_low_numba(com_df2, n=3, column=x.name))\n",
    "\n",
    "com_df2[[f\"{symbol}_3dhigh\" for symbol in final_symbols]] = com_df2[[f\"{symbol}_ma2_low\" for symbol in final_symbols]].apply(lambda x: get_x_day_high_numba(com_df2, n=3, column=x.name))\n",
    "\n",
    "\n",
    "\n",
    "# 1. Get the lists of column names\n",
    "high_3d_cols = [f\"{symbol}_3dhigh\" for symbol in final_symbols]\n",
    "low_3d_cols = [f\"{symbol}_3dlow\" for symbol in final_symbols]\n",
    "\n",
    "# 2. Perform the calculation on NumPy arrays\n",
    "range_values = (com_df2[high_3d_cols].values / com_df2[low_3d_cols].values) - 1\n",
    "\n",
    "# 3. Create a new DataFrame from the results\n",
    "# This is the key step to avoid the error.\n",
    "range_cols = [f\"{symbol}_3d_rng\" for symbol in final_symbols]\n",
    "range_df = pd.DataFrame(range_values, index=com_df2.index, columns=range_cols)\n",
    "\n",
    "# 4. Join the new DataFrame back to the main one\n",
    "com_df2 = com_df2.join(range_df)\n",
    "\n",
    "\n",
    "# 1. Define the column groups\n",
    "close_cols = [f\"{symbol}_close\" for symbol in final_symbols]\n",
    "low_3d_cols = [f\"{symbol}_3dlow\" for symbol in final_symbols]\n",
    "high_3d_cols = [f\"{symbol}_3dhigh\" for symbol in final_symbols]\n",
    "\n",
    "# 2. Perform the calculation in a more chained and memory-conscious way\n",
    "# Using .to_numpy(dtype=np.float32) can cut memory usage by 50%\n",
    "numerator = com_df2[close_cols].to_numpy(dtype=np.float32) - com_df2[low_3d_cols].to_numpy(dtype=np.float32)\n",
    "denominator = com_df2[high_3d_cols].to_numpy(dtype=np.float32) - com_df2[low_3d_cols].to_numpy(dtype=np.float32)\n",
    "\n",
    "# np.divide handles division by zero safely\n",
    "percentile_values = np.divide(numerator, denominator, where=denominator != 0)\n",
    "\n",
    "# 3. Create the new feature DataFrame\n",
    "pctl_cols = [f\"{symbol}_3d_Pctl\" for symbol in final_symbols]\n",
    "percentile_df = pd.DataFrame(percentile_values, index=com_df2.index, columns=pctl_cols)\n",
    "\n",
    "# 4. Clip values and join back (same as your code)\n",
    "percentile_df = percentile_df.clip(0, 1)\n",
    "com_df2 = com_df2.join(percentile_df)\n",
    "\n",
    "com_df2['decent'] = np.where((com_df2['SPY_todayo'] < com_df2['SPY_prevdayc'] * 1.003) & (com_df2['SPY_todayo'] > com_df2['SPY_prevdayc'] * 0.98), 1, 0)\n",
    "com_df = com_df2[com_df2['decent']==1]\n",
    "com_df = com_df[com_df.index.year>=2024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87a7f44e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "option_ts\n",
       "2024-01-03 09:30:00   470.46000000\n",
       "2024-01-03 09:31:00   470.37000000\n",
       "2024-01-03 09:32:00   470.47000000\n",
       "2024-01-03 09:33:00   470.23000000\n",
       "2024-01-03 09:34:00   470.15000000\n",
       "                          ...     \n",
       "2024-12-23 13:16:00   593.32000000\n",
       "2024-12-23 13:22:00   593.22000000\n",
       "2024-12-23 13:23:00   592.97000000\n",
       "2024-12-23 13:24:00   593.05000000\n",
       "2024-12-23 13:25:00   592.96000000\n",
       "Name: SPY_close, Length: 7217, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com_df['SPY_close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e277f78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_market_calendars as mcal\n",
    "\n",
    "def fetch_option_data_for_n_days(con, ticker, end_date_str, n_days, strike, expiry_str, option_type):\n",
    "    \"\"\"\n",
    "    Fetches a range of minute-level data for a single option contract\n",
    "    for a specified number of business days ending on a given date.\n",
    "\n",
    "    Args:\n",
    "        con: The DuckDB connection object.\n",
    "        ticker (str): The stock ticker (e.g., 'SPY').\n",
    "        end_date_str (str): The last day to query, in 'YYYY-MM-DD' format.\n",
    "        n_days (int): The number of business days of data to fetch.\n",
    "        strike (float): The specific strike price.\n",
    "        expiry_str (str): The expiry date to filter for, in 'YYYY-MM-DD' format.\n",
    "        option_type (int): The type of option to fetch (1 for Calls, 0 for Puts).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the minute-by-minute data.\n",
    "    \"\"\"\n",
    "    option_name = \"Call\" if option_type == 1 else \"Put\"\n",
    "    #print(f\"Fetching last {n_days} b-days of data ending on {end_date_str} for {ticker} {strike} {option_name} expiring {expiry_str}...\")\n",
    "    \n",
    "    try:\n",
    "        # --- Business Day Calculation Logic ---\n",
    "        # 1. Get the market calendar\n",
    "        nyse = mcal.get_calendar('NYSE')\n",
    "        \n",
    "        # 2. To be safe, find a start date far enough in the past\n",
    "        #    (n * 2 is a safe buffer for weekends/holidays)\n",
    "        start_buffer = pd.to_datetime(end_date_str) - pd.Timedelta(days=n_days * 2)\n",
    "        \n",
    "        # 3. Generate a schedule of valid trading days\n",
    "        schedule = nyse.schedule(start_date=start_buffer, end_date=end_date_str)\n",
    "        \n",
    "        # 4. Get the last 'n_days' from the schedule and format them\n",
    "        business_day_list = [d.strftime('%Y-%m-%d') for d in schedule.index[-n_days:]]\n",
    "        \n",
    "        # --- Query Logic (remains the same) ---\n",
    "        path_list = [\n",
    "            f\"'s3://duckdata/ORATS/Options/ticker={ticker}/day={d}/*.parquet'\"\n",
    "            for d in business_day_list\n",
    "        ]\n",
    "        \n",
    "        query = f\"\"\"\n",
    "            SELECT ts, strike, expiry, dte, optionType, volume, oi, close,\n",
    "                bidPrice, askPrice, iv, stockPrice, ticker\n",
    "            FROM read_parquet([{\",\".join(path_list)}])\n",
    "            WHERE \n",
    "                CAST(strike AS FLOAT) = {strike} AND\n",
    "                expiry = '{expiry_str}' AND\n",
    "                optionType = {option_type}\n",
    "            ORDER BY ts;\n",
    "        \"\"\"\n",
    "        \n",
    "        multi_day_df = con.execute(query).df()\n",
    "        \n",
    "        if multi_day_df.empty:\n",
    "            print(\"INFO: No data found for this contract in the specified date range.\")\n",
    "        \n",
    "        multi_day_df['ts'] = pd.to_datetime(multi_day_df['ts'].dt.tz_localize('UTC').dt.tz_convert('America/New_York').dt.tz_localize(None))\n",
    "        multi_day_df.set_index('ts' , inplace=True)\n",
    "        return multi_day_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# ==============================================================================\n",
    "# --- Example Usage ---\n",
    "# ==============================================================================\n",
    "# Assume 'con' is your active DuckDB connection\n",
    "\n",
    "# Get data for the 5 business days ending on September 12, 2025\n",
    "multi_day_df = fetch_option_data_for_n_days(\n",
    "    con=con,\n",
    "    ticker='SPY',\n",
    "    end_date_str='2024-04-12', # The last day of the range\n",
    "    n_days=3,                  # How many business days to go back\n",
    "    strike=510.0,\n",
    "    expiry_str='2024-04-12',\n",
    "    option_type=1              # 1 for Call\n",
    ")\n",
    "\n",
    "if not multi_day_df.empty:\n",
    "    print(\"\\n--- Found Data for Date Range ---\")\n",
    "    print(multi_day_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61824784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def fetch_bulk_option_data(con, ticker, end_date_str, n_days, strikes: List[float], expiries: List[str], option_type):\n",
    "    \"\"\"\n",
    "    Fetches a range of minute-level data for MULTIPLE option contracts\n",
    "    in a single, efficient query.\n",
    "    \"\"\"\n",
    "    # 1. Get the list of business days to query (same as before)\n",
    "    nyse = mcal.get_calendar('NYSE')\n",
    "    start_buffer = pd.to_datetime(end_date_str) - pd.Timedelta(days=n_days)\n",
    "    schedule = nyse.schedule(start_date=start_buffer, end_date=end_date_str)\n",
    "    business_day_list = [d.strftime('%Y-%m-%d') for d in schedule.index[-n_days:]]\n",
    "    \n",
    "    # 2. Build the list of S3 paths (same as before)\n",
    "    path_list = [\n",
    "        f\"'s3://duckdata/ORATS/Options/ticker={ticker}/day={d}/*.parquet'\"\n",
    "        for d in business_day_list ]\n",
    "    \n",
    "    # ▼▼▼ NEW QUERY LOGIC ▼▼▼\n",
    "    # 3. Format the lists for the SQL 'IN' clause\n",
    "    strikes_str = \",\".join(map(str, strikes)) # For numbers: 470.0,471.0,472.0\n",
    "    expiries_str = \",\".join([f\"'{e}'\" for e in expiries]) # For strings: '2024-01-05','2024-01-08'\n",
    "    \n",
    "    query = f\"\"\"\n",
    "        SELECT ts, strike, expiry, close, bidPrice, askPrice, volume, oi, dte, optionType , iv\n",
    "        FROM read_parquet([{\",\".join(path_list)}])\n",
    "        WHERE \n",
    "            CAST(strike AS FLOAT) IN ({strikes_str}) AND\n",
    "            expiry IN ({expiries_str}) AND\n",
    "            optionType = {option_type}\n",
    "        ORDER BY ts;\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        bulk_df = con.execute(query).df()\n",
    "        \n",
    "        if bulk_df.empty:\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        # Perform timezone conversion and set index (same as before)\n",
    "        bulk_df['ts'] = pd.to_datetime(bulk_df['ts'].dt.tz_localize('UTC').dt.tz_convert('America/New_York').dt.tz_localize(None))\n",
    "        bulk_df.set_index('ts' , inplace=True)\n",
    "        return bulk_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during bulk fetch: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e149d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting backtest for 147 unique days...\n",
      "\n",
      "--- Processing Day: 2024-01-05 ---\n",
      "Underlying data is                        SPY_close    VIX_open    VIX_high     VIX_low  \\\n",
      "option_ts                                                              \n",
      "2024-01-05 09:36:00 468.15000000 13.59000000 13.60000000 13.59000000   \n",
      "2024-01-05 09:38:00 468.14000000 13.58000000 13.59000000 13.58000000   \n",
      "2024-01-05 09:39:00 468.20000000 13.57000000 13.61000000 13.56000000   \n",
      "2024-01-05 09:40:00 467.94000000 13.63000000 13.67000000 13.63000000   \n",
      "2024-01-05 09:41:00 467.67000000 13.65000000 13.68000000 13.65000000   \n",
      "...                          ...         ...         ...         ...   \n",
      "2024-01-05 13:22:00 467.01000000 13.68000000 13.68000000 13.66000000   \n",
      "2024-01-05 13:24:00 466.97000000 13.68000000 13.68000000 13.66000000   \n",
      "2024-01-05 13:25:00 467.12000000 13.65000000 13.65000000 13.60000000   \n",
      "2024-01-05 13:26:00 467.37000000 13.59000000 13.59000000 13.57000000   \n",
      "2024-01-05 13:28:00 467.54000000 13.57000000 13.57000000 13.53000000   \n",
      "\n",
      "                      VIX_close  SPY_intraday_low  VIX_intraday_low  \\\n",
      "option_ts                                                             \n",
      "2024-01-05 09:36:00 13.59000000      468.15000000       13.59000000   \n",
      "2024-01-05 09:38:00 13.59000000      468.14000000       13.59000000   \n",
      "2024-01-05 09:39:00 13.61000000      468.14000000       13.59000000   \n",
      "2024-01-05 09:40:00 13.67000000      467.94000000       13.59000000   \n",
      "2024-01-05 09:41:00 13.67000000      467.67000000       13.59000000   \n",
      "...                         ...               ...               ...   \n",
      "2024-01-05 13:22:00 13.67000000      466.74000000       13.30000000   \n",
      "2024-01-05 13:24:00 13.66000000      466.74000000       13.30000000   \n",
      "2024-01-05 13:25:00 13.60000000      466.74000000       13.30000000   \n",
      "2024-01-05 13:26:00 13.57000000      466.74000000       13.30000000   \n",
      "2024-01-05 13:28:00 13.53000000      466.74000000       13.30000000   \n",
      "\n",
      "                     SPY_intraday_high  VIX_intraday_high   SPY_todayo  ...  \\\n",
      "option_ts                                                               ...   \n",
      "2024-01-05 09:36:00       468.15000000        13.59000000 468.15000000  ...   \n",
      "2024-01-05 09:38:00       468.15000000        13.59000000 468.15000000  ...   \n",
      "2024-01-05 09:39:00       468.20000000        13.61000000 468.15000000  ...   \n",
      "2024-01-05 09:40:00       468.20000000        13.67000000 468.15000000  ...   \n",
      "2024-01-05 09:41:00       468.20000000        13.67000000 468.15000000  ...   \n",
      "...                                ...                ...          ...  ...   \n",
      "2024-01-05 13:22:00       470.30000000        13.72000000 468.15000000  ...   \n",
      "2024-01-05 13:24:00       470.30000000        13.72000000 468.15000000  ...   \n",
      "2024-01-05 13:25:00       470.30000000        13.72000000 468.15000000  ...   \n",
      "2024-01-05 13:26:00       470.30000000        13.72000000 468.15000000  ...   \n",
      "2024-01-05 13:28:00       470.30000000        13.72000000 468.15000000  ...   \n",
      "\n",
      "                     VIX_ma2_low    SPY_3dlow   VIX_3dlow   SPY_3dhigh  \\\n",
      "option_ts                                                                \n",
      "2024-01-05 09:36:00  13.90000000 468.61600000 13.29000000 473.49200000   \n",
      "2024-01-05 09:38:00  13.82400000 468.61600000 13.29000000 473.49200000   \n",
      "2024-01-05 09:39:00  13.74600000 468.61600000 13.29000000 473.49200000   \n",
      "2024-01-05 09:40:00  13.68200000 468.61600000 13.29000000 473.49200000   \n",
      "2024-01-05 09:41:00  13.62600000 468.61600000 13.29000000 473.49200000   \n",
      "...                          ...          ...         ...          ...   \n",
      "2024-01-05 13:22:00  13.69600000 468.61600000 13.29000000 473.49200000   \n",
      "2024-01-05 13:24:00  13.68400000 468.61600000 13.29000000 473.49200000   \n",
      "2024-01-05 13:25:00  13.66000000 468.61600000 13.29000000 473.49200000   \n",
      "2024-01-05 13:26:00  13.63400000 468.61600000 13.29000000 473.49200000   \n",
      "2024-01-05 13:28:00  13.60600000 468.61600000 13.29000000 473.49200000   \n",
      "\n",
      "                     VIX_3dhigh  SPY_3d_rng  VIX_3d_rng  SPY_3d_Pctl  \\\n",
      "option_ts                                                              \n",
      "2024-01-05 09:36:00 14.19800000  0.01040511  0.06832205   0.00000000   \n",
      "2024-01-05 09:38:00 14.19800000  0.01040511  0.06832205   0.00000000   \n",
      "2024-01-05 09:39:00 14.19800000  0.01040511  0.06832205   0.00000000   \n",
      "2024-01-05 09:40:00 14.19800000  0.01040511  0.06832205   0.00000000   \n",
      "2024-01-05 09:41:00 14.19800000  0.01040511  0.06832205   0.00000000   \n",
      "...                         ...         ...         ...          ...   \n",
      "2024-01-05 13:22:00 14.19800000  0.01040511  0.06832205   0.00000000   \n",
      "2024-01-05 13:24:00 14.19800000  0.01040511  0.06832205   0.00000000   \n",
      "2024-01-05 13:25:00 14.19800000  0.01040511  0.06832205   0.00000000   \n",
      "2024-01-05 13:26:00 14.19800000  0.01040511  0.06832205   0.00000000   \n",
      "2024-01-05 13:28:00 14.19800000  0.01040511  0.06832205   0.00000000   \n",
      "\n",
      "                     VIX_3d_Pctl  decent  \n",
      "option_ts                                 \n",
      "2024-01-05 09:36:00   0.33039668       1  \n",
      "2024-01-05 09:38:00   0.33039668       1  \n",
      "2024-01-05 09:39:00   0.35242257       1  \n",
      "2024-01-05 09:40:00   0.41850233       1  \n",
      "2024-01-05 09:41:00   0.41850233       1  \n",
      "...                          ...     ...  \n",
      "2024-01-05 13:22:00   0.41850233       1  \n",
      "2024-01-05 13:24:00   0.40748885       1  \n",
      "2024-01-05 13:25:00   0.34141016       1  \n",
      "2024-01-05 13:26:00   0.30836976       1  \n",
      "2024-01-05 13:28:00   0.26431692       1  \n",
      "\n",
      "[82 rows x 38 columns]\n",
      "Fetching options across 468.15\n",
      "Total options found for today are: 11\n",
      "\n",
      "--- Phase 2: Preparing Master DataFrame ---\n",
      "✅ Successfully created master DataFrame with 110 total columns.\n",
      "  -> BUY ENTRY for P_465.0_20240105 at 12:10:00 and price 0.21\n",
      "  -> MA cross down exit of P_465.0_20240105 at 0.03. Profit: $-18000.00\n",
      "\n",
      "--- Processing Day: 2024-01-08 ---\n",
      "Underlying data is                        SPY_close    VIX_open    VIX_high     VIX_low  \\\n",
      "option_ts                                                              \n",
      "2024-01-08 09:30:00 468.43000000         NaN         NaN         NaN   \n",
      "2024-01-08 10:06:00 469.90000000 13.63000000 13.63000000 13.63000000   \n",
      "2024-01-08 10:34:00 470.18000000 13.55000000 13.55000000 13.53000000   \n",
      "2024-01-08 10:45:00 470.38000000 13.55000000 13.55000000 13.55000000   \n",
      "2024-01-08 10:53:00 470.72000000 13.53000000 13.54000000 13.53000000   \n",
      "2024-01-08 11:21:00 470.58000000 13.45000000 13.45000000 13.45000000   \n",
      "2024-01-08 11:35:00 470.53000000 13.46000000 13.46000000 13.46000000   \n",
      "2024-01-08 11:40:00 470.48000000 13.45000000 13.46000000 13.45000000   \n",
      "2024-01-08 11:43:00 470.61000000 13.45000000 13.45000000 13.44000000   \n",
      "2024-01-08 11:48:00 470.59000000 13.44000000 13.44000000 13.44000000   \n",
      "2024-01-08 12:01:00 470.94000000 13.38000000 13.38000000 13.38000000   \n",
      "2024-01-08 12:03:00 471.03000000 13.36000000 13.37000000 13.36000000   \n",
      "2024-01-08 12:04:00 471.11000000 13.36000000 13.36000000 13.36000000   \n",
      "2024-01-08 12:05:00 471.10000000 13.37000000 13.37000000 13.37000000   \n",
      "2024-01-08 12:06:00 471.02000000 13.38000000 13.38000000 13.35000000   \n",
      "2024-01-08 12:07:00 471.06000000 13.36000000 13.37000000 13.36000000   \n",
      "2024-01-08 12:08:00 471.12000000 13.37000000 13.37000000 13.37000000   \n",
      "2024-01-08 12:10:00 470.99000000 13.39000000 13.39000000 13.39000000   \n",
      "2024-01-08 12:11:00 471.07000000 13.40000000 13.40000000 13.40000000   \n",
      "2024-01-08 12:14:00 471.01000000 13.40000000 13.40000000 13.40000000   \n",
      "2024-01-08 12:15:00 470.95000000 13.39000000 13.39000000 13.38000000   \n",
      "2024-01-08 12:16:00 471.14000000 13.37000000 13.38000000 13.37000000   \n",
      "2024-01-08 12:20:00 471.08000000 13.39000000 13.40000000 13.39000000   \n",
      "2024-01-08 12:22:00 470.83000000 13.41000000 13.42000000 13.41000000   \n",
      "2024-01-08 12:25:00 470.70000000 13.40000000 13.41000000 13.40000000   \n",
      "2024-01-08 12:26:00 470.73000000 13.41000000 13.41000000 13.40000000   \n",
      "2024-01-08 12:34:00 470.90000000 13.35000000 13.35000000 13.34000000   \n",
      "2024-01-08 12:35:00 470.98000000 13.34000000 13.34000000 13.34000000   \n",
      "2024-01-08 12:40:00 471.25000000 13.33000000 13.34000000 13.33000000   \n",
      "2024-01-08 12:42:00 471.31000000 13.33000000 13.33000000 13.32000000   \n",
      "2024-01-08 12:44:00 471.24000000 13.34000000 13.35000000 13.34000000   \n",
      "2024-01-08 12:45:00 471.19000000 13.34000000 13.34000000 13.34000000   \n",
      "2024-01-08 12:47:00 471.23000000 13.35000000 13.35000000 13.35000000   \n",
      "2024-01-08 12:48:00 471.18000000 13.34000000 13.34000000 13.33000000   \n",
      "2024-01-08 12:49:00 471.22000000 13.33000000 13.33000000 13.33000000   \n",
      "2024-01-08 13:01:00 471.43000000 13.32000000 13.32000000 13.32000000   \n",
      "2024-01-08 13:10:00 471.57000000 13.34000000 13.35000000 13.34000000   \n",
      "2024-01-08 13:11:00 471.36000000 13.36000000 13.36000000 13.36000000   \n",
      "2024-01-08 13:20:00 471.39000000 13.34000000 13.34000000 13.34000000   \n",
      "2024-01-08 13:21:00 471.53000000 13.33000000 13.33000000 13.33000000   \n",
      "\n",
      "                      VIX_close  SPY_intraday_low  VIX_intraday_low  \\\n",
      "option_ts                                                             \n",
      "2024-01-08 09:30:00         NaN      468.43000000               NaN   \n",
      "2024-01-08 10:06:00 13.63000000      468.43000000       13.63000000   \n",
      "2024-01-08 10:34:00 13.53000000      468.43000000       13.53000000   \n",
      "2024-01-08 10:45:00 13.55000000      468.43000000       13.53000000   \n",
      "2024-01-08 10:53:00 13.54000000      468.43000000       13.53000000   \n",
      "2024-01-08 11:21:00 13.45000000      468.43000000       13.45000000   \n",
      "2024-01-08 11:35:00 13.46000000      468.43000000       13.45000000   \n",
      "2024-01-08 11:40:00 13.46000000      468.43000000       13.45000000   \n",
      "2024-01-08 11:43:00 13.44000000      468.43000000       13.44000000   \n",
      "2024-01-08 11:48:00 13.44000000      468.43000000       13.44000000   \n",
      "2024-01-08 12:01:00 13.38000000      468.43000000       13.38000000   \n",
      "2024-01-08 12:03:00 13.37000000      468.43000000       13.37000000   \n",
      "2024-01-08 12:04:00 13.36000000      468.43000000       13.36000000   \n",
      "2024-01-08 12:05:00 13.37000000      468.43000000       13.36000000   \n",
      "2024-01-08 12:06:00 13.35000000      468.43000000       13.35000000   \n",
      "2024-01-08 12:07:00 13.37000000      468.43000000       13.35000000   \n",
      "2024-01-08 12:08:00 13.37000000      468.43000000       13.35000000   \n",
      "2024-01-08 12:10:00 13.39000000      468.43000000       13.35000000   \n",
      "2024-01-08 12:11:00 13.40000000      468.43000000       13.35000000   \n",
      "2024-01-08 12:14:00 13.40000000      468.43000000       13.35000000   \n",
      "2024-01-08 12:15:00 13.38000000      468.43000000       13.35000000   \n",
      "2024-01-08 12:16:00 13.38000000      468.43000000       13.35000000   \n",
      "2024-01-08 12:20:00 13.40000000      468.43000000       13.35000000   \n",
      "2024-01-08 12:22:00 13.42000000      468.43000000       13.35000000   \n",
      "2024-01-08 12:25:00 13.40000000      468.43000000       13.35000000   \n",
      "2024-01-08 12:26:00 13.40000000      468.43000000       13.35000000   \n",
      "2024-01-08 12:34:00 13.34000000      468.43000000       13.34000000   \n",
      "2024-01-08 12:35:00 13.34000000      468.43000000       13.34000000   \n",
      "2024-01-08 12:40:00 13.33000000      468.43000000       13.33000000   \n",
      "2024-01-08 12:42:00 13.32000000      468.43000000       13.32000000   \n",
      "2024-01-08 12:44:00 13.35000000      468.43000000       13.32000000   \n",
      "2024-01-08 12:45:00 13.34000000      468.43000000       13.32000000   \n",
      "2024-01-08 12:47:00 13.35000000      468.43000000       13.32000000   \n",
      "2024-01-08 12:48:00 13.33000000      468.43000000       13.32000000   \n",
      "2024-01-08 12:49:00 13.33000000      468.43000000       13.32000000   \n",
      "2024-01-08 13:01:00 13.32000000      468.43000000       13.32000000   \n",
      "2024-01-08 13:10:00 13.35000000      468.43000000       13.32000000   \n",
      "2024-01-08 13:11:00 13.36000000      468.43000000       13.32000000   \n",
      "2024-01-08 13:20:00 13.34000000      468.43000000       13.32000000   \n",
      "2024-01-08 13:21:00 13.33000000      468.43000000       13.32000000   \n",
      "\n",
      "                     SPY_intraday_high  VIX_intraday_high   SPY_todayo  ...  \\\n",
      "option_ts                                                               ...   \n",
      "2024-01-08 09:30:00       468.43000000                NaN 468.43000000  ...   \n",
      "2024-01-08 10:06:00       469.90000000        13.63000000 468.43000000  ...   \n",
      "2024-01-08 10:34:00       470.18000000        13.63000000 468.43000000  ...   \n",
      "2024-01-08 10:45:00       470.38000000        13.63000000 468.43000000  ...   \n",
      "2024-01-08 10:53:00       470.72000000        13.63000000 468.43000000  ...   \n",
      "2024-01-08 11:21:00       470.72000000        13.63000000 468.43000000  ...   \n",
      "2024-01-08 11:35:00       470.72000000        13.63000000 468.43000000  ...   \n",
      "2024-01-08 11:40:00       470.72000000        13.63000000 468.43000000  ...   \n",
      "2024-01-08 11:43:00       470.72000000        13.63000000 468.43000000  ...   \n",
      "2024-01-08 11:48:00       470.72000000        13.63000000 468.43000000  ...   \n",
      "2024-01-08 12:01:00       470.94000000        13.63000000 468.43000000  ...   \n",
      "2024-01-08 12:03:00       471.03000000        13.63000000 468.43000000  ...   \n",
      "2024-01-08 12:04:00       471.11000000        13.63000000 468.43000000  ...   \n",
      "2024-01-08 12:05:00       471.11000000        13.63000000 468.43000000  ...   \n",
      "2024-01-08 12:06:00       471.11000000        13.63000000 468.43000000  ...   \n",
      "2024-01-08 12:07:00       471.11000000        13.63000000 468.43000000  ...   \n",
      "2024-01-08 12:08:00       471.12000000        13.63000000 468.43000000  ...   \n",
      "2024-01-08 12:10:00       471.12000000        13.63000000 468.43000000  ...   \n",
      "2024-01-08 12:11:00       471.12000000        13.63000000 468.43000000  ...   \n",
      "2024-01-08 12:14:00       471.12000000        13.63000000 468.43000000  ...   \n",
      "2024-01-08 12:15:00       471.12000000        13.63000000 468.43000000  ...   \n",
      "2024-01-08 12:16:00       471.14000000        13.63000000 468.43000000  ...   \n",
      "2024-01-08 12:20:00       471.14000000        13.63000000 468.43000000  ...   \n",
      "2024-01-08 12:22:00       471.14000000        13.63000000 468.43000000  ...   \n",
      "2024-01-08 12:25:00       471.14000000        13.63000000 468.43000000  ...   \n",
      "2024-01-08 12:26:00       471.14000000        13.63000000 468.43000000  ...   \n",
      "2024-01-08 12:34:00       471.14000000        13.63000000 468.43000000  ...   \n",
      "2024-01-08 12:35:00       471.14000000        13.63000000 468.43000000  ...   \n",
      "2024-01-08 12:40:00       471.25000000        13.63000000 468.43000000  ...   \n",
      "2024-01-08 12:42:00       471.31000000        13.63000000 468.43000000  ...   \n",
      "2024-01-08 12:44:00       471.31000000        13.63000000 468.43000000  ...   \n",
      "2024-01-08 12:45:00       471.31000000        13.63000000 468.43000000  ...   \n",
      "2024-01-08 12:47:00       471.31000000        13.63000000 468.43000000  ...   \n",
      "2024-01-08 12:48:00       471.31000000        13.63000000 468.43000000  ...   \n",
      "2024-01-08 12:49:00       471.31000000        13.63000000 468.43000000  ...   \n",
      "2024-01-08 13:01:00       471.43000000        13.63000000 468.43000000  ...   \n",
      "2024-01-08 13:10:00       471.57000000        13.63000000 468.43000000  ...   \n",
      "2024-01-08 13:11:00       471.57000000        13.63000000 468.43000000  ...   \n",
      "2024-01-08 13:20:00       471.57000000        13.63000000 468.43000000  ...   \n",
      "2024-01-08 13:21:00       471.57000000        13.63000000 468.43000000  ...   \n",
      "\n",
      "                     VIX_ma2_low    SPY_3dlow   VIX_3dlow   SPY_3dhigh  \\\n",
      "option_ts                                                                \n",
      "2024-01-08 09:30:00          NaN 466.82200000 13.38800000 472.27200000   \n",
      "2024-01-08 10:06:00          NaN 466.82200000 13.38800000 472.27200000   \n",
      "2024-01-08 10:34:00          NaN 466.82200000 13.38800000 472.27200000   \n",
      "2024-01-08 10:45:00          NaN 466.82200000 13.38800000 472.27200000   \n",
      "2024-01-08 10:53:00          NaN 466.82200000 13.38800000 472.27200000   \n",
      "2024-01-08 11:21:00  13.54000000 466.82200000 13.38800000 472.27200000   \n",
      "2024-01-08 11:35:00  13.50600000 466.82200000 13.38800000 472.27200000   \n",
      "2024-01-08 11:40:00  13.49200000 466.82200000 13.38800000 472.27200000   \n",
      "2024-01-08 11:43:00  13.47000000 466.82200000 13.38800000 472.27200000   \n",
      "2024-01-08 11:48:00  13.45000000 466.82200000 13.38800000 472.27200000   \n",
      "2024-01-08 12:01:00  13.43600000 466.82200000 13.38800000 472.27200000   \n",
      "2024-01-08 12:03:00  13.41800000 466.82200000 13.38800000 472.27200000   \n",
      "2024-01-08 12:04:00  13.39800000 466.82200000 13.38800000 472.27200000   \n",
      "2024-01-08 12:05:00  13.38400000 466.82200000 13.38800000 472.27200000   \n",
      "2024-01-08 12:06:00  13.36600000 466.82200000 13.38800000 472.27200000   \n",
      "2024-01-08 12:07:00  13.36400000 466.82200000 13.38800000 472.27200000   \n",
      "2024-01-08 12:08:00  13.36400000 466.82200000 13.38800000 472.27200000   \n",
      "2024-01-08 12:10:00  13.37000000 466.82200000 13.38800000 472.27200000   \n",
      "2024-01-08 12:11:00  13.37600000 466.82200000 13.38800000 472.27200000   \n",
      "2024-01-08 12:14:00  13.38600000 466.82200000 13.38800000 472.27200000   \n",
      "2024-01-08 12:15:00  13.38800000 466.82200000 13.38800000 472.27200000   \n",
      "2024-01-08 12:16:00  13.39000000 466.82200000 13.38800000 472.27200000   \n",
      "2024-01-08 12:20:00  13.39200000 466.82200000 13.38800000 472.27200000   \n",
      "2024-01-08 12:22:00  13.39600000 466.82200000 13.38800000 472.27200000   \n",
      "2024-01-08 12:25:00  13.39600000 466.82200000 13.38800000 472.27200000   \n",
      "2024-01-08 12:26:00  13.40000000 466.82200000 13.38800000 472.27200000   \n",
      "2024-01-08 12:34:00  13.39200000 466.82200000 13.38800000 472.27200000   \n",
      "2024-01-08 12:35:00  13.38000000 466.82200000 13.38800000 472.27200000   \n",
      "2024-01-08 12:40:00  13.36200000 466.82200000 13.38800000 472.27200000   \n",
      "2024-01-08 12:42:00  13.34600000 466.82200000 13.38800000 472.27200000   \n",
      "2024-01-08 12:44:00  13.33600000 466.82200000 13.38800000 472.27200000   \n",
      "2024-01-08 12:45:00  13.33600000 466.82200000 13.38800000 472.27200000   \n",
      "2024-01-08 12:47:00  13.33800000 466.82200000 13.38800000 472.27200000   \n",
      "2024-01-08 12:48:00  13.33800000 466.82200000 13.38800000 472.27200000   \n",
      "2024-01-08 12:49:00  13.34000000 466.82200000 13.38800000 472.27200000   \n",
      "2024-01-08 13:01:00  13.33400000 466.82200000 13.38800000 472.27200000   \n",
      "2024-01-08 13:10:00  13.33600000 466.82200000 13.38800000 472.27200000   \n",
      "2024-01-08 13:11:00  13.33800000 466.82200000 13.38800000 472.27200000   \n",
      "2024-01-08 13:20:00  13.34000000 466.82200000 13.38800000 472.27200000   \n",
      "2024-01-08 13:21:00  13.34000000 466.82200000 13.38800000 472.27200000   \n",
      "\n",
      "                     VIX_3dhigh  SPY_3d_rng  VIX_3d_rng  SPY_3d_Pctl  \\\n",
      "option_ts                                                              \n",
      "2024-01-08 09:30:00 14.19800000  0.01167469  0.06050194   0.29504552   \n",
      "2024-01-08 10:06:00 14.19800000  0.01167469  0.06050194   0.56476992   \n",
      "2024-01-08 10:34:00 14.19800000  0.01167469  0.06050194   0.61614573   \n",
      "2024-01-08 10:45:00 14.19800000  0.01167469  0.06050194   0.65284514   \n",
      "2024-01-08 10:53:00 14.19800000  0.01167469  0.06050194   0.71522963   \n",
      "2024-01-08 11:21:00 14.19800000  0.01167469  0.06050194   0.68953896   \n",
      "2024-01-08 11:35:00 14.19800000  0.01167469  0.06050194   0.68036687   \n",
      "2024-01-08 11:40:00 14.19800000  0.01167469  0.06050194   0.67119485   \n",
      "2024-01-08 11:43:00 14.19800000  0.01167469  0.06050194   0.69504327   \n",
      "2024-01-08 11:48:00 14.19800000  0.01167469  0.06050194   0.69137561   \n",
      "2024-01-08 12:01:00 14.19800000  0.01167469  0.06050194   0.75559676   \n",
      "2024-01-08 12:03:00 14.19800000  0.01167469  0.06050194   0.77210981   \n",
      "2024-01-08 12:04:00 14.19800000  0.01167469  0.06050194   0.78678620   \n",
      "2024-01-08 12:05:00 14.19800000  0.01167469  0.06050194   0.78495514   \n",
      "2024-01-08 12:06:00 14.19800000  0.01167469  0.06050194   0.77027315   \n",
      "2024-01-08 12:07:00 14.19800000  0.01167469  0.06050194   0.77761418   \n",
      "2024-01-08 12:08:00 14.19800000  0.01167469  0.06050194   0.78862286   \n",
      "2024-01-08 12:10:00 14.19800000  0.01167469  0.06050194   0.76476878   \n",
      "2024-01-08 12:11:00 14.19800000  0.01167469  0.06050194   0.77945077   \n",
      "2024-01-08 12:14:00 14.19800000  0.01167469  0.06050194   0.76844209   \n",
      "2024-01-08 12:15:00 14.19800000  0.01167469  0.06050194   0.75743341   \n",
      "2024-01-08 12:16:00 14.19800000  0.01167469  0.06050194   0.79229617   \n",
      "2024-01-08 12:20:00 14.19800000  0.01167469  0.06050194   0.78128183   \n",
      "2024-01-08 12:22:00 14.19800000  0.01167469  0.06050194   0.73541039   \n",
      "2024-01-08 12:25:00 14.19800000  0.01167469  0.06050194   0.71156192   \n",
      "2024-01-08 12:26:00 14.19800000  0.01167469  0.06050194   0.71706629   \n",
      "2024-01-08 12:34:00 14.19800000  0.01167469  0.06050194   0.74825573   \n",
      "2024-01-08 12:35:00 14.19800000  0.01167469  0.06050194   0.76293772   \n",
      "2024-01-08 12:40:00 14.19800000  0.01167469  0.06050194   0.81247687   \n",
      "2024-01-08 12:42:00 14.19800000  0.01167469  0.06050194   0.82348561   \n",
      "2024-01-08 12:44:00 14.19800000  0.01167469  0.06050194   0.81064028   \n",
      "2024-01-08 12:45:00 14.19800000  0.01167469  0.06050194   0.80146819   \n",
      "2024-01-08 12:47:00 14.19800000  0.01167469  0.06050194   0.80880922   \n",
      "2024-01-08 12:48:00 14.19800000  0.01167469  0.06050194   0.79963154   \n",
      "2024-01-08 12:49:00 14.19800000  0.01167469  0.06050194   0.80697256   \n",
      "2024-01-08 13:01:00 14.19800000  0.01167469  0.06050194   0.84550303   \n",
      "2024-01-08 13:10:00 14.19800000  0.01167469  0.06050194   0.87119371   \n",
      "2024-01-08 13:11:00 14.19800000  0.01167469  0.06050194   0.83265764   \n",
      "2024-01-08 13:20:00 14.19800000  0.01167469  0.06050194   0.83816761   \n",
      "2024-01-08 13:21:00 14.19800000  0.01167469  0.06050194   0.86385274   \n",
      "\n",
      "                     VIX_3d_Pctl  decent  \n",
      "option_ts                                 \n",
      "2024-01-08 09:30:00          NaN       1  \n",
      "2024-01-08 10:06:00   0.29876599       1  \n",
      "2024-01-08 10:34:00   0.17530879       1  \n",
      "2024-01-08 10:45:00   0.20000070       1  \n",
      "2024-01-08 10:53:00   0.18765475       1  \n",
      "2024-01-08 11:21:00   0.07654351       1  \n",
      "2024-01-08 11:35:00   0.08888946       1  \n",
      "2024-01-08 11:40:00   0.08888946       1  \n",
      "2024-01-08 11:43:00   0.06419756       1  \n",
      "2024-01-08 11:48:00   0.06419756       1  \n",
      "2024-01-08 12:01:00   0.00000000       1  \n",
      "2024-01-08 12:03:00   0.00000000       1  \n",
      "2024-01-08 12:04:00   0.00000000       1  \n",
      "2024-01-08 12:05:00   0.00000000       1  \n",
      "2024-01-08 12:06:00   0.00000000       1  \n",
      "2024-01-08 12:07:00   0.00000000       1  \n",
      "2024-01-08 12:08:00   0.00000000       1  \n",
      "2024-01-08 12:10:00   0.00247013       1  \n",
      "2024-01-08 12:11:00   0.01481491       1  \n",
      "2024-01-08 12:14:00   0.01481491       1  \n",
      "2024-01-08 12:15:00   0.00000000       1  \n",
      "2024-01-08 12:16:00   0.00000000       1  \n",
      "2024-01-08 12:20:00   0.01481491       1  \n",
      "2024-01-08 12:22:00   0.03950682       1  \n",
      "2024-01-08 12:25:00   0.01481491       1  \n",
      "2024-01-08 12:26:00   0.01481491       1  \n",
      "2024-01-08 12:34:00   0.00000000       1  \n",
      "2024-01-08 12:35:00   0.00000000       1  \n",
      "2024-01-08 12:40:00   0.00000000       1  \n",
      "2024-01-08 12:42:00   0.00000000       1  \n",
      "2024-01-08 12:44:00   0.00000000       1  \n",
      "2024-01-08 12:45:00   0.00000000       1  \n",
      "2024-01-08 12:47:00   0.00000000       1  \n",
      "2024-01-08 12:48:00   0.00000000       1  \n",
      "2024-01-08 12:49:00   0.00000000       1  \n",
      "2024-01-08 13:01:00   0.00000000       1  \n",
      "2024-01-08 13:10:00   0.00000000       1  \n",
      "2024-01-08 13:11:00   0.00000000       1  \n",
      "2024-01-08 13:20:00   0.00000000       1  \n",
      "2024-01-08 13:21:00   0.00000000       1  \n",
      "\n",
      "[40 rows x 38 columns]\n",
      "Fetching options across 468.43\n",
      "Total options found for today are: 11\n",
      "\n",
      "--- Phase 2: Preparing Master DataFrame ---\n",
      "✅ Successfully created master DataFrame with 110 total columns.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "strike_interval = 1\n",
    "lookback_days = 1\n",
    "option_multiplier = 100\n",
    "from collections import  defaultdict\n",
    "\n",
    "# ==============================================================================\n",
    "# SCRIPT STARTS HERE\n",
    "# ==============================================================================\n",
    "\n",
    "trade_log = []\n",
    "nyse = mcal.get_calendar('NYSE') \n",
    "unique_days = com_df.index.normalize().unique()\n",
    "print(f\"🚀 Starting backtest for {len(unique_days)} unique days...\")\n",
    "\n",
    "intial_capital = 100000\n",
    "position_cap = 100000\n",
    "\n",
    "# Track local trade log\n",
    "local_trade_log2 = pd.DataFrame(columns=['name', 'Side', 'entry_date', 'entry_price' , 'qty' ,  'exit_date', 'exit_price', 'order_no','profit' ])\n",
    "\n",
    "# ✅ Initialize a list to hold all trade dictionaries.\n",
    "final_trade_log_list = []\n",
    "\n",
    "\n",
    "# --- Main Loop: Iterate Through Each Day ---\n",
    "for day in unique_days[2:4]:\n",
    "    \n",
    "    day_str = day.strftime('%Y-%m-%d')\n",
    "    print(f\"\\n--- Processing Day: {day_str} ---\")\n",
    "\n",
    "    try:\n",
    "        # Get the underlying data for today\n",
    "        daily_underlying_df = com_df[com_df.index.date == day.date()]\n",
    "        if daily_underlying_df.empty: continue\n",
    "        \n",
    "        print(f\"Underlying data is {daily_underlying_df}\")\n",
    "\n",
    "        open_price = daily_underlying_df['SPY_close'].iloc[0]\n",
    "        print(f\"Fetching options across {open_price}\")\n",
    "        \n",
    "        if pd.isna(open_price): continue\n",
    "        \n",
    "        atm_strike_open = np.round(open_price)\n",
    "\n",
    "        # ==============================================================================\n",
    "        # --- PHASE 1: PRE-FETCH ALL POTENTIAL OPTION DATA FOR THE DAY ---\n",
    "        # ==============================================================================\n",
    "        \n",
    "        strikes_to_fetch = set()\n",
    "        expiries_to_fetch = set()\n",
    "        legs_to_trade_info = {} # Store info to map data back to legs\n",
    "\n",
    "        for leg_name, params in strategy_config.items():\n",
    "            center_target_strike = atm_strike_open \n",
    "                        # 1. Calculate the lower and upper price bounds\n",
    "            lower_bound = center_target_strike * 0.99\n",
    "            upper_bound = center_target_strike * 1.01\n",
    "\n",
    "            # 2. Round the bounds to the nearest valid strike\n",
    "            rounded_lower = round_to_nearest(lower_bound, interval=strike_interval)\n",
    "            rounded_upper = round_to_nearest(upper_bound, interval=strike_interval)\n",
    "\n",
    "            # 3. Generate all strikes from the lower to upper bound, inclusive\n",
    "            all_strikes_in_range = np.arange(rounded_lower, rounded_upper + strike_interval, strike_interval)\n",
    "\n",
    "                        \n",
    "            # 4. Add all of these strikes to your set for fetching\n",
    "            strikes_to_fetch.update(all_strikes_in_range)\n",
    "\n",
    "            # print(f\"\\nDEBUG: Final unique strikes to be fetched: {sorted(list(strikes_to_fetch))}\")\n",
    "\n",
    "            schedule = nyse.schedule(start_date=day, end_date=day + pd.Timedelta(days=14))\n",
    "            target_expiry_str = schedule.index[params['expiry_offset']].strftime('%Y-%m-%d')\n",
    "            expiries_to_fetch.add(target_expiry_str)\n",
    "            \n",
    "            # Store the central parameters for this leg\n",
    "            legs_to_trade_info[leg_name] = {'strike': center_target_strike, 'expiry': target_expiry_str}\n",
    "\n",
    "        # Make one single, efficient call to fetch all data\n",
    "        bulk_data_df = fetch_bulk_option_data(\n",
    "            con=con, ticker='SPY', end_date_str=day_str, n_days=lookback_days,\n",
    "            strikes=list(strikes_to_fetch), expiries=list(expiries_to_fetch), option_type=-1\n",
    "        )\n",
    "\n",
    "        if bulk_data_df.empty:\n",
    "            print(\"Could not pre-fetch any option data for the day. Skipping.\")\n",
    "            continue\n",
    "        grps = bulk_data_df.groupby(by=['strike' , 'optionType' , 'expiry'])\n",
    "        total_legs =grps.ngroups \n",
    "        print(f\"Total options found for today are: {total_legs}\")       \n",
    "\n",
    "\n",
    "                # Assume 'grps' is your DataFrameGroupBy object and 'daily_underlying_df' has SPY data for the day\n",
    "\n",
    "        # ==============================================================================\n",
    "        # --- PHASE 2: PREPARE THE MASTER SIMULATION DATAFRAME ---\n",
    "        # ==============================================================================\n",
    "\n",
    "        print(\"\\n--- Phase 2: Preparing Master DataFrame ---\")\n",
    "\n",
    "        # 1. Create a list to hold the processed DataFrame for each option contract\n",
    "        all_option_dfs = []\n",
    "\n",
    "        # 2. Loop through each group (each unique option contract)\n",
    "        for (strike, option_type, expiry), contract_df in grps:\n",
    "            \n",
    "            # 3. Create a unique, descriptive prefix for this specific contract\n",
    "            # Example: 'C_468.0_20240105'\n",
    "            type_char = 'C' if option_type == 1 else 'P'\n",
    "            expiry_str = pd.to_datetime(expiry).strftime('%Y%m%d')\n",
    "            prefix = f\"{type_char}_{strike}_{expiry_str}_\"\n",
    "            \n",
    "            # 4. Add the prefix to all columns of this contract's DataFrame\n",
    "            prefixed_df = contract_df.add_prefix(prefix)\n",
    "            \n",
    "            # 5. Add the prepared DataFrame to our list\n",
    "            all_option_dfs.append(prefixed_df)\n",
    "\n",
    "        # 6. Concatenate all individual option DataFrames into one wide DataFrame\n",
    "        if all_option_dfs:\n",
    "            com_legs_df = pd.concat(all_option_dfs, axis=1, join='outer')\n",
    "                       \n",
    "            \n",
    "            print(f\"✅ Successfully created master DataFrame with {len(com_legs_df.columns)} total columns.\")\n",
    "            \n",
    "        else:\n",
    "            print(\"❌ No valid option groups to process.\")\n",
    "            \n",
    "\n",
    "         \n",
    "        legs_to_process  = [ symbol for symbol in com_legs_df.filter(like='_close').columns.str.replace('_close', '').unique()\n",
    "                                if symbol.startswith('C_') or symbol.startswith('P_')]\n",
    "\n",
    "        #print(legs_to_process)\n",
    "\n",
    "        com_legs_df[[f\"{legs}_intraday_low\" for legs in legs_to_process]] = com_legs_df[[f\"{legs}_bidPrice\" for legs in legs_to_process]].apply(lambda x: intraday_low(com_legs_df, low_col=x.name,agg_func='min'))\n",
    "\n",
    "        com_legs_df[[f\"{legs}_intraday_high\" for legs in legs_to_process]] = com_legs_df[[f\"{legs}_askPrice\" for legs in legs_to_process]].apply(lambda x: intraday_high(com_legs_df, high_col=x.name,agg_func='max'))\n",
    "\n",
    "        close_cols = [f'{leg}_close' for leg in legs_to_process]\n",
    "        com_legs_df[[f'{leg}_ma10' for leg in legs_to_process]] =  com_legs_df[close_cols].rolling(window=5).mean()\n",
    "\n",
    "        # Calculate Today's Opening Price for each leg\n",
    "        com_legs_df[[f'{leg}_todayo' for leg in legs_to_process]] =  com_legs_df[[f'{leg}_close' for leg in legs_to_process]].resample('D').transform('first')\n",
    "\n",
    "        # Calculate Previous Day's Low (using bidPrice) for each leg\n",
    "        com_legs_df[[f\"{leg}_prevdayl\" for leg in legs_to_process]] = com_legs_df[[f\"{leg}_bidPrice\" for leg in legs_to_process]].apply(lambda x: get_x_day_low_numba(com_legs_df, n=1, column=x.name))\n",
    "\n",
    "        # Calculate Previous Day's High (using askPrice) for each leg\n",
    "        com_legs_df[[f\"{leg}_prevdayh\" for leg in legs_to_process]] = \\\n",
    "            com_legs_df[[f\"{leg}_askPrice\" for leg in legs_to_process]].apply(lambda x: get_x_day_high_numba(com_legs_df, n=1, column=x.name) )\n",
    "\n",
    "        \n",
    "        window_size = 15\n",
    "\n",
    "        # # 1. Rolling 15-Bar Highest Close\n",
    "        # com_legs_df[[f'{leg}_15min_high' for leg in legs_to_process]] = \\\n",
    "        #     com_legs_df[close_cols].rolling(window=window_size).max()\n",
    "\n",
    "        # # 2. Rolling 15-Bar Lowest Close\n",
    "        # com_legs_df[[f'{leg}_15min_low' for leg in legs_to_process]] = \\\n",
    "        #     com_legs_df[close_cols].rolling(window=window_size).min()\n",
    "\n",
    "        # # 3. Rolling 15-Bar Mean Close\n",
    "        # com_legs_df[[f'{leg}_15min_mean' for leg in legs_to_process]] = \\\n",
    "        #     com_legs_df[close_cols].rolling(window=window_size).mean()\n",
    "\n",
    "        com_legs_df[[f'{leg}_ROC_5' for leg in legs_to_process]] = com_legs_df[[f\"{leg}_close\" for leg in legs_to_process]].pct_change(periods=5)\n",
    "\n",
    "        \n",
    "        # 7. Join the wide option data with the underlying SPY data\n",
    "        #com_df3 = pd.concat([daily_underlying_df, com_legs_df], join='outer', axis=1)\n",
    "        \n",
    "        com_df3 = com_legs_df.join(daily_underlying_df)\n",
    "               \n",
    "        # 1. Calculate the rounded ATM strike for every row\n",
    "        atm_strikes = round_to_nearest(com_df3['SPY_close'], interval=1)\n",
    "        atm1_strikes = round_to_nearest(com_df3['SPY_close'], interval=1) + 1\n",
    "\n",
    "        # 2. Build the corresponding leg name for every row\n",
    "        # This creates a pandas Series where each value is a string like 'C_468.0_20240105'\n",
    "        com_df3['current_atm_leg'] = atm_strikes\n",
    "\n",
    "\n",
    "            # 2. Initialize state dictionaries, now including position_side\n",
    "        is_in_position = {}\n",
    "        entry_price = {}\n",
    "        entry_timestamp = {} # <-- ADD THIS LINE\n",
    "        unrealized_pnl = {}\n",
    "        position_side = {} # <-- ADD THIS\n",
    "        bar_count = {}\n",
    "        qty = {}\n",
    "        entry_strike = {}\n",
    "                \n",
    "            # The main loop now iterates using an integer index 'i'\n",
    "        for i in range(len(com_df3)):\n",
    "            \n",
    "            # Get the timestamp and row data using the integer index 'i'\n",
    "            timestamp = com_df3.index[i]\n",
    "            row = com_df3.iloc[i]\n",
    "            \n",
    "            # --- Part A: Update PnL & Check Exits for OPEN positions ---\n",
    "            for open_leg_name in list(is_in_position.keys()):\n",
    "                #print(f\"Found position in {open_leg_name}\")\n",
    "\n",
    "                side = position_side[open_leg_name]\n",
    "                current_price = row[f'{open_leg_name}_close']\n",
    "                \n",
    "                pnl = (current_price - entry_price[open_leg_name]) * qty[open_leg_name] * side\n",
    "                unrealized_pnl[open_leg_name] = pnl\n",
    "                bar_count[open_leg_name] += 1\n",
    "\n",
    "                # --- Part B: Check for NEW entries ---\n",
    "            for leg_name in legs_to_process:\n",
    "                #print(f\"Leg name is {leg_name}\")\n",
    "                if not is_in_position.get(leg_name):\n",
    "                # and row['SPY_close']>row['SPY_intraday_high']*0.997                                         \n",
    "                                        # Add the 'f' prefix to all strings accessing columns with leg_name\n",
    "                                        # and \n",
    "                        #row[f'{leg_name}_close'] < min((com_df3[f\"{leg_name}_intraday_high\"].iloc[i - 1] * 0.9), com_df3[f\"{leg_name}_close\"].iloc[i - 1] * 1.1)\n",
    "                    entry_condition_met = (\n",
    "                        time(9, 45) <= timestamp.time() <= time(13, 15) and  row['SPY_close']<row['SPY_intraday_high']*0.998 and\n",
    "                        row[f\"{leg_name}_close\"] < 0.8 and row[f\"{leg_name}_optionType\"] ==-1 and  row[f'{leg_name}_close'] >(com_df3[f\"{leg_name}_intraday_low\"].iloc[i - 1] * 1.25) and \n",
    "                        row[f\"{leg_name}_strike\"] <= row['SPY_close']-2   and  row[f\"{leg_name}_strike\"] >= row['SPY_close']-3 and\n",
    "                        row[\"SPY_3d_Pctl\"] < 0.7 and row[f'{leg_name}_close'] < min((com_df3[f\"{leg_name}_intraday_high\"].iloc[i - 1] * 0.9), com_df3[f\"{leg_name}_close\"].iloc[i - 1] * 1.25) and \n",
    "                        row[f\"{leg_name}_ROC_5\"] > 0.2 and row[f'{leg_name}_close']>row[f'{leg_name}_ma10']+0.05  )\n",
    "\n",
    "                    short_condition_met = (\n",
    "                        time(15, 45) <= timestamp.time() <= time(13, 45) and row['SPY_intraday_low']>row['SPY_todayo']*0.999 and\n",
    "                        row[f\"{leg_name}_close\"] > 0.15 and row[f\"{leg_name}_optionType\"] ==-1 and\n",
    "                        row[f\"{leg_name}_intraday_high\"] <1 and row[f\"{leg_name}_strike\"] >= row['SPY_close']-4 and row[f\"{leg_name}_strike\"] < row['SPY_close']-3 and \n",
    "                        row[f'{leg_name}_close'] < min((com_df3[f\"{leg_name}_intraday_high\"].iloc[i - 1] * 0.75), com_df3[f\"{leg_name}_close\"].iloc[i - 1] * 0.998) and\n",
    "                        row[\"SPY_3d_Pctl\"] > 0.7 and # Also better to access from 'row' for current timestamp data\n",
    "                        row[f\"{leg_name}_ROC_5\"] < -0.01  )\n",
    "\n",
    "\n",
    "\n",
    "                    # --- Check for a BUY Entry ---\n",
    "                    if entry_condition_met:\n",
    "                        \n",
    "                        is_in_position[leg_name] = True\n",
    "                        position_side[leg_name] = 1 # <-- Set side to 1 for long\n",
    "                        entry_price[leg_name] = row[f'{leg_name}_askPrice']\n",
    "                        qty[leg_name] = 1000\n",
    "                        bar_count[leg_name] = 0\n",
    "                        entry_timestamp[leg_name] = timestamp\n",
    "                        entry_strike[leg_name] = row[f'{leg_name}_strike']\n",
    "                        print(f\"  -> BUY ENTRY for {leg_name} at {timestamp.time()} and price {entry_price[leg_name]}\")\n",
    "\n",
    "                    if short_condition_met:\n",
    "                        \n",
    "                        is_in_position[leg_name] = True\n",
    "                        position_side[leg_name] = -1 # <-- Set side to 1 for long\n",
    "                        entry_price[leg_name] = row[f'{leg_name}_askPrice']\n",
    "                        qty[leg_name] = 750\n",
    "                        bar_count[leg_name] = 0\n",
    "                        entry_timestamp[leg_name] = timestamp\n",
    "                        entry_strike[leg_name] = row[f'{leg_name}_strike']\n",
    "                        print(f\"  -> SELL ENTRY for {leg_name} at {timestamp.time()} and price {entry_price[leg_name]}\")\n",
    "\n",
    "\n",
    "                if is_in_position.get(leg_name):\n",
    "\n",
    "\n",
    "                    sell_condition_met = timestamp.time() >= time(14, 45) and row[f'{leg_name}_bidPrice']<com_df3[f'{leg_name}_bidPrice'].iloc[i-1]-0.01\n",
    "                    #sell_condition_met3 = timestamp.time() >= time(15, 45) \n",
    "\n",
    "                    sell_condition_2 = row[f'{leg_name}_bidPrice']>entry_price[leg_name]+0.45 and row[f'{leg_name}_bidPrice']>com_df3[f'{leg_name}_bidPrice'].iloc[i-1]*1.01\n",
    "\n",
    "                    sell_condition_3 = row[f'{leg_name}_bidPrice']<entry_price[leg_name]-0.25 and row[f'{leg_name}_bidPrice']<com_df3[f'{leg_name}_bidPrice'].iloc[i-1]*0.99\n",
    "\n",
    "                    sell_condition_4 = row[f'{leg_name}_close']<com_df3[f'{leg_name}_intraday_low'].iloc[i-1]*1.05 \n",
    "\n",
    "                    if sell_condition_met :\n",
    "                        print(f\"EOD CLosing of {position_side[leg_name]} side positon in {leg_name} at {timestamp}\")    \n",
    "                            # 1. Get the Exit Price (use bid price since you are selling to close)\n",
    "                        exit_price = row[f'{leg_name}_bidPrice']\n",
    "                        side = position_side[leg_name]\n",
    "                            # ... etc.\n",
    "                            # 2. Calculate the final profit\n",
    "                        profit = (exit_price - entry_price[leg_name]) * qty[leg_name] * side * option_multiplier\n",
    "\n",
    "                        # 3. Create a dictionary for the trade log\n",
    "                        trade_details = {\n",
    "                            'name': leg_name,\n",
    "                            'Side': 'BUY' if side == 1 else 'SELL',\n",
    "                            'entry_date': entry_timestamp[leg_name], # We will add this in the next step\n",
    "                            'entry_price': entry_price[leg_name],\n",
    "                            'qty': qty[leg_name],\n",
    "                            'exit_date': timestamp,\n",
    "                            'exit_price': exit_price,\n",
    "                            'profit': profit,\n",
    "                            'bars_held': bar_count[leg_name]\n",
    "                        }\n",
    "                        \n",
    "                        # 4. Append the log to our final list\n",
    "                        final_trade_log_list.append(trade_details)\n",
    "                        \n",
    "                        print(f\"  -> EOD EXIT for {leg_name} at {exit_price:.2f}. Profit: ${profit:.2f}\")\n",
    "\n",
    "                        # 5. Reset all state dictionaries for this leg\n",
    "                        del is_in_position[leg_name]\n",
    "                        del position_side[leg_name]\n",
    "                        del entry_price[leg_name]\n",
    "                        del qty[leg_name]\n",
    "                        del bar_count[leg_name]\n",
    "                        del unrealized_pnl[leg_name]\n",
    "                        del entry_timestamp[leg_name] # Also remove the entry timestamp\n",
    "                    \n",
    "                    if sell_condition_3 :\n",
    "                        \n",
    "                            \n",
    "                        exit_price = row[f'{leg_name}_bidPrice']\n",
    "                        side = position_side[leg_name]\n",
    "                            # ... etc.\n",
    "                            # 2. Calculate the final profit\n",
    "                        profit = (exit_price - entry_price[leg_name]) * qty[leg_name] * side * option_multiplier\n",
    "                        print(f\"Stoploss of  {position_side[leg_name]} side entry was {entry_price[leg_name]} in {leg_name} at {exit_price}\")    \n",
    "                        # 3. Create a dictionary for the trade log\n",
    "                        trade_details = {\n",
    "                            'name': leg_name,\n",
    "                            'Side': 'BUY' if side == 1 else 'SELL',\n",
    "                            'entry_date': entry_timestamp[leg_name], # We will add this in the next step\n",
    "                            'entry_price': entry_price[leg_name],\n",
    "                            'qty': qty[leg_name],\n",
    "                            'exit_date': timestamp,\n",
    "                            'exit_price': exit_price,\n",
    "                            'profit': profit,\n",
    "                            'bars_held': bar_count[leg_name]\n",
    "                        }\n",
    "                        \n",
    "                        # 4. Append the log to our final list\n",
    "                        final_trade_log_list.append(trade_details)\n",
    "                        \n",
    "                        print(f\"  -> EOD EXIT for {leg_name} at {exit_price:.2f}. Profit: ${profit:.2f}\")\n",
    "\n",
    "                        # 5. Reset all state dictionaries for this leg\n",
    "                        del is_in_position[leg_name]\n",
    "                        del position_side[leg_name]\n",
    "                        del entry_price[leg_name]\n",
    "                        del qty[leg_name]\n",
    "                        del bar_count[leg_name]\n",
    "                        del unrealized_pnl[leg_name]\n",
    "                        del entry_timestamp[leg_name] # Also remove the entry timestamp\n",
    "                    \n",
    "\n",
    "\n",
    "                    if sell_condition_2:\n",
    "                       \n",
    "                       \n",
    "                        exit_price = row[f'{leg_name}_bidPrice']\n",
    "                        side = position_side[leg_name]\n",
    "                            # ... etc.\n",
    "                            # 2. Calculate the final profit\n",
    "                        profit = (exit_price - entry_price[leg_name]) * qty[leg_name] * side * option_multiplier\n",
    "\n",
    "                        # 3. Create a dictionary for the trade log\n",
    "                        trade_details = {\n",
    "                            'name': leg_name,\n",
    "                            'Side': 'BUY' if side == 1 else 'SELL',\n",
    "                            'entry_date': entry_timestamp[leg_name], # We will add this in the next step\n",
    "                            'entry_price': entry_price[leg_name],\n",
    "                            'qty': qty[leg_name],\n",
    "                            'exit_date': timestamp,\n",
    "                            'exit_price': exit_price,\n",
    "                            'profit': profit,\n",
    "                            'bars_held': bar_count[leg_name]\n",
    "                        }\n",
    "                        \n",
    "                        # 4. Append the log to our final list\n",
    "                        final_trade_log_list.append(trade_details)\n",
    "                        \n",
    "                        print(f\"  -> Target EXIT for {leg_name} at {exit_price:.2f}. Profit: ${profit:.2f}\")\n",
    "\n",
    "                        # 5. Reset all state dictionaries for this leg\n",
    "                        del is_in_position[leg_name]\n",
    "                        del position_side[leg_name]\n",
    "                        del entry_price[leg_name]\n",
    "                        del qty[leg_name]\n",
    "                        del bar_count[leg_name]\n",
    "                        del unrealized_pnl[leg_name]\n",
    "                        del entry_timestamp[leg_name] # Also remove the entry timestamp\n",
    "\n",
    "                    if sell_condition_4:\n",
    "                       \n",
    "                       \n",
    "                        exit_price = row[f'{leg_name}_bidPrice']\n",
    "                        side = position_side[leg_name]\n",
    "                            # ... etc.\n",
    "                            # 2. Calculate the final profit\n",
    "                        profit = (exit_price - entry_price[leg_name]) * qty[leg_name] * side * option_multiplier\n",
    "\n",
    "                        # 3. Create a dictionary for the trade log\n",
    "                        trade_details = {\n",
    "                            'name': leg_name,\n",
    "                            'Side': 'BUY' if side == 1 else 'SELL',\n",
    "                            'entry_date': entry_timestamp[leg_name], # We will add this in the next step\n",
    "                            'entry_price': entry_price[leg_name],\n",
    "                            'qty': qty[leg_name],\n",
    "                            'exit_date': timestamp,\n",
    "                            'exit_price': exit_price,\n",
    "                            'profit': profit,\n",
    "                            'bars_held': bar_count[leg_name]\n",
    "                        }\n",
    "                        \n",
    "                        # 4. Append the log to our final list\n",
    "                        final_trade_log_list.append(trade_details)\n",
    "                        \n",
    "                        print(f\"  -> MA cross down exit of {leg_name} at {exit_price:.2f}. Profit: ${profit:.2f}\")\n",
    "\n",
    "                        # 5. Reset all state dictionaries for this leg\n",
    "                        del is_in_position[leg_name]\n",
    "                        del position_side[leg_name]\n",
    "                        del entry_price[leg_name]\n",
    "                        del qty[leg_name]\n",
    "                        del bar_count[leg_name]\n",
    "                        del unrealized_pnl[leg_name]\n",
    "                        del entry_timestamp[leg_name] # Also remove the entry timestamp\n",
    "\n",
    "\n",
    "\n",
    "                   \n",
    "\n",
    "\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ An error occurred on {day_str}: {e}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f321ddc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SPY_close</th>\n",
       "      <th>P_463.0_20240108_close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ts</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-01-08 09:30:00</th>\n",
       "      <td>468.43000000</td>\n",
       "      <td>0.00000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-08 09:31:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.03500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-08 09:32:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.02500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-08 09:33:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.02500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-08 09:34:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.02500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-08 15:56:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-08 15:57:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-08 15:58:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-08 15:59:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-08 16:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>391 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       SPY_close  P_463.0_20240108_close\n",
       "ts                                                      \n",
       "2024-01-08 09:30:00 468.43000000              0.00000000\n",
       "2024-01-08 09:31:00          NaN              0.03500000\n",
       "2024-01-08 09:32:00          NaN              0.02500000\n",
       "2024-01-08 09:33:00          NaN              0.02500000\n",
       "2024-01-08 09:34:00          NaN              0.02500000\n",
       "...                          ...                     ...\n",
       "2024-01-08 15:56:00          NaN              0.00500000\n",
       "2024-01-08 15:57:00          NaN              0.00500000\n",
       "2024-01-08 15:58:00          NaN              0.00500000\n",
       "2024-01-08 15:59:00          NaN              0.00500000\n",
       "2024-01-08 16:00:00          NaN              0.00500000\n",
       "\n",
       "[391 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com_df3[['SPY_close' , 'P_463.0_20240108_close' , 'P_463.0_20240108_' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8787d503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "def calculate_delta_vectorized(S, K, T_years, r, IV, option_type_series):\n",
    "    \"\"\"\n",
    "    Calculates Delta for all rows of data at once using NumPy.\n",
    "    \n",
    "    Args:\n",
    "        S (pd.Series): Underlying prices\n",
    "        K (pd.Series): Strike prices\n",
    "        T_years (pd.Series): Time to expiry in years\n",
    "        r (float): Risk-free interest rate\n",
    "        IV (pd.Series): Implied Volatility\n",
    "        option_type_series (pd.Series): Column with 1 for calls, -1 for puts\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: An array of delta values.\n",
    "    \"\"\"\n",
    "    # Add a small epsilon to prevent division by zero in edge cases\n",
    "    IV = IV + 1e-9 \n",
    "    T_years = T_years + 1e-9\n",
    "    \n",
    "    d1 = (np.log(S / K) + (r + 0.5 * IV**2) * T_years) / (IV * np.sqrt(T_years))\n",
    "    \n",
    "    # Calculate delta using the CDF and np.where for conditional logic\n",
    "    call_delta = norm.cdf(d1)\n",
    "    put_delta = call_delta - 1\n",
    "    \n",
    "    # Use np.where to apply the correct formula based on option type\n",
    "    delta = np.where(option_type_series == 1, call_delta, put_delta)\n",
    "    \n",
    "    return delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b95eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for leg_name in legs_to_process:\n",
    "    # Check if all necessary columns exist for this leg\n",
    "    required_cols = [\n",
    "        f'{leg_name}_strike', f'{leg_name}_dte', \n",
    "        f'{leg_name}_iv', f'{leg_name}_optionType', 'SPY_close'\n",
    "    ]\n",
    "\n",
    "    print(required_cols)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d54ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "com_legs_df[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f568288",
   "metadata": {},
   "outputs": [],
   "source": [
    "com_legs_df.filter(regex='P_463').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f168763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "com_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7bf53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (This line should already be in your code)\n",
    "# com_df3 = pd.concat([daily_underlying_df, com_legs_df], join='outer', axis=1)\n",
    "\n",
    "print(\"\\n--- Calculating Delta for all option legs ---\")\n",
    "\n",
    "# Define the risk-free rate\n",
    "risk_free_rate = 0.03\n",
    "\n",
    "# Loop through each unique option leg you've processed\n",
    "for leg_name in legs_to_process:\n",
    "    # Check if all necessary columns exist for this leg in the CORRECT DataFrame\n",
    "    required_cols = [\n",
    "        f'{leg_name}_strike', f'{leg_name}_dte',\n",
    "        f'{leg_name}_iv', f'{leg_name}_optionType', 'SPY_close'\n",
    "    ]\n",
    "    # ▼▼▼ FIX: Check in com_df3 ▼▼▼\n",
    "    if not all(col in com_df3.columns for col in required_cols):\n",
    "        print(f\"  -> Skipping {leg_name}, missing required data in com_df3.\")\n",
    "        continue\n",
    "\n",
    "    # Create the new delta column for the current leg in the CORRECT DataFrame\n",
    "    # ▼▼▼ FIX: Use com_df3 for all inputs and to store the result ▼▼▼\n",
    "    com_df3[f'{leg_name}_delta'] = calculate_delta_vectorized(\n",
    "        S=com_df3['SPY_close'],\n",
    "        K=com_df3[f'{leg_name}_strike'],\n",
    "        T_years=com_df3[f'{leg_name}_dte'] / 365.0,\n",
    "        r=risk_free_rate,\n",
    "        IV=com_df3[f'{leg_name}_iv'],\n",
    "        option_type_series=com_df3[f'{leg_name}_optionType']\n",
    "    )\n",
    "\n",
    "print(\"✅ Delta calculations complete.\")\n",
    "\n",
    "# Display the first few rows of some new delta columns to verify\n",
    "delta_cols_to_show = [col for col in com_df3.columns if '_delta' in col]\n",
    "print(\"\\nSample of calculated Delta columns:\")\n",
    "print(com_df3[delta_cols_to_show].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a25b01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_cols_to_show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef81528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_trade_log2 = pd.DataFrame(final_trade_log_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0c4e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_trade_log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e1b78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_profits = local_trade_log2.groupby(local_trade_log2['entry_date'].dt.date)['profit'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5b9f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "com_legs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ca51e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_profits.cumsum().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e75396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ---  HELPER FUNCTION for Clean and Fast Trade Logging ---\n",
    "# ==============================================================================\n",
    "def log_and_close_trade(trade_log_list, leg, exit_timestamp, exit_price, exit_reason, state_vars):\n",
    "    \"\"\"\n",
    "    Calculates P&L, appends the trade to a log list, and resets the state for that leg.\n",
    "    \"\"\"\n",
    "    # Unpack state variables for easier access\n",
    "    entry_dates, entry_prices, qty, position_type, instrument_positions, bar_count = (\n",
    "        state_vars['entry_dates'], state_vars['entry_prices'], state_vars['qty'],\n",
    "        state_vars['position_type'], state_vars['instrument_positions'], state_vars['bar_count']\n",
    "    )\n",
    "    \n",
    "    if entry_prices[leg] is None: return # Avoid logging if no entry price exists\n",
    "\n",
    "    if position_type[leg] == 1:  # Long position\n",
    "        profit = (exit_price - entry_prices[leg]) * qty[leg]\n",
    "        side = 'BUY'\n",
    "    else: # Short position\n",
    "        profit = (entry_prices[leg] - exit_price) * qty[leg]\n",
    "        side = 'SELL'\n",
    "\n",
    "    trade_log_list.append({\n",
    "        'name': leg, 'Side': side, 'entry_date': entry_dates[leg],\n",
    "        'entry_price': entry_prices[leg], 'qty': qty[leg],\n",
    "        'exit_date': exit_timestamp, 'exit_price': exit_price,\n",
    "        'profit': profit, 'exit_reason': exit_reason\n",
    "    })\n",
    "\n",
    "    # Reset state variables\n",
    "    instrument_positions[leg] = False\n",
    "    entry_prices[leg] = None\n",
    "    bar_count[leg] = 0\n",
    "    \n",
    "    print(f\"   -> EXIT {leg} ({side}) at {exit_price:.2f} for {exit_reason}. P&L: ${profit:.2f}\")\n",
    "\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a89e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'entry_date' in local_trade_log2.columns:\n",
    "\n",
    "    local_trade_log2.set_index('entry_date' , inplace=True)\n",
    "\n",
    "    local_trade_log2.sort_index( ascending=True, inplace=True)\n",
    "\n",
    "if 'exit_date' in local_trade_log2.columns:\n",
    "    local_trade_log2['exit_date'] = pd.to_datetime(local_trade_log2['exit_date'])\n",
    "\n",
    "# Calculate the absolute percentage difference between entry and exit prices\n",
    "local_trade_log2['price_change_pct'] = (local_trade_log2['exit_price'] - local_trade_log2['entry_price']).abs() / local_trade_log2['entry_price']\n",
    "\n",
    "# Count trades before filtering\n",
    "trades_before_filtering = len(local_trade_log2)\n",
    "\n",
    "average_trade_profit = local_trade_log2['profit'].mean()\n",
    "\n",
    "# First ensure consistent case for Side column\n",
    "\n",
    "if 'side' in local_trade_log2.columns:\n",
    "    local_trade_log2.rename(columns={'side': 'Side'}, inplace=True)\n",
    "\n",
    "\n",
    "local_trade_log2['Side'] = local_trade_log2['Side'].str.lower()\n",
    "\n",
    "# Calculate profit in bps (handles both buy/long and sell/short)\n",
    "local_trade_log2['profit_bps'] = np.where(\n",
    "    local_trade_log2['Side'].isin(['buy', 'long']),\n",
    "    ((local_trade_log2['exit_price'] - local_trade_log2['entry_price']) / local_trade_log2['entry_price']) * 10000,\n",
    "    ((local_trade_log2['entry_price'] - local_trade_log2['exit_price']) / local_trade_log2['entry_price']) * 10000)\n",
    "\n",
    "# open_positions = list(long_position_count.values())\n",
    "# print(f\"Max open long positions are: {np.max(open_positions)}\")\n",
    "\n",
    "# open__short_positions = list(short_position_count.values())\n",
    "# print(f\"Max open shrt positions are: {np.max(open__short_positions)}\")\n",
    "\n",
    "total_trade_count = len(local_trade_log2)\n",
    "max_entries_per_day = local_trade_log2.groupby(local_trade_log2.index.date).size().max()\n",
    "\n",
    "# Identify losing trades and percentage\n",
    "\n",
    "losers = local_trade_log2[local_trade_log2['profit'] < 0]\n",
    "num_losing_trades = len(losers)\n",
    "losers_percentage = (num_losing_trades / total_trade_count) * 100 if total_trade_count > 0 else 0\n",
    "\n",
    "winners = local_trade_log2[local_trade_log2['profit'] > 0]\n",
    "\n",
    "# # 1. Create a pandas Series from your PnL data\n",
    "# pnl_series = pd.Series(total_pnl_series)\n",
    "\n",
    "# # 2. Find the minimum value in the series\n",
    "# max_unrealized_loss = pnl_series.min()\n",
    "\n",
    "# print(f\"The highest unrealized loss (minimum PnL) was: ${max_unrealized_loss:,.2f}\")\n",
    "\n",
    "# Updated side checks throughout\n",
    "num_long_trades = len(local_trade_log2[local_trade_log2['Side'].isin(['buy', 'long'])])\n",
    "num_short_trades = len(local_trade_log2[local_trade_log2['Side'].isin(['sell', 'short'])])\n",
    "print(f\"Number of long trades: {num_long_trades} and short trades: {num_short_trades} out of total trades: {total_trade_count}\")\n",
    "\n",
    "# Updated profit calculations\n",
    "avg_long_profit_bps = local_trade_log2[local_trade_log2['Side'].isin(['buy', 'long'])]['profit_bps'].mean()\n",
    "avg_short_profit_bps = local_trade_log2[local_trade_log2['Side'].isin(['sell', 'short'])]['profit_bps'].mean()\n",
    "print(f\"Average long trade profit: {avg_long_profit_bps:.2f} bps and short: {avg_short_profit_bps:.2f} bps\")\n",
    "\n",
    "profit_total_long = local_trade_log2[local_trade_log2['Side'].isin(['buy', 'long'])]['profit'].sum()\n",
    "profit_total_short = local_trade_log2[local_trade_log2['Side'].isin(['sell', 'short'])]['profit'].sum()\n",
    "print(f\"Net long profit: {profit_total_long:.2f} and short profit: {profit_total_short:.2f}\")\n",
    "\n",
    "# Compute holding period\n",
    "local_trade_log2['holding_days'] = (local_trade_log2['exit_date'] - local_trade_log2.index).dt.days\n",
    "\n",
    "# Compute equity curve (EC)\n",
    "local_trade_log2['EC'] = (position_cap*100) + local_trade_log2['profit'].cumsum()\n",
    "\n",
    "#============================================\n",
    "\n",
    "# The index should be the 'exit_date' because that's when the equity changes\n",
    "equity_at_exit = pd.Series(data=local_trade_log2['EC'].values, index=pd.to_datetime(local_trade_log2['exit_date']))\n",
    "\n",
    "print(f\"Equity at exit is : {equity_at_exit}\")\n",
    "\n",
    "# Print average trade profit\n",
    "print(f\"Average Trade Profit: {average_trade_profit:.2f}\")\n",
    "\n",
    "# Print profit in basis points (bps)\n",
    "print(\"Profit (bps) for each trade:\")\n",
    "print(f\"Avergae trade profit in bps : {local_trade_log2['profit_bps'].mean()}\" )\n",
    "\n",
    "# Print total trade count\n",
    "print(f\"Total Trade Count: {total_trade_count}\")\n",
    "\n",
    "# Print max entries per day\n",
    "print(f\"Max Entries Per Day: {max_entries_per_day}\")\n",
    "\n",
    "# Print number of losing trades\n",
    "print(f\"Number of Losing Trades: {num_losing_trades}\")\n",
    "\n",
    "print(f\"Losing Trades Percentage: {losers_percentage:.2f}%\")\n",
    "\n",
    "# Print holding period\n",
    "print(\"Avg Holding Period in (Days) :\")\n",
    "print(local_trade_log2['holding_days'].mean())\n",
    "\n",
    "# Add: Number of trades with loss greater than 10% (i.e. -1000 bps)\n",
    "loss_threshold_bps = -1000  # -10%\n",
    "deep_losers = local_trade_log2[local_trade_log2['profit_bps'] < loss_threshold_bps]\n",
    "num_deep_losers = len(deep_losers)\n",
    "\n",
    "print(f\"Number of Losing Trades > 10% Loss: {num_deep_losers}\")\n",
    "\n",
    "# Add: Average loss of all losing trades\n",
    "avg_losing_trade = losers['profit'].mean()\n",
    "print(f\"Average Loss per Losing Trade: {avg_losing_trade:.4f}\")\n",
    "\n",
    "# Calculate average profit percentage (in % terms)\n",
    "average_profit_percent = local_trade_log2['profit'].mean() / position_cap * 100\n",
    "print(f\"\\nAverage Profit Percentage per Trade: {average_profit_percent:.2f}%\")\n",
    "\n",
    "# Calculate Trade Expectancy (in monetary terms)\n",
    "win_rate = len(winners) / total_trade_count\n",
    "avg_win = winners['profit'].mean()\n",
    "avg_loss = losers['profit'].mean()\n",
    "trade_expectancy = (win_rate * avg_win) - ((1 - win_rate) * abs(avg_loss))\n",
    "print(f\"Trade Expectancy: ${trade_expectancy:.2f} per trade\")\n",
    "\n",
    "# Calculate Risk-Adjusted Metrics\n",
    "risk_reward_ratio = abs(avg_win / avg_loss) if avg_loss != 0 else float('inf')\n",
    "print(f\"Risk-Reward Ratio: {risk_reward_ratio:.2f}:1\")\n",
    "\n",
    "\n",
    "# Add: Average loss of all losing trades\n",
    "avg_winning_trade = winners['profit'].mean()\n",
    "print(f\"Average Loss per Losing Trade: {avg_winning_trade:.4f}\")\n",
    "\n",
    "\n",
    "abs_profit_df =  pd.DataFrame()\n",
    "for symbol , data in absolute_symbol_pnl_series.items():\n",
    "\n",
    "    tdf = pd.DataFrame(data , columns=['timestamp' , symbol])\n",
    "    tdf.set_index('timestamp' , inplace=True)\n",
    "\n",
    "    # Reindex to match df_index, filling missing values with 0\n",
    "    abs_profit_df[symbol] = tdf.reindex(com_df2.index, fill_value=0)\n",
    "\n",
    "max_unrealised_drawdown = abs_profit_df.sum(axis=1).min()\n",
    "print(f\"Max unrealised loss ( which is unrealised drawdown on notional) is {max_unrealised_drawdown}\")\n",
    "\n",
    "\n",
    "# Long/Short specific metrics\n",
    "# Update all calculations to use the new condition\n",
    "long_trades = local_trade_log2[local_trade_log2['Side'].isin(['buy', 'long'])]\n",
    "short_trades = local_trade_log2[local_trade_log2['Side'].isin(['sell', 'short'])]\n",
    "\n",
    "num_long_trades = len(long_trades)\n",
    "num_short_trades = len(short_trades)\n",
    "\n",
    "# Profit metrics\n",
    "avg_long_profit_bps = long_trades['profit_bps'].mean()\n",
    "avg_short_profit_bps = short_trades['profit_bps'].mean()\n",
    "profit_total_long = long_trades['profit'].sum()\n",
    "profit_total_short = short_trades['profit'].sum()\n",
    "\n",
    "# Win/loss metrics\n",
    "def calculate_consecutive_wins_losses(trades):\n",
    "    trades = trades.sort_index()\n",
    "    consecutive = []\n",
    "    current_streak = 0\n",
    "    prev_result = None\n",
    "\n",
    "    for profit in trades['profit']:\n",
    "        current_result = 'win' if profit > 0 else 'loss'\n",
    "        if current_result == prev_result or prev_result is None:\n",
    "            current_streak += 1\n",
    "        else:\n",
    "            consecutive.append((prev_result, current_streak))\n",
    "            current_streak = 1\n",
    "        prev_result = current_result\n",
    "\n",
    "    if current_streak > 0:\n",
    "        consecutive.append((prev_result, current_streak))\n",
    "\n",
    "    return pd.DataFrame(consecutive, columns=['type', 'length'])\n",
    "\n",
    "# Calculate for long and short separately\n",
    "long_consecutive = calculate_consecutive_wins_losses(long_trades)\n",
    "short_consecutive = calculate_consecutive_wins_losses(short_trades)\n",
    "\n",
    "def print_streak_stats(name, df):\n",
    "    if len(df) > 0:\n",
    "        max_win_streak = df[df['type'] == 'win']['length'].max()\n",
    "        max_loss_streak = df[df['type'] == 'loss']['length'].max()\n",
    "        avg_win_streak = df[df['type'] == 'win']['length'].mean()\n",
    "        avg_loss_streak = df[df['type'] == 'loss']['length'].mean()\n",
    "        print(f\"\\n{name} Streaks:\")\n",
    "        print(f\"Max Winning Streak: {max_win_streak}\")\n",
    "        print(f\"Max Losing Streak: {max_loss_streak}\")\n",
    "        print(f\"Avg Winning Streak: {avg_win_streak:.1f}\")\n",
    "        print(f\"Avg Losing Streak: {avg_loss_streak:.1f}\")\n",
    "\n",
    "print_streak_stats(\"Long\", long_consecutive)\n",
    "print_streak_stats(\"Short\", short_consecutive)\n",
    "\n",
    "# Winning ratios\n",
    "long_winners = len(long_trades[long_trades['profit'] > 0])\n",
    "short_winners = len(short_trades[short_trades['profit'] > 0])\n",
    "\n",
    "long_win_ratio = long_winners / num_long_trades if num_long_trades > 0 else 0\n",
    "short_win_ratio = short_winners / num_short_trades if num_short_trades > 0 else 0\n",
    "\n",
    "print(f\"\\nLong Trades Win Ratio: {long_win_ratio:.2%}\")\n",
    "print(f\"\\nShort Trades Win Ratio: {short_win_ratio:.2%}\")\n",
    "\n",
    "# Print all metrics\n",
    "print(\"\\n==========================================\")\n",
    "print(f\"Number of long trades: {num_long_trades}\")\n",
    "print(f\"Number of short trades: {num_short_trades}\")\n",
    "print(f\"Total trades: {total_trade_count}\")\n",
    "\n",
    "print(f\"\\nAverage long trade profit: {avg_long_profit_bps:.2f} bps\")\n",
    "print(f\"Average short trade profit: {avg_short_profit_bps:.2f} bps\")\n",
    "print(f\"Net long profit: {profit_total_long:.2f}\")\n",
    "print(f\"Net short profit: {profit_total_short:.2f}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scatter plot of trades\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# X-axis: trade index (or entry date if you want timeline)\n",
    "x = range(len(local_trade_log2))\n",
    "\n",
    "# Y-axis: profit\n",
    "y = local_trade_log2['profit']\n",
    "\n",
    "# Scatter: green for winners, red for losers\n",
    "colors = ['green' if p > 0 else 'red' for p in y]\n",
    "\n",
    "plt.scatter(x, y, c=colors, alpha=0.7, edgecolors='k')\n",
    "\n",
    "# Add horizontal line at 0 (break-even)\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "\n",
    "plt.title(\"Scatter of Trade Profits\")\n",
    "plt.xlabel(\"Trade Index\")\n",
    "plt.ylabel(\"Profit ($)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "# Create equity curve plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "local_trade_log2['EC'].plot(title='Equity Curve', grid=True)\n",
    "plt.ylabel('Portfolio Value')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot to temporary buffer\n",
    "buffer = BytesIO()\n",
    "plt.savefig(buffer, format='png')\n",
    "plt.close()\n",
    "plot_data = base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "\n",
    "\n",
    "#=== symbols specific\n",
    "\n",
    "# 2. Group by the symbol name and calculate the sum of profits for each.\n",
    "symbol_profits = local_trade_log2.groupby('name')['profit'].sum()\n",
    "\n",
    "# Display the total profit for each symbol\n",
    "print(\"--- Total Profit per Symbol ---\")\n",
    "print(symbol_profits)\n",
    "\n",
    "\n",
    "# 3. Sort the results to easily find the best and worst.\n",
    "sorted_symbols = symbol_profits.sort_values(ascending=False)\n",
    "\n",
    "\n",
    "# 4. Extract the best and worst symbols from the sorted list.\n",
    "# The best symbol is the first item (index 0).\n",
    "best_symbol = sorted_symbols.index[0]\n",
    "best_profit = sorted_symbols.iloc[0]\n",
    "\n",
    "# The worst symbol is the last item (index -1).\n",
    "worst_symbol = sorted_symbols.index[-1]\n",
    "worst_loss = sorted_symbols.iloc[-1]\n",
    "\n",
    "\n",
    "# 3. Sort the profits in ASCENDING order.\n",
    "#    This places the symbols with the largest losses at the top.\n",
    "sorted_by_loss = symbol_profits.sort_values(ascending=True)\n",
    "\n",
    "\n",
    "# 4. Select the first 10 rows from the sorted list using .head(10).\n",
    "worst_10_symbols = sorted_by_loss.head(10)\n",
    "\n",
    "\n",
    "# 5. Print the final result.\n",
    "print(\"--- 10 Worst Performing Symbols (by Total Profit) ---\")\n",
    "print(worst_10_symbols)\n",
    "\n",
    "#======================================\n",
    "\n",
    "\n",
    "\n",
    "# --- NEW: Calculate and print the additional statistics ---\n",
    "\n",
    "# Count the number of symbols with negative total profit (losers)\n",
    "num_loss_making_symbols = (symbol_profits <= 0).sum()\n",
    "\n",
    "# Count the number of symbols with positive total profit (winners)\n",
    "num_profit_making_symbols = (symbol_profits > 0).sum()\n",
    "\n",
    "# Calculate the ratio of losers to winners\n",
    "# Add a check to prevent division by zero if there are no winners\n",
    "if num_profit_making_symbols > 0:\n",
    "    loss_to_profit_ratio = num_loss_making_symbols / num_profit_making_symbols\n",
    "else:\n",
    "    loss_to_profit_ratio = float('inf') # Indicate infinite ratio if no winners\n",
    "\n",
    "# Print the new stats\n",
    "print(\"\\n--- Symbol Profit/Loss Summary ---\")\n",
    "print(f\"Number of Profit-Making Symbols: {num_profit_making_symbols}\")\n",
    "print(f\"Number of Loss-Making Symbols:   {num_loss_making_symbols}\")\n",
    "print(f\"Ratio of Loss-Makers to Profit-Makers: {loss_to_profit_ratio:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_max_daily_gmv(trade_log):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculates the maximum and mean Gross Merchandise Value (GMV) on a single day\n",
    "    from a trade log DataFrame. The mean calculation accounts for all days in the\n",
    "    period, including non-trading days.\n",
    "\n",
    "    Args:\n",
    "        trade_log (pd.DataFrame): DataFrame containing the trade log. \n",
    "                                  Index must be a datetime object ('entry_date').\n",
    "                                  Must contain 'entry_price', 'exit_price', \n",
    "                                  'qty', and 'exit_date' columns.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the max_gmv_value and the mean_gmv_value.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the total value of each entry and exit transaction\n",
    "    trade_log['entry_value'] = trade_log['entry_price'] * trade_log['qty']\n",
    "    trade_log['exit_value'] = trade_log['exit_price'] * trade_log['qty']\n",
    "\n",
    "    # Group all entry values by their calendar day and sum them up\n",
    "    daily_entries = trade_log.groupby(trade_log.index.date)['entry_value'].sum()\n",
    "\n",
    "    # Group all exit values by their calendar day and sum them up\n",
    "    # CORRECTED: Group exits by 'exit_date', not the index ('entry_date')\n",
    "    daily_exits = trade_log.groupby(trade_log['exit_date'].dt.date)['exit_value'].sum()\n",
    "\n",
    "    # Combine the daily entry and exit sums to get the total GMV for each day\n",
    "    # This series only contains days where a trade occurred.\n",
    "    daily_gmv = daily_entries.add(daily_exits, fill_value=0)\n",
    "\n",
    "    # Find the maximum value. This can be done before reindexing for efficiency.\n",
    "    if daily_gmv.empty:\n",
    "        return 0, 0\n",
    "    max_gmv_value = daily_gmv.max()\n",
    "\n",
    "    # --- MODIFICATION TO ACCOUNT FOR NON-TRADING DAYS ---\n",
    "\n",
    "    # 1. Determine the full date range of the backtest period.\n",
    "    #    We use the earliest entry and latest exit to define the period.\n",
    "    start_date = trade_log.index.min().date()\n",
    "    end_date = trade_log['exit_date'].max().date()\n",
    "    all_days_in_period = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "    # 2. Reindex the 'daily_gmv' Series to include all days.\n",
    "    #    This assigns a GMV of 0 to all non-trading days.\n",
    "    daily_gmv = daily_gmv.reindex(all_days_in_period, fill_value=0)\n",
    "    \n",
    "    # --- END OF MODIFICATION ---\n",
    "\n",
    "    # Now, this calculation correctly finds the mean over the entire period.\n",
    "    mean_gmv_value = daily_gmv.mean()\n",
    "\n",
    "    return max_gmv_value, mean_gmv_value\n",
    "\n",
    "\n",
    "max_gmv , mean_gmv = find_max_daily_gmv(local_trade_log2)\n",
    "\n",
    "print(f\"Average daily GMV is {mean_gmv}\")\n",
    "print(f\"Max daily gmv is {max_gmv}\")\n",
    "\n",
    "\n",
    "print(f\"Return on average GMV is {local_trade_log2['profit'].sum()/mean_gmv}\")\n",
    "print(f\"Return on average GMV is {local_trade_log2['profit'].sum()/max_gmv}\")\n",
    "\n",
    "\n",
    "\n",
    "# Create HTML report\n",
    "html_report = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Backtest Performance Report</title>\n",
    "    <style>\n",
    "        body {{ font-family: Arial, sans-serif; margin: 20px; }}\n",
    "        h1 {{ color: #2c3e50; border-bottom: 2px solid #3498db; }}\n",
    "        h2 {{ color: #2980b9; }}\n",
    "        .metric-container {{ display: flex; flex-wrap: wrap; gap: 20px; }}\n",
    "        .metric-card {{\n",
    "            background: #f8f9fa;\n",
    "            border: 1px solid #dee2e6;\n",
    "            border-radius: 5px;\n",
    "            padding: 15px;\n",
    "            flex: 1 1 300px;\n",
    "            box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n",
    "        }}\n",
    "        .metric-title {{ font-weight: bold; color: #2c3e50; }}\n",
    "        .metric-value {{ font-size: 1.2em; color: #27ae60; }}\n",
    "        .negative {{ color: #e74c3c; }}\n",
    "        .plot-container {{ margin: 30px 0; text-align: center; }}\n",
    "        table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}\n",
    "        th, td {{ padding: 10px; text-align: left; border-bottom: 1px solid #ddd; }}\n",
    "        th {{ background-color: #3498db; color: white; }}\n",
    "        tr:nth-child(even) {{ background-color: #f2f2f2; }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Strategy Backtest Performance Report</h1>\n",
    "\n",
    "    <div class=\"plot-container\">\n",
    "        <h2>Equity Curve</h2>\n",
    "        <img src=\"data:image/png;base64,{plot_data}\" alt=\"Equity Curve\">\n",
    "    </div>\n",
    "\n",
    "    <h2>Key Performance Metrics</h2>\n",
    "    <div class=\"metric-container\">\n",
    "        <div class=\"metric-card\">\n",
    "            <div class=\"metric-title\">Total Trades</div>\n",
    "            <div class=\"metric-value\">{total_trade_count:,}</div>\n",
    "        </div>\n",
    "        <div class=\"metric-card\">\n",
    "            <div class=\"metric-title\">Win Rate</div>\n",
    "            <div class=\"metric-value\">{100 - losers_percentage:.1f}%</div>\n",
    "        </div>\n",
    "        <div class=\"metric-card\">\n",
    "            <div class=\"metric-title\">Average Trade Profit</div>\n",
    "            <div class=\"metric-value\">${average_trade_profit:,.2f}</div>\n",
    "        </div>\n",
    "        <div class=\"metric-card\">\n",
    "            <div class=\"metric-title\">Trade Expectancy</div>\n",
    "            <div class=\"metric-value\">${trade_expectancy:,.2f}</div>\n",
    "        </div>\n",
    "        <div class=\"metric-card\">\n",
    "            <div class=\"metric-title\">Risk-Reward Ratio</div>\n",
    "            <div class=\"metric-value\">{risk_reward_ratio:.2f}:1</div>\n",
    "        </div>\n",
    "        <div class=\"metric-card\">\n",
    "            <div class=\"metric-title\">Max Drawdown</div>\n",
    "            <div class=\"metric-value negative\">${max_unrealised_drawdown:,.2f}</div>\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    <h2>Long/Short Breakdown</h2>\n",
    "    <table>\n",
    "        <tr>\n",
    "            <th>Metric</th>\n",
    "            <th>Long Trades</th>\n",
    "            <th>Short Trades</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Count</td>\n",
    "            <td>{num_long_trades:,}</td>\n",
    "            <td>{num_short_trades:,}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Win Rate</td>\n",
    "            <td>{long_win_ratio:.1%}</td>\n",
    "            <td>{short_win_ratio:.1%}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Avg Profit (bps)</td>\n",
    "            <td>{avg_long_profit_bps:.1f}</td>\n",
    "            <td>{avg_short_profit_bps:.1f}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Total Profit</td>\n",
    "            <td>${profit_total_long:,.2f}</td>\n",
    "            <td>${profit_total_short:,.2f}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Max Win Streak</td>\n",
    "            <td>{long_consecutive[long_consecutive['type']=='win']['length'].max() if len(long_consecutive) > 0 else 0}</td>\n",
    "            <td>{short_consecutive[short_consecutive['type']=='win']['length'].max() if len(short_consecutive) > 0 else 0}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Max Loss Streak</td>\n",
    "            <td>{long_consecutive[long_consecutive['type']=='loss']['length'].max() if len(long_consecutive) > 0 else 0}</td>\n",
    "            <td>{short_consecutive[short_consecutive['type']=='loss']['length'].max() if len(short_consecutive) > 0 else 0}</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "\n",
    "    <h2>Position Sizing</h2>\n",
    "    <div class=\"metric-container\">\n",
    "        \n",
    "        <div class=\"metric-card\">\n",
    "            <div class=\"metric-title\">Max Entries Per Day</div>\n",
    "            <div class=\"metric-value\">{max_entries_per_day}</div>\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    <h2>Trade Duration</h2>\n",
    "    <div class=\"metric-container\">\n",
    "        <div class=\"metric-card\">\n",
    "            <div class=\"metric-title\">Avg Holding Days</div>\n",
    "            <div class=\"metric-value\">{local_trade_log2['holding_days'].mean():.1f}</div>\n",
    "        </div>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Display the report\n",
    "HTML(html_report)\n",
    "\n",
    "# To save the report to a file\n",
    "with open('Nasdaq_Vol_stocks.html', 'w') as f:\n",
    "    f.write(html_report)\n",
    "\n",
    "local_trade_log2.to_csv(\"Nasdaq_TL.csv\", index=True)\n",
    "\n",
    "# Convert total_pnl_series dict (timestamp → pnl) into DataFrame\n",
    "mtm_equity_df = pd.DataFrame(list(total_pnl_series.items()), columns=[\"timestamp\", \"MTM_PnL\"])\n",
    "\n",
    "# Sort by timestamp (important to keep the curve ordered)\n",
    "mtm_equity_df.sort_values(\"timestamp\", inplace=True)\n",
    "\n",
    "# Add equity column: starting capital + cumulative PnL\n",
    "mtm_equity_df[\"Equity\"] = intial_capital + mtm_equity_df[\"MTM_PnL\"].cumsum()\n",
    "\n",
    "# Set timestamp as index for convenience\n",
    "mtm_equity_df.set_index(\"timestamp\", inplace=True)\n",
    "\n",
    "# Define full path\n",
    "mtm_equity_file = os.path.join(trades_strategy_folder, \"MTM_equity.csv\")\n",
    "\n",
    "# Save to CSV\n",
    "mtm_equity_df.to_csv(mtm_equity_file)\n",
    "\n",
    "print(f\"✅ MTM equity curve saved at: {mtm_equity_file}\")\n",
    "\n",
    "\n",
    "# for symbol, data in absolute_symbol_pnl_series.items():\n",
    "#     if not data:\n",
    "#         print(f\"No data for {symbol}, skipping.\")\n",
    "#         continue\n",
    "#     try:\n",
    "#         tdf = pd.DataFrame(data, columns=['timestamp', 'pnl_absolute'])\n",
    "#         tdf.set_index('timestamp', inplace=True)\n",
    "#         file_path = os.path.join(df_folder, f\"{symbol}_pnl.csv\")\n",
    "#         tdf.to_csv(file_path)\n",
    "#         print(f\"Successfully saved {file_path}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing {symbol}: {e}\")\n",
    "\n",
    "\n",
    "import quantstats as qs\n",
    "qs.extend_pandas()\n",
    "\n",
    "qs.reports.basic(local_trade_log2['EC'])\n",
    "\n",
    "# --- 2. Plotting the 'EC' column ---\n",
    "plt.figure(figsize=(12, 6)) # Optional: Adjust the figure size\n",
    "\n",
    "local_trade_log2['EC'].plot(grid=True) # The main plotting command\n",
    "\n",
    "# --- 3. Add labels and a title for clarity ---\n",
    "plt.title('EC Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('EC Value')\n",
    "\n",
    "# --- 4. Display the plot ---\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5806c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_trade_log2['profit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f55ad49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
